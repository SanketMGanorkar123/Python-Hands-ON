{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What made you switch your career to DataScience?\n",
    "# 2. Why you want to resign from current job?\n",
    "# 3. What Package you are expecting (In HR round)?\n",
    "> Whichever comapny has called for interview search about its history whther it is Product based or Service based Industry.\n",
    "# Why projects actually exists(Service based)?\n",
    "1. Time Management\n",
    "2. Automation \n",
    "3. Process Management \n",
    "4. Revenue Generation >> Maximization\n",
    "5. Revenue loss >> Optimization \n",
    "> Proof of Concept > POC > Paid or Unpaid \n",
    "> Duration of POC > 2-3 months , 6 months , 1 year \n",
    "* \n",
    "> In POC we get Problem statement / Business problems / Business requirements / What was the Business scenario\n",
    "* Approach towards project > Python \n",
    "                           > MAchine learning\n",
    "                           > NLP\n",
    "                           > Deep Learning\n",
    "                           > Time Series Analysis \n",
    "> Product based Industry have their own database and we can retrieve data from database.\n",
    "> Who are the people involved in the project?\n",
    "1. Project Manager  - 01 \n",
    "2. Team Leader      - 01 \n",
    "3. Business Analyst - 01\n",
    "4. Data Engineer    - 01 or 02 >>  Data Engineer fetches data from database\n",
    "5. Data Scientist   - 01\n",
    "6. ML Engineer      - 01 \n",
    "7. Python Developer - 01  >> For Web Framework(Flask , Django , FastAPI , gRPC)\n",
    "8. Power Bi Engineer- 01  >> For Data Visualization\n",
    "9. DevOps Engineer  - 01 or 02 >> For Project deployment (Cloud platform : AWS , Azure(Azure ML) , GCP)\n",
    "10. Front end developer : UX Designer >> Create Design \n",
    "                        : UI Developer >> Integrate API \n",
    "> Project Planning \n",
    "* Project planning Tools \n",
    "1. JIRA \n",
    "2. Zoho \n",
    "3. Trello \n",
    "4. Asana \n",
    "> Pipeline : In Industry we are getting data seperate for training and seperate for testing . we will write single functions in which we carry out necessary steps (EDA , Feature Engineering , Feature selection) and then we will call the same function for testing data. So in this way we are creating a pipleine (flow) for all steps.\n",
    "> Docker : It is used for creating pipeline(Flow / Connection) .It works as a Ship carrying different containers.\n",
    "* Generally when we create a model and depoly it , we will do some observations . In Observation we will see how our model is performing on new data and we will monitor this process. This process is called as CICD pipeline (Continous integration and Continous Developement). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* During call with HR ask him about JD(Job Description).\n",
    "* 1st round is generalized round (Introduction , end of the introduction will decide the further interview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Project \n",
    "* First step is project planning and timeline.So for this purpose we use project planning tools (JIRA)\n",
    "* In big Industries we will use JIRA and in small scale companies we will use Excel file for project planning.\n",
    "> Kickoff Meeting : The first meeting when the project is asiggned we will discuss project agenda , project team , introduction.\n",
    "> Onboarding of project / Project kickoff : \n",
    "* Client meet \n",
    "* Project problem discussion\n",
    "* PPT / Deck\n",
    "* Main work \n",
    "* Project Management : JIRA , ZOHO , Trello , Asana \n",
    "> Project Duration / Project timeline >> It will be decided by Higher Authorities(Project Manager , Team Lead , Technical lead)\n",
    "* \n",
    "> VDE (Virtual desktop Environment) : \n",
    "1. Local System(Laptop/Computer) : System available with you physically and you are working on it.\n",
    "2. Virtual system(vmware) : System where you can login online and it has interface as local system.\n",
    "> While scheduling the interview with Hr tell him let me check my calendar wall and then we will schedule the interview.\n",
    "> ScrumMaster(Big Companies) : Person who handles the JIRA software and gives updates.In small company Team Lead will be the scrum master.\n",
    "* \n",
    "> Epic(JIRA) >> Project plan >> suppose 5months \n",
    "* Data Gathering \n",
    "* EDA \n",
    "* Feature Engineering\n",
    "* Feature Selection\n",
    "* Model Building \n",
    "> Subpart of Epic >> Sprint (1 Week or 2 Week) :\n",
    "* Within Sprint we will have subparts as story1 and story2 . Suppose we have story1 as Data Gathering and story2 as EDA.\n",
    "* We can have multiple Sprints.And in that Sprint we can have multiple stories.\n",
    "> Issues : Problems faced \n",
    "* \n",
    "> From where do we get the data?\n",
    "* We will get data from different Databases(MySQL , PostgreSQL , MongoDB(NoSQL) ,Company DataBase(Product Based), client DataBase,Cloud S3 Bucket (Sagemaker) , Data Engineer) in the form of CSV , Excel , JSON , Text.\n",
    "* Data Engineer fetches data from Databases and will send it to us by mail on CLient mailid.\n",
    "> Web Scrapping (Sentiment Analysis , NLP ,Movie reviews) for data.\n",
    "* Sometimes the client can only provide the feature names.For Data we can use Kaggle or other resources.\n",
    "> Synthetic Data Generation : Hugging Face \n",
    "* \n",
    "> DataBase access : Read and write access \n",
    "* \n",
    "> Evironment access(Done after project completion)   \n",
    "1. Production Environment\n",
    "2. Testing Environment(SandBox)\n",
    "* First we will deploy our model on Testing Environment and after it is performing good we will go for Production Environment.\n",
    "* \n",
    "> What was your data size?\n",
    "* 0.5 Million to 2 million >> Rows \n",
    "* 10 to 40 >> Features \n",
    "> Features \n",
    "1. Original features\n",
    "2. Derived features >> New features that we can introduce \n",
    "* Suppose we have house price prediction and we want to derive new features.\n",
    "> Celonis : Process mining tool , it uses a vertica sequel same as sql \n",
    "> How frequently your data was updating?\n",
    "* Suppose we have 100000 rows and 25 features and the data was updating to 200000 rows and 30 features.\n",
    "> How frequently was your meeting with clients?\n",
    "* Weekly , 15 days ,Biweekly ,alternate days \n",
    "> What was your role in CLient Meeting?\n",
    "* As a Data Scientist i would discuss technical problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After creating resume when we upload resume on Naukri \n",
    "1. Hr Dynamic >> Resume >> Naukri >> Call(Hr) >> Discussion >> Interview schedule \n",
    "2. In Company > \n",
    "* After joining >> They will use you as shadow resource \n",
    "> Shadow Resource : Some part of existing work will be allocated to you.\n",
    "> Probation period : Starting period >> 3 to 6 months > Temporary >> After 3 to 6 months we will taken On Role in Industry.\n",
    "* New project >> Team Finalize \n",
    "              >> Internal meeting : Introduction to team members \n",
    "              >> Project Kickoff : With Client \n",
    "              >> OnBoarding >> VM Credentials >> Userid , Password , token generation \n",
    "              >> Project Management : Tools >> (JIRA) \n",
    "              >> NDA >> Non Disclosure Agreement\n",
    "* List >> Required features from the client.\n",
    "* Then the client will add the required features in Database.\n",
    "* We will get the required features from different tables and after joining these all tables we will get a MasterTable or Masterdata.\n",
    "* MasterTable will be given to Data Scientist.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "> Note : EDA and Feature Engineering will be different for different Algorithms.\n",
    "> Basic EDA \n",
    "1. Data obsevations \n",
    "* Independence \n",
    "* Balancing \n",
    "* Anomaly Detection (disturbance in the data >> duplicate rows(df.drop_duplicates))\n",
    "* distribution of the data (Kdeplot , QQplot , Skew)\n",
    "* data sanity >> Coefficient of Corelation , VIF\n",
    "\n",
    "* EDA has 2 types \n",
    "> Univariate and Multivariate analysis\n",
    "* Univariate analysis is where only one feature is used and in Multivariate analysis we use more that one feature.\n",
    "* Handling Missing values \n",
    "* Delete the Observations \n",
    "* Imputation  >> Continous data >> Mean , median ,KNN imputer (Nan Euclidean distance) , MICE\n",
    "               >> Categorical Data >> mode , KNN imputer(k=1) \n",
    "> What is difference between normal distribution and binomial distribution?\n",
    "1. Normal Distribution \n",
    "* Used in situations where the data follows a continous , symmetric distribution.\n",
    "* Commonly used in statistical interference and hypothesis testing.\n",
    "2. Binomail Distribution \n",
    "* Used when dealing with discrete set of trials with two possible outcomes.\n",
    "* Commonly used in scenarios involving binary events(Success / Failure) , such as coin flips or pass/fail situations.\n",
    "> Normal Distribution is more appropriate for continous variables , while binomial distribution is more appropriate for dicrete variables with a fixed number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Lecture\n",
    "> In Industry target feature is not easily available. We need to identify among all features if not then we need to create target features.We need to create clusters.\n",
    "* Business analyst will help identify the target variable.\n",
    "> Suppose we are considering Logistic regression for a datset and we have outliers in that dataset.\n",
    "> How to check for outliers?\n",
    "* IQR \n",
    "* Z-Score \n",
    "* Boxplot\n",
    "* Scatterplot\n",
    "> When to use which method to find outliers?\n",
    "* For normally distributed data we use Z-score and if data is not normally distributed we use IQR method.\n",
    "* To check data is normally distributed we can use kdeplot , QQplot , Skew and also statistical methods like shapiro ,kstest , normal test , Chi-square , Annova , T-test , Z-test to check data is normally distributed or not.\n",
    "* For Statistical methods we check P_Value . If P_Value is >= 0.05 then Null hypothesis is True.If P_Value is < 0.05 then Alternate hypothesis is True.\n",
    "> Type 1 error : False Positive >> When negative class is wrongly predicted as positive class\n",
    "* When True Null hypothesis is rejected \n",
    "> Type 2 error : False Negative >> When positive class is wrongly predicted as negative class\n",
    "* When False Hypothesis is selected\n",
    "* When we accept Null hypothesis but it is False.\n",
    "> Precision : TP / TP + FP     \n",
    "> (TP + FP) : Out of total predicted positive results and TP is actual positive.\n",
    "* We try to reduce FP.\n",
    "* Highest value of Precision is 1 and Lowest is 0.\n",
    "* Precision should be as high as possible.\n",
    "> Recall : TP / TP + FN \n",
    "* We try to reduce FN.\n",
    "* \n",
    "> Backk to Outliers\n",
    "* In Bell shaped curve values after 3 standard deviations we consider values as extreme outliers.\n",
    "> IQR\n",
    "* Formula For IQR = Q3 - Q1 \n",
    "* Lower Tail : Q1 - 1.5*IQR \n",
    "* Upper Tail : Q3 + 1.5*IQR \n",
    "> Z-score \n",
    "* (X - Xmean) / Standard Deviation \n",
    "* We wil calcuate Z-score for each value of a particular feature and then we will pass threshold and then we will easily detect Outliers.\n",
    "* \n",
    "> How to handle Outliers?\n",
    "1. Delete Observations \n",
    "2. Imputation >> Mean,median,mode,knn_imputer(Replace outliers with Nan values),uppertail,lowertail,static value,Zero.\n",
    "3. Transformation >> Squareroot , log , Cuberoot , reciprocal , Boxcox transformation(Skewed data with positive value).\n",
    "* \n",
    "> Encoding (Categorical Features):\n",
    "1. One hot Encoding (Nominal Data) \n",
    "2. Labeled Encoding(Ordinal data) >> High Medium Low , \n",
    "* \n",
    "> Important terms \n",
    "1. Sigmoid Function / Logistic Function\n",
    "* Formula : 1 / (1 + e^-Y)\n",
    "* Values lies within 0 to 1.\n",
    "* It is a inverse of Logit function.\n",
    "> Logit function : Log of Odd ratio \n",
    "> Odd ratio : probability of success divided by probability of failure\n",
    "* Sigmoid function : P(Y) = 1 / (1 + e^-Y)\n",
    "* Logit function : log(P / 1 - P)\n",
    "* Logit function is used to convert the output in Non Binary.\n",
    "* \n",
    "2. Cost function : Log - loss function\n",
    "* Log - loss function : -1/N summation [Yi * logPi + (1 - Yi) * log(1 - Pi)]\n",
    "3. Gradient Descent Algorithm \n",
    "4. One Over Rest (Multi class classification)\n",
    "5. Regularization (L1 Lasso , L2 Ridge)\n",
    "* \n",
    "> Assumptions \n",
    "1. Linearity (Continuous Independent Variable with Log(Odds))\n",
    "* Variance ,Covariance , standard deviation \n",
    "* Coefficient of Corelation(R) = Covariance / Product of Standard Deviation \n",
    "* Corelation gives strength of linear relationship between 2 variables and Covariance gives directional relationship of these 2 variables.\n",
    "2. No Multi Co linearity \n",
    "* VIF = 1 / (1 - R2)\n",
    "3. Feature Selection \n",
    "* Filter method (Before Model Training) > Corr,VIF,Fisher score,Annova,Chisquare\n",
    "* Wrapper Method (During Model Training) > Forward , Backward , RFE \n",
    "* Embedded Method (After Model Training) > Lasso regression(WIth and Without scaling)\n",
    "4. Feature Extraction\n",
    "* Principal Component Analysis(PCA)\n",
    "* Linear Discriminant Analysis(LDA)\n",
    "5. Evaluation (Confusion Matrix >> TP,TN,FP,FN)\n",
    "* Accuracy score (Used when data is balanced)\n",
    "* Precision (WE try to reduce FP)\n",
    "* Recall (We try to reduce FN)\n",
    "* F1- score (when precision and recall are equally important)\n",
    "* AUC-ROC curve \n",
    "* Underfitting (High Bias low variance)\n",
    "* Overfitting (Low Bias high variance)\n",
    "* Bias-Variance tradeoff\n",
    "> In Logistic Regression we have threshold as 0.5 but if we have many data points on 0.4 and you want to take that data points as class1 classification we need to do it manually after model building . We cannot make changes in threshold while model building.Whatever changes are needed to make in threshold are made after model building manually.\n",
    "* \n",
    "> If we get Imbalanced data points?\n",
    "1. Oversampling \n",
    "* Random Oversampler\n",
    "* SMOTE (KNN Algorithm ) \n",
    "2. Undersampling\n",
    "* Random Undersampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm\n",
    "> Scaling\n",
    "* Normalization(MinMaxScalar) :(0 to 1) >> Impacted by Outliers\n",
    "* Standardization(StandardScalar) :(-3 to +3) >> Not impacted by outliers\n",
    "* Lazy algorithm\n",
    "* Storing the data \n",
    "* Distance based Algorithm \n",
    "* Not preferable large data sets.\n",
    "* \n",
    "# Decision Tree Algorithm\n",
    "> Working \n",
    "* We select the root node.\n",
    "* Attribution selection measures \n",
    "1. Entropy (0 to 1) >> Binary classification\n",
    "2. Gini Index (0 to 0.5) >> Binary classification\n",
    "* We will calculate Information gain.\n",
    "* Information gain : E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a major issue for Decision Tree.\n",
    "* For this purpose, we do \n",
    "> Hyperparameter tuning\n",
    "* max_depth\n",
    "* min_sample_leaf\n",
    "* min_sample_split\n",
    "* n_estimators\n",
    "* Criterion \n",
    "> Pruining (Cutting Branches)\n",
    "* Post Pruining >> CCP alpha\n",
    "* Pre Pruining >> \n",
    "> Feature selection \n",
    "* Decision tree feature importance \n",
    "* \n",
    "# Random Forest Algorithm\n",
    "> Working \n",
    "* We select the root node.\n",
    "* Attribution selection measures \n",
    "1. Entropy (0 to 1) >> Binary classification\n",
    "2. Gini Index (0 to 0.5) >> Binary classification\n",
    "* We will calculate Information gain.\n",
    "* Information gain : E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a major issue for Rnadom Forest. \n",
    "> Ensemble Techniques \n",
    "> Bagging \n",
    "* Parallel approach (Bootstrap aggregation)\n",
    "* Row Sampling (Randomly)\n",
    "* OOB samples (Out of Bag samples)  \n",
    "> Boosting\n",
    "* Sequential approach\n",
    "> Difference between Bagging and Boosting?\n",
    "> Bagging \n",
    "* Creates a team of diverse learners from random data samples.\n",
    "* Reduces Variance (wont overfit easily)\n",
    "* Good for high variance models like Decision Tree\n",
    "* Bagging for less Overfitting.>> Boosting \n",
    "* Builds learners one by one ,focusing on the mistakes made by the previous learner.\n",
    "* Reduces Bias (Better at capturing patterns)\n",
    "* Good for high bias models like linear regression.\n",
    "* Boosting for better accuracy.\n",
    "> Hyperparameter tuning\n",
    "* max_depth\n",
    "* min_sample_leaf\n",
    "* min_sample_split\n",
    "* n_estimators \n",
    "* Criterion \n",
    "* max_features >> Sqrt(n_features)\n",
    "* bootstrap = True\n",
    "* oob_score = True \n",
    "> Feature selection\n",
    "* random forest feature importance\n",
    "* \n",
    "# AdaBoost algorithm\n",
    "> Boosting >> Sequential approach\n",
    "> Decision stump (One root node and two or more leaf nodes)\n",
    "* Weak learners conversion into strong learners.\n",
    "1. Sample Weight\n",
    "2. Model Train \n",
    "3. Total error  >> No of Misclassified samples / Total samples \n",
    "4. Performance  >> 1/2 log(1 - Total error)\n",
    "5. New sample weight \n",
    "* New Sample weight Correctly classified samples = old sample weight * e^-performance\n",
    "* New Sample weight wrongly classified samples = old sample weight * e^performance\n",
    "6. Normalization\n",
    "7. Create Buckets \n",
    "8. Create New dataset\n",
    "> Hyperparameter tuning (Cross Validation)\n",
    "* n_estimators (default = 50)\n",
    "* learning_rate (alpha) >> Contribution of each and every model in your results. \n",
    "> Feature selection\n",
    "* AdaBoost feature importance\n",
    "* \n",
    "# Naive Bayes \n",
    "* Perform well on High dimesnional dataset \n",
    "* Works on Bayes theorem \n",
    "1. Gaussian >> Continuous (Data Should be normally distributed) \n",
    "2. Multinomial \n",
    "3. Bernoulli \n",
    "> How to check Normality?\n",
    "1. kdeplot \n",
    "2. QQ plot \n",
    "3. Hypothesis testing\n",
    "* Shapiro test \n",
    "* kstest \n",
    "* normal test \n",
    "4. Skewness (Skew = 0 >> Normally distributed)\n",
    "* \n",
    "# Support vector machine Algorithm \n",
    "* Performs well on High Dimensional dataset \n",
    "> Advantage \n",
    "* It performs well for linearly seperable data and non linearly seperable data \n",
    "* Low Dimensional data works well \n",
    "* \n",
    "* We convert low dimensional dataset into High Dimensional dataset \n",
    "> Kernals \n",
    "* RBF \n",
    "* Linear \n",
    "* Polynomial \n",
    "* Sigmoid \n",
    "* \n",
    "> Home work : While applying Linear regression in backend what does it apply Gradient descent algorithm or ordinary least sqaure method? Default method is OLS if i want to apply Gardient descent algorithm what in need to do?\n",
    "> Introduction\n",
    "* Hr call >> Introduction , Background \n",
    "* Name \n",
    "* Designation , company name , location\n",
    "* Skills >> Domain technical \n",
    "* Roles and responsibilities\n",
    "* Job change >> Talk politely and Clarity \n",
    "* Plan your Interview >> How to tell answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
