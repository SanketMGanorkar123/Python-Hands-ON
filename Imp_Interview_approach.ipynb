{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whichever company has called for interview search about its history wether it is product based or service based Industry.\n",
    "> Why projects actually exist?\n",
    "1. Time management\n",
    "2. Automation\n",
    "3. Process management\n",
    "4. Revenue Generation\n",
    "5. Revenue loss optimization\n",
    "> Proof of concept > POC > Paid or unpaid duration of POC > 2-3 months , 6 months , 1 year\n",
    "* \n",
    "> In POC we will get problem statement / Business problem / Business requirements / What was the business scenario\n",
    "* Approach towards project > Python > Machine Learning > NLP > Deep Learning > Time series analysis.\n",
    "> Product based companies have their own database and we can retrieve data from the database.Who are the people involved in the project?\n",
    "1. Project Manager - 01 \n",
    "2. Team Leader - 01\n",
    "3. Business Analyst - 01\n",
    "4. Data Engineer - 01 or 02 >> Data Engineer fetches data from the database.\n",
    "5. Data Scientist - 01 \n",
    "6. ML Engineer - 01 \n",
    "7. Python Developer - 01 >> For web framework(Flask , Django , FastApi , gRPC)\n",
    "8. Power Bi developers - 01 >> Data Visualization\n",
    "9. Devops Engineer - 01 or 02 >> For project deployment(Cloud platform : AWS ,Azure(Azure ML),GCP)\n",
    "10. Front End Developer : UX Designer >> Create Design\n",
    "                        : UI Developer >> Integrate API \n",
    "* \n",
    "> Project Planning\n",
    "* Project Planning tools \n",
    "1. JIRA \n",
    "2. Zoho \n",
    "3. Trello \n",
    "4. Asana \n",
    "> Pipeline : In Industry we are getting data seperate for training and seperate for testing.We will write single functions in which we carry out necessary steps (EDA ,Feature Engineering,Feature Selection) and then we call the same function for testing data.So in this way we are creating a pipeline for all steps.\n",
    "> Docker : It is used for creating Pipeline(Flow/COnnection).It Works as a ship carrying different containers.\n",
    "* Generally when we create a model and deploy it , we will do some observations.In Observations we will see how our model is performing on new data and we will monitor this process.This process is called as CICD pipeline(Continous Integration and Continous Development).\n",
    "* During call with Hr ask him about the Job description(JD).\n",
    "* 1st round is generalized round (Introduction ,end of the introduction will deicde the further interview)\n",
    "> Project \n",
    "* First step is project planning and Timeline.So for this purpose we use project planning tools (JIRA).\n",
    "* In Big Industry we will use JIRA and in small scale Industry we will use Excel file for project planning.\n",
    "> Kickoff meeting : The first meeting after the project is assigned we will discuss project agenda , project team , introduction. OnBoarding of project / Project Kick Off \n",
    "* CLient Meet \n",
    "* Project problem discussion\n",
    "* PPT / Deck \n",
    "* Main Work \n",
    "* Project Management : JIRA , ZOHO , Trello , Asana \n",
    "> Project Duration / Project Timeline >> It will be decided by higher authorities (Project Manager , Team Lead , Technical Lead)\n",
    "* \n",
    "> VDE (Virtual desktop Environment)\n",
    "1. Local System(Laptop / Computer) : System available with you physically and you are working on it.\n",
    "2. Virtual System(VmWare) : System where you can login online and it has interface as Local System.\n",
    "> While scheduling the interview with Hr tell him let me check my calendar wall and then we ill schedule the interview.\n",
    "> ScrumMaster(Big Companies) : Person who handles the JIRA software gives updates.In Small companies Team Lead will be the Scrum Master.\n",
    "* \n",
    "> Epic(JIRA) >> Project Plan >> Suppose 5 months \n",
    "* Data Gathering \n",
    "* EDA \n",
    "* Feature Engineering\n",
    "* Feature selection\n",
    "* Model Building\n",
    "> Subpart of Epic >> Sprint(1 Week or 2 Weeks)\n",
    "* Within SPrint we will have subparts as story1 and story2.Supose we have Story1 as Data Gathering and Story2 as EDA.\n",
    "* We can have multiple sprints.And in that sprint we can have multiple stories.\n",
    "> From where do we get the data?\n",
    "* We will get the data from different databases(MySQL, postgresql, MongoDB(NoSql), Company database(Product based) , Client database , CloudS3 (Sagemaker) , Data Engineer) in the form of CSV ,Excel , JSON , text.\n",
    "* Data Engineer will fetch the data from databases and will send to us by mail on CLient Mailid which is shared with us.\n",
    "> Web Scrapping (Sentiment analysis , NLP , movie reviews) for data.\n",
    "* Sometimes the client can only provide the feature names.For data we can use Kaggle or other resources.\n",
    "> Synthetic data Generation >> Hugging Face \n",
    "* \n",
    "> DataBase access : Read and write access \n",
    "* \n",
    "> Environment access(DOne After project completion) \n",
    "1. Production Environment \n",
    "2. Testing Environment (Sandbox)(UAT)\n",
    "* First we will deploy our model on Testing Environment and after we are convinced it is performing good then we will deploy it on Production Environment.\n",
    "* \n",
    "> What was your data size?\n",
    "* 0.5 million to 2 million >> rows\n",
    "* 10 to 40 >> Features \n",
    "> Features \n",
    "1. Original features\n",
    "2. Derived features  >> New features that can be introduced.\n",
    "> Celonis : Process Mining tool , it uses a vertica sequel same as SQL.\n",
    "> How frequently was your data was updating?\n",
    "* Suppose we have 100000 rows and 25 features and the data was updating to 200000 rows and 30 features.\n",
    "> How frequently was your meeting with your clients?\n",
    "* Weekly , 15 Days , Biweekly , Alternate days \n",
    "> What was your role in CLient meeting?\n",
    "* As a Data Scientist i would discuss technical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After creating resume when we upload the resume on Naukri\n",
    "1. Hr Dynamic >> Resume >> Naukri >> Call Hr >> Discussion >> Interview schedule \n",
    "2. InCompany \n",
    "* After joining they will use you as a shadow resource.\n",
    "> Shadow resource : Some part of existing work will be allocated to you.\n",
    "> Probation Period >> 3 to 6 months (Temporary) >> After 3 to 6 months we will be taken as On Role in Industry.\n",
    "* New project >> Team Finalize >> Internal Meeting : Introduction to team Members >> Project Kickoff : With Client >> OnBoarding >> VM Credentials >> Userid , Password , Token Generation >> Project Management tools >> JIRA >> NDA (Non Disclosure agreement)\n",
    "* List given to client for required features.\n",
    "* Then the client will add the required features in Database.\n",
    "* We will get the required features from different tables and after joining these tables we will get a MAsterTable or MasterData.\n",
    "* MasterTable will be given to DataScientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "> Note : EDA and Feature Engineering will be different for different Algorithms.\n",
    "* Basic EDA \n",
    "1. Data Obesrvations \n",
    "* Independence \n",
    "* Balancing \n",
    "* Anomaly Detection (Disturbance in the data >> Duplicate rows (df.drop_duplicate()))\n",
    "* Distribution of the data\n",
    "* Data Sanity >> Coefficient of correlation , VIF \n",
    "* EDA has 2 types \n",
    "> Univariate Analysis and Multivariate Analysis\n",
    "* Univariate Analysis is where only one feature is taken in consideration and in Multivariate Analysis we consider more than one feature.\n",
    "* Handling Missing values \n",
    "* Delete the Observations \n",
    "* Imputation >> Continous data >> Mean , Median , KNN Imputer (Nan Eucledian distance) , MICE\n",
    "             >> Categorical data >> Mode , KNN Imputer(K=1)\n",
    "> What is difference between Normal Distribution and Binomial distribution?\n",
    "1. Normal Distribution \n",
    "* Used in situations where the data follows a continous , symmetric distribution.\n",
    "* Commonly used in statistical interference and hypothesis testing.\n",
    "2. Binomail Distribution \n",
    "* Used when dealing with discrete set of trials with two possible outcomes.\n",
    "* Commonly used in scenarios involving binary events(Success / Failure) , such as coin flips or pass/fail situations.\n",
    "> Normal Distribution is more appropriate for continous variables , while binomial distribution is more appropriate for dicrete variables with a fixed number of trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What made you switch your career to DataScience?\n",
    "> Why you want to resign from current job?\n",
    "> What package are you expecting (In Hr Round)?\n",
    "* Whichever company has called you search about its history whterher it is a Product based ar Service based Industry.\n",
    "# Why Projects actually exist(Service Based)? \n",
    "1. Time Management \n",
    "2. Automation \n",
    "3. Process Management\n",
    "4. Revenue Generation >> Maximization\n",
    "5. Revenue Loss >> Optimization\n",
    "> Proof of Concept >> POC >> Paid or Unpaid >> Duration >> 2-3 months , 6 months , 1 Year \n",
    "* \n",
    "> In POC we get a problem statement / Business Problem / Business Requirements / What was the Business Scenario?\n",
    "* Approach towards Project > Python > Machine Learning > NLP > Deep Learning > Time Series Anlaysis \n",
    "> Product based Industry have their own Database and we can retrieve data from the Database.\n",
    "> Who are the People involved in the Project?\n",
    "1. Project Manager - 01 \n",
    "2. Team Leader - 01 \n",
    "3. Business Analyst - 01 \n",
    "4. Data Engineer - 01 or 02 >> Data Engineer fetched data from DataBase.\n",
    "5. Data SCientist - 01 \n",
    "6. ML Engineer - 01 \n",
    "7. Python Developer - 01 >> For Web Framework(Flask , Django , FastAPI , gRPC)\n",
    "8. PowerBI Engineer - 01 >> For Data Visualization \n",
    "9. Devops Engineer - 01 or 02 >> For Project Deployment (Cloud Platform : AWS , Azure(Azure ML), GCP)\n",
    "10. Front End Developer - 01 >>\n",
    "* UX Designer >> Creates Design\n",
    "* UI Developer >> Integrates API \n",
    "> Project Planning \n",
    "* Project Planning Tools \n",
    "1. JIRA \n",
    "2. ZOHA \n",
    "3. TRELLO \n",
    "4. Asana \n",
    "> Pipeline : In Industry we are getting seperate data for Training and separate data for testing. We will write single functions in which we carry out necessary steps(EDA , Feature Engineering , Feature Selection) and then we will call the same function for testing data.So in this way we are creating a pipeline(flow) for all steps. \n",
    "> Docker : It is used for creating the pipeline(Flow/Connection).It works as ship carrying different Containers.\n",
    "* Generally when we create a model and deploy it we will do some Observations.In Observations we will see how our model is performing on new data and we will monitor this process.This Process Is called as CICD pipeline(Continous Integration and Continuous Development)\n",
    "* During call with Hr ask him about the Job Description.\n",
    "* 1st round is generalized round (Introduction , end of the instruction will decide the further interview)\n",
    "> Project \n",
    "* First Step is project planning and Timeline.So for this purpose we use project planning tools(JIRA).\n",
    "* In Big Industries we will use JIRA and in small companies we will use Excel file for Project Planning.\n",
    "> Kickoff Meeting : First meeting when the project is assigned , we will discuss project agenda , project team , introduction. Onboarding of Project / Project Kickoff \n",
    "* Client Meet \n",
    "* Project Problem Discussion\n",
    "* PPT / Deck\n",
    "* Main Work \n",
    "* Project Management : JIRA , ZOHO ,TRELLO , Asana \n",
    "> Project Duration / Project Timeline >> It will be decided by the higher authorities(Project Manager , Team Lead , Technical Lead)\n",
    "* \n",
    "> VDE (Virtual Desktop Environment) :\n",
    "1. Local System(Laptop / COmputer) : System available with you physiaclly and you are working on it.\n",
    "2. Virtual System(VMware) : System where you can login online and it has interface same as local system.\n",
    "> While scheduling the interview with Hr tell him let me check my calendar wall and then we will schedule the interview.\n",
    "> Scrum Master(Big Companies) : Person who handles the JIRA software and give updates. In small companies team lead will be the Scrum Master.\n",
    "* \n",
    "> Epic(JIRA) >> Project Plan >> Suppose 5 months \n",
    "* Data Gathering \n",
    "* EDA \n",
    "* Feature Engineering\n",
    "* Feature selection\n",
    "* Model Building\n",
    "> Subpart of Epic >> SPrint (1 Week or 2 Weeks) : \n",
    "* Within sprint we will have subparts as story1 and story2.Suppose we have story1 as data gathering and story2 as EDA.\n",
    "* We can have mutiple sprint . And in that sprint we can have multiple stories.\n",
    "* \n",
    "> From where do we get the data?\n",
    "* We will get data from different databases(MySql , PostgreSQL , MongoDB(NoSQL) , Company Database(Product based) , Cient Database , CLoudS3 Bucket(Sagemaker) , Data Engineer) in the form of CSV ,EXcel , JSON , Text.\n",
    "* Data Engineer fetches data from the database and will send it to us by mail on CLient emailid.\n",
    "> Web Scrapping (Sentiment Analysis , NLP , Movie reviews) for data.\n",
    "* Sometimes the client can provide only feature names.For data we can use Kaggle or other resources.\n",
    "> Synthetic data Generation : Hugging face \n",
    "* \n",
    "> DataBase access : Read and write access \n",
    "* \n",
    "> Environment access(Done after project completion)\n",
    "1. Production environment \n",
    "2. Testing environment(Sandbox) \n",
    "* First we will deploy our model on Testing environment and after it is performing Good we will go for Production Environment.\n",
    "> What was your data Size?\n",
    "* 0.5 Million to 2 million >> Rows \n",
    "* 10 to 40 >> Features \n",
    "> Features \n",
    "1. Original Features \n",
    "2. Derived Features >> New features that we can introduce \n",
    "> Celonis : Process Mining tool , it uses vertica sequel same as sql .\n",
    "> How frequently was your data is updating?\n",
    "* Suppose we have 100000 rows and 25 features and the data may update to 200000 rows and 30 features.\n",
    "> How Frequently was your meeting with Clients?\n",
    "* Weekly , 15 days , Biweekly , alternate days \n",
    "> What was your role in CLient Meeting?\n",
    "* As a Data Scientist i would discuss more technical issues with the project.\n",
    "* \n",
    "# After creating resume when we upload resume on Naukri \n",
    "1. Hr Dynamic >> Resume >> Naukri >> Call with Hr >> Discussion >> Interview schedule \n",
    "2. In Company >\n",
    "* After joining >> They will use you as a shadow resource at the beginning.\n",
    "> Shadow Resource : Some part of the existing work will be alocated to you.\n",
    "> Probabtion period : Starting period >> 3 to 6 months >> After that we will be taken On role in Industry.\n",
    "* New project >> Team FInalize \n",
    "              >> Internal Meeting : Introduction to team members \n",
    "              >> Project Kickoff : with client \n",
    "              >> On Boarding >> VM credentials >> Userid , password , token generation \n",
    "              >> Project management : tools >> (JIRA)\n",
    "              >> NDA >> Non Disclosure Agreement\n",
    "* List >> Required features from the client \n",
    "* Then the client will add the required features in the database.\n",
    "* We will get the required features in the form of different tables and after joining all tables we will get the Master Table or Master Data.\n",
    "* Master table will given to the Data Scientist.\n",
    "* \n",
    "# EDA \n",
    "> Note : EDA and Feature Engineering will be different for different Algorithms.\n",
    "> Basic EDA \n",
    "1. Data Observations\n",
    "* Independence \n",
    "* Balancing \n",
    "* Anomaly detection (disturbance in the data >> Duplicate rows(df.drop_duplicates()))\n",
    "* Distribution of the data \n",
    "* Data Sanity >> Coefficient of correlation , VIF \n",
    "* EDA has 2 types \n",
    "> Univariate analysis and multivariate analysis \n",
    "* Univariate analysis is where we consider only one feature and in multivariate analysis we will consider more than one feature.\n",
    "* Handling Missing values\n",
    "* Delete the obseravations \n",
    "* Imputation >> Continous data >> mean , median , knn imputer(Nan Eucledian distance),MICE\n",
    "             >> Categorical data >> mode , KNn imputer(k=1) \n",
    "> What is the difference between normal distribution and Binomial distribution?\n",
    "1. Normal Distribution \n",
    "* Used in situations where the data follows continous , symmteric distribution \n",
    "* Commonly used in statustical interference and hypothesis testing\n",
    "2. Binomial Distribution \n",
    "* Used when dealing with discrete set of trials with 2 possible outcomes.\n",
    "* Commonly used in Scenarios involving Binary events (Success/ Failure) , such as coin flips , or pass/Fail Situatons.\n",
    "> Normal Distribution is more appropriate for Continous variables , while Binomial distribution is more appropriate for discrete variables with a fixed number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last lecture \n",
    "> In Industrt target variable is not easily available.We need to identify among all features if not then we need to create the target variable.We need to create clusters.\n",
    "* Business Analyst will help identify the target variable.\n",
    "> Suppose we are considering Logistic regression for a dataset and we have outliers in the dataset.\n",
    "> How to check for outliers?\n",
    "* IQR \n",
    "* Z-score \n",
    "* Boxplot \n",
    "* Scatterplot \n",
    "> When to use which method to detect outliers?\n",
    "* For nornally distributed data we use Z-score and for the data which is not normally distributed we use IQR method.\n",
    "* To check if data is normally distributed we can use kdeplot , QQplot ,skew and also statistical methods like Shapiro , kstest , normal test ,chi-squared , Annova , T-Test , Z-Test to check if data is normally distributed.\n",
    "* For statistical methods we check for p_val.If p_val >= 0.05 then Null Hypothesis is True.If p_val is > 0.05 then Alternate Hypothesis is True.\n",
    "> Type 1 error : False positive >> When Negative class is wrongly predicted as positive.\n",
    "* When true Null Hypothesis is rejected.\n",
    "> Type 2 error : False Negative >> When positive class is wrongly predicted as negative class.\n",
    "* When false hypothsesis is selected.\n",
    "* When we accept null hypothesis but it is False.\n",
    "> Presicision : TP / TP + FP \n",
    "> (TP + FP) : Out of total predicted positive class and TP is actual positive.\n",
    "* We try to reduce FP \n",
    "* Highest value of precision is 1 and lowest is 0.\n",
    "* Precision should be high as possible.\n",
    "> Recall : TP / TP + FN \n",
    "* We try to reduce FN \n",
    "* \n",
    "> Back to Outliers \n",
    "* In Bell shaped curve values after 3 standard deviations are considered as extreme outliers.\n",
    "> IQR \n",
    "* IQR = Q3 - Q1 \n",
    "* Lower tail : Q1 - 1.5*IQR\n",
    "* Upper tail : Q3 + 1.5*IQR\n",
    "> Z-Score \n",
    "* (Xi - Xmean) / Standard deviation\n",
    "* We will calculate Z-Score for each value in a particular feature and then we will pass a threshold and then we can easily detect outliers.\n",
    "> How to Handle outliers?\n",
    "1. Delete Observations\n",
    "2. Imputation : Mean,Median,Mode,Knn Imputer(Replace outliers with Nan Values), upper tail, lower tail,static value , Zero\n",
    "3. Transformation : Squareroot , cuberoot , log , reciprocal , boxcox transformation(Skewed data with positive values)\n",
    "* \n",
    "> Encoding :\n",
    "1. One hot Encoding(Nominal Data)\n",
    "2. Labeled Encoding(Ordinal data) >> we have order in data : High Median low \n",
    "* \n",
    "> Important terms \n",
    "1. Sigmoid function / Logistic function\n",
    "* formula : 1 / 1 + e^-Y\n",
    "* Value lies between 0 and 1\n",
    "* It is a inverse of logit function.\n",
    "> Logit function : log of odd ratio \n",
    "> odd ratio : Probability of Success divided by Probability of Failure\n",
    "* Sigmoid Function : P(Y) = 1 / (1 + e^-Y)\n",
    "* Logit Function : log(P / 1 - P)\n",
    "* Logit Function is used to convert the output in Non Binary.\n",
    "2. Cost Function : log - loss function \n",
    "* Log - loss function : -1/N summation[Yi * logPi + (1 - Yi) * log(1 - Pi)]\n",
    "3. Gradient Descent Algorithm \n",
    "4. One Over Rest (Multi class classification)\n",
    "5. Regularization (L1 Lasso , L2 Ridge)\n",
    "* \n",
    "> Assumptions \n",
    "1. Linearity (Continous Independent variable with Log(odds))\n",
    "* Variance , covariance , standard deviation \n",
    "* Coefficient of Corelation : Covariance / product of standard deviation \n",
    "* Corelation gives strength of linear relationshiop between 2 variables and covariance gives directional relationship between 2 variables.\n",
    "2. No Multi Co linearity\n",
    "* VIF = 1 / (1 - R2)\n",
    "3. Feature Selection\n",
    "* Filter method (before model training) >> Corr , VIF , fisher-score , Annova , ChiSquare\n",
    "* Wrapper method (During Model Training) >> Forward , Backward , RFE \n",
    "* Embedded method (After Model Training) >> Lasso regression(With and without scaling)\n",
    "4. Feature Extraction\n",
    "* Principal Component Analysis(PCA)\n",
    "* Linear Discriminant Analysis(LDA)\n",
    "5. Evaluation (Confusion Matrix >> TP,TN,FP,FN)\n",
    "* Accuracy score (Used when data is balanced)\n",
    "* Precision (We try to reduce FP)\n",
    "* Recall (We try to reduce FN)\n",
    "* F1-Score (When precision and recall are both important)\n",
    "* AUC-ROC curve \n",
    "* Underfitting (High Bias and Low Variance)\n",
    "* Overfitting (Low Bias and high variance)\n",
    "* Bias and Variance trade off\n",
    "> In Logistic Regression we have threshold as 0.5 but if we have many data points on 0.4 and we want them to predict as class1 then we need to do it manually after model building.We cannot make changes in threshold while model building.Whatever changes are needed to make in threshold are made after model building manually.\n",
    "* \n",
    "> If we have Imbalanced data points ?\n",
    "1. OverSampling \n",
    "* Random Over Sampler \n",
    "* SMOTE (KNN algorithm)\n",
    "2. UnderSampling\n",
    "* Random Under Sampler \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN algorithm\n",
    "> Scaling \n",
    "* Normalization(MinMaxScalar) : (0 to 1) >> Impacted by outliers \n",
    "* Standardization(StandardScalar) : (-3 to +3) >> Not impacted by outliers \n",
    "* Lazy Algorithm \n",
    "* Storing the data \n",
    "* Distance Based Algorithm \n",
    "* Not preferable for large data sets \n",
    "* \n",
    "# Decision Tree Algorithm \n",
    "* We select the root node.\n",
    "* Attribution Selection measure \n",
    "1. Entropy (0 to 1) >> Binary classification \n",
    "2. Gini Index (0 to 0.5) >> Binary classification \n",
    "* We will calculate Information gain.\n",
    "* Information gain : E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a major issue for Decision Tree \n",
    "* For this purpose we do \n",
    "> Hyper paramater tuning \n",
    "* max_depth\n",
    "* min_samples_leaf \n",
    "* min_samples_split \n",
    "* n_estimators \n",
    "* criterion \n",
    "> pruining (Cutting Branches)\n",
    "* Post pruining >> CCP alpha\n",
    "* Pre pruining >>\n",
    "> Feature selection\n",
    "* Decision tree feature importance\n",
    "* \n",
    "# Random Forest Algorithm\n",
    "> Working \n",
    "* We select the root node.\n",
    "* Attribution selection measures\n",
    "1. Entropy (0 to 1) >> Binary classification\n",
    "2. Gini Index(0 to 0.5) >> Binary classification\n",
    "* We will calculate Information gain\n",
    "* Information gain : E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a major issue for Random Forest algorithm\n",
    "> Ensemble techniques\n",
    "> Bagging \n",
    "* Parallel approach (Bootstrap aggregation)\n",
    "* Row Sampling (Randomly)\n",
    "* OOB SAmples (Out of bag samples)\n",
    "> Boosting \n",
    "* Sequential approach\n",
    "> Difference betweeen Bagging and Boosting \n",
    "> Bagging \n",
    "* Creates a team of diverse learners from random data samples.\n",
    "* Reduces variance (Wont be overfit easily)\n",
    "* Good for high variance models like Decision Tree.\n",
    "* Bagging for less Overfitting\n",
    "> Boosting \n",
    "* Build learners one by one , focusing on the mistakes made by the previous learner.\n",
    "* Reduces Bias (Better at capturing patterns)\n",
    "* Good for high biased models like Linear Regression.\n",
    "* Boosting for better accuracy.\n",
    "> Hyperparameter tuning\n",
    "* max_depth \n",
    "* min_sample_leaf\n",
    "* min_sample_split\n",
    "* n_estimators\n",
    "* criterion \n",
    "* max_features >> Sqrt (n_features)\n",
    "* bootstrap = True\n",
    "* oob_score = True\n",
    "> Feature selection\n",
    "* Random Forest feature importance\n",
    "* \n",
    "# Adaboost algorithm \n",
    "> Boosting >> Sequential approach \n",
    "> Decision stump (one root node an 2 or more leaf node)\n",
    "* Weak learners conversion into strong learners.\n",
    "1. Sample Weight \n",
    "2. Model Train \n",
    "3. Total Error : No of misclassified samples / Total Samples\n",
    "4. Performance : 1/2 log(1 - Total error)\n",
    "5. New sample weight \n",
    "* New sample weight for correctly classified samples = old sample weight * e^-performance \n",
    "* New sample weight for wrongly classified samples = old sample weight * e^performance \n",
    "6. Normalization \n",
    "7. Create Buckets \n",
    "8. Create new data set \n",
    "> Hyperparameter tuning (Cross Validation)\n",
    "* n_estimators (default = 50)\n",
    "* learning_rate (alpha) >> Contribution of each and every model in your results.\n",
    "> Feature Importance\n",
    "* AdaBoost feature importance \n",
    "* \n",
    "# Naive Bayes Algorithm\n",
    "* Works on Bayes theorem\n",
    "* Performs well on high dimensional data\n",
    "1. Gaussian >> Continuous ( Data should be normally distributed)\n",
    "2. Mutlinomial \n",
    "3. Bernoulli \n",
    "> How to check normality?\n",
    "1. kdeplot \n",
    "2. QQ plot \n",
    "3. Hypothesis testing \n",
    "* Shapiro test \n",
    "* kstest \n",
    "* normal test \n",
    "4. Skewness (skew = 0 >> Normally distributed)\n",
    "* \n",
    "# Support Vector machine (SVM) Algorithm \n",
    "* Performs well on high dimensional data \n",
    "> Advanatage \n",
    "* It performs well for linearly seperable data and non linearly seperable data.\n",
    "* Works well for low dimensional data \n",
    "* \n",
    "* We convert low dimensional data into high dimensional data.\n",
    "> Kernals \n",
    "* RBF \n",
    "* Linear \n",
    "* Polynomial \n",
    "* Sigmoid \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Lecture \n",
    "> In Industry Target feature is not easily available. We need to identify among all features , if not then we need to create target features.We need to create clusters.\n",
    "* Business Analyst will help identify the target feature.\n",
    "> Suppose we are considering Logistic regression for a dataset and we have outliers in the dataset.\n",
    "> How to check for outliers?\n",
    "* IQR \n",
    "* Z-SCore \n",
    "* Boxplot \n",
    "* Scatterplot \n",
    "> When to use which method to detect outliers?\n",
    "* For normally distributed data we use Z-SCORE method and if the data is not normally distributed then we use IQR method.\n",
    "* To check data is normally distributed we use kdeplot, QQplot , Skew and also statistical method like Shapiro test , kstest , normal test , Chi-squared test , Annova , T-Test , Z-Test to check if data is normally distributed or not.\n",
    "* For statistical method we check p_val.If p_val >= 0.05 then Null Hypothesis is True and data is normally dustributed.If p_val < 0.05 then Alternate hypothesis is True and data is not normally distributed.\n",
    "> Type 1 error : False positive : Negative class got wrongly predicted as positive.\n",
    "* WHen True null hypothesis is rejected.\n",
    "> Type 2 error : False Negative : Positive class got wrongly predicted as Positive.\n",
    "* When False hypothesis is selected.\n",
    "* When we accept null hypothesis but it is False.\n",
    "> Precision : TP / (TP + FP)\n",
    "* WE try to reduce FP \n",
    "* Highest value of preicision is 1 and lowest is 0.\n",
    "* Precision should be high as possible.\n",
    "> Recall : TP / (TP + FN)\n",
    "* We try to reduce FN \n",
    "* \n",
    "> Back to Outliers \n",
    "* In Bell shaped curve values after 3 standard deviations are consirded as extreme outliers.\n",
    "> IQR \n",
    "* Formula for IQR : Q3 - Q1 \n",
    "* Lower Tail : Q1 - 1.5*IQR \n",
    "* Upper Tail : Q3 + 1.5*IQR\n",
    "> Z-Score \n",
    "* Z-Score : (Xi - Xmean) / Standard Deviation\n",
    "* We will calculate Z-Score for each feature of a particular feature and then we will pass threshold and then we can easily detect outliers.\n",
    "> How to handle outliers?\n",
    "1. Delete observations\n",
    "2. Imputations : Mean ,Median , Mode , Knn Imputer(replace outliers with Nan Values) , upper tail , lower tail , static value , Zero \n",
    "3. Transformations : Squareroot , Cuberoot , log , reciprocal , Boxcox transformation(Skewed data with positive values)\n",
    "* \n",
    "> Encoding ( Categorical features)\n",
    "1. One Hot encoding (Nominal data)\n",
    "2. Label encoding (Ordinal data) >> High medium low \n",
    "* \n",
    "> Important terms \n",
    "1. Sigmoid Function / Logistic function\n",
    "* Formula : 1 / (1 + e^-Y)\n",
    "* Value lies between 0 and 1\n",
    "* It is a inverse if logit function\n",
    "> Logit function : Log of Odd ratio \n",
    "> Odd retio : probability of success divided by probability of failure\n",
    "* Sigmoid function : P(Y) = 1 / (1 + e^-Y)\n",
    "* Logit function : log(P / (1 - P))\n",
    "* Logit function is used to convert the output in Non Binary format.\n",
    "2. Cost function : Log-Loss function\n",
    "* Log-Loss function : -1/N summation[Yi * log(Pi) + (1 - Yi) * log(1 - Pi)]\n",
    "3. Gradient descent Algorithm\n",
    "4. One over Rest (Multi class classification)\n",
    "5. Regularization (L1 Lasso , L2 Ridge)\n",
    "> Assumptions \n",
    "1. Linearity (Continous Independent variable with Log(Odds))\n",
    "* Variance , Covariance ,Standard deviation \n",
    "* Coefficient of Corelation(R) : Covariance / product of standard deviation \n",
    "* R gives strength of linear relationship between 2 variables and Covariance gives directional relationship between 2 variables.\n",
    "2. No Multi co Linearity \n",
    "* VIF = 1 / (1 - R2)\n",
    "3. Feature selection \n",
    "* Filter method (BEfore model training) >> corr , fisher score , Annova , Chisquare \n",
    "* Wrapper method (During model training) >> Forward , Backward , RFE \n",
    "* Embedded method (After model training) >> Lasso regression(With and without scaling)\n",
    "4. Feature Extraction\n",
    "* Principal Component Analysis(PCA)\n",
    "* Linear Dicriminant Analysis (LDA)\n",
    "5. Evaluation (Confusion Matrix : TP, TN, FP , FN)\n",
    "* Accuracy score (Used when data is balanced)\n",
    "* Precision ( We try to reduce FP)\n",
    "* Recall (We try to reduce FN)\n",
    "* F1 score (Used when precision  and recall are equally important)\n",
    "* AUC - ROC curve \n",
    "* Underfitting (High Bias and Low Variance)\n",
    "* Overfitting (Low Bias and High Variance)\n",
    "* Bias-Variance tradeoff\n",
    "> In logistic regression we have threshold as 0.5 but if we have many data points at 0.4 and you want to classify them as Class1 classification we need to do it manually after model building. We cannot make changes in threshold while model building.Whatever changes are needed to make in threshold need to be made after model building manually.\n",
    "* \n",
    "> If we get Imbalanced data points?\n",
    "1. Oversampling \n",
    "* Random Over sampler \n",
    "* SMOTE (KNN Algorithm)\n",
    "2. UnderSampling \n",
    "* Random Undersampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN algorithm\n",
    "> Scaling \n",
    "* Normalization(MinMaxScalar) : (0 to 1) >> Impacted by Outliers \n",
    "* Standardization(StandardScalar) : (-3 to +3) >> Not impacted by Outliers\n",
    "* Lazy Algorithm \n",
    "* Storing the data \n",
    "* Distance based algorithm \n",
    "* Not preferable for large data sets.\n",
    "* \n",
    "# Decision Tree Algorithm \n",
    "> Working \n",
    "* We select the root node.\n",
    "* Attribution selection measure \n",
    "1. Entropy (0 to 1) >> Binary classification \n",
    "2. Gini Index (0 to 0.5) >> Binary classification \n",
    "* We will calculate Information Gain \n",
    "* Information Gain : E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a majot issue for Decision Tree algorithm\n",
    "* For this purpose we do\n",
    "> Hyperparameter tuning \n",
    "* max_depth\n",
    "* min_sample_leaf \n",
    "* min_sample_split\n",
    "* n_estimators\n",
    "* Criterion \n",
    "> pruining (Cutting Branches)\n",
    "* Post pruining >> CCP alpha \n",
    "* Pre pruining \n",
    "> Feature selection \n",
    "* Decision Tree feature importance \n",
    "* \n",
    "# Random Forest Algorithm \n",
    "> Working \n",
    "* We select the root node.\n",
    "* Attribution selection measure \n",
    "1. Entropy (0 to 1) >> Binary classification \n",
    "2. Gini Index (0 to 0.5) >> Binary classification \n",
    "* We will calculate Information Gain \n",
    "* Information Gain = E(S) - summation(weighted average * entropy of each sample)\n",
    "> Overfitting is a major issue for random forest \n",
    "> Ensemble techniques \n",
    "> Bagging \n",
    "* Parallel Approach (Bootstrap Aggregation)\n",
    "* Row Sampling  (Randomly)\n",
    "* OOB Samples (Out of Bag samples)\n",
    "> Boosting \n",
    "* Sequential approach\n",
    "> Difference between Bagging and Boosting\n",
    "> Bagging\n",
    "* Creates a team of diverse learners from random data samples.\n",
    "* Reduces Variance (wont easily overfit)\n",
    "* Good for high variance models like Decision Tree\n",
    "* Bagging for less overfitting.\n",
    "> Boosting\n",
    "* Builds learners one by one , focusing on the mistakes made by the previous learner.\n",
    "* Reduces Bias ( Better at capturing patterns)\n",
    "* Good for High Biased models like Linear Regression.\n",
    "* Boosting for Better accuracy.\n",
    "> Hyperparameter tuning\n",
    "* max_depth\n",
    "* min_samples_leaf\n",
    "* min_samples_split\n",
    "* n_estimators\n",
    "* criterion\n",
    "* max_features >> sqrt(n_features)\n",
    "* bootstrap = True\n",
    "* oob_score = True \n",
    "> feature selection\n",
    "* Random Forest feature importance \n",
    "* \n",
    "# Adaboost algorithm \n",
    "> Boosting > Sequential approach\n",
    "> Decision stump (one root node and two or more leaf nodes)\n",
    "* Weak learners conversion into strong learners.\n",
    "1. Sample Weight \n",
    "2. Model Training\n",
    "3. Total error : No of Misclassified samples / Total samples \n",
    "4. Performance : 1/2 log(1 - Total error)\n",
    "5. New sample weight\n",
    "* New sample weight for correctly classified samples = old sample weight * e^-performance \n",
    "* New sample weight for wrongly classified samples = pld sample weight * e^performance \n",
    "6. Normalization \n",
    "7. Create Buckets \n",
    "8. Create new dataset \n",
    "> Hyperparameter tuning \n",
    "* n_estimators (default = 50)\n",
    "* learning_rate (alpha) >> Contribution of each and every model in your results.\n",
    "> Feature selection \n",
    "* Adaboost feature importance \n",
    "# Naive Bayes Algorithm \n",
    "* Works on bayes theorem \n",
    "* Performs well on High Dimensional data.\n",
    "1. Gaussian >> Continous (Data should be normally distributed)\n",
    "2. Multinomial \n",
    "3. Bernoulli \n",
    "> How to check normality?\n",
    "1. Kdeplot \n",
    "2. QQ plot \n",
    "3. Hypothesis testing \n",
    "* Shapiro test \n",
    "* Kstest \n",
    "* Normal test \n",
    "4. Skewness (Skew = 0 >> Normally distributed)\n",
    "*\n",
    "# Support Vector Machine(SVM) Algorithm \n",
    "* Performs well on high dimensional data.\n",
    "> Advantages \n",
    "* It performs well for linearly seperable data and Non linearly seperable data.\n",
    "* Low dimensional data works well.\n",
    "* \n",
    "* We convert low dimensional data into high dimensional data.\n",
    "> Kernals \n",
    "* RBF\n",
    "* Linear \n",
    "* Polynomial \n",
    "* Sigmoid \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
