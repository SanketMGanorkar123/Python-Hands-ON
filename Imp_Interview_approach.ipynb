{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whichever company has called for interview search about its history wether it is product based or service based Industry.\n",
    "> Why projects actually exist?\n",
    "1. Time management\n",
    "2. Automation\n",
    "3. Process management\n",
    "4. Revenue Generation\n",
    "5. Revenue loss optimization\n",
    "> Proof of concept > POC > Paid or unpaid duration of POC > 2-3 months , 6 months , 1 year\n",
    "* \n",
    "> In POC we will get problem statement / Business problem / Business requirements / What was the business scenario\n",
    "* Approach towards project > Python > Machine Learning > NLP > Deep Learning > Time series analysis.\n",
    "> Product based companies have their own database and we can retrieve data from the database.Who are the people involved in the project?\n",
    "1. Project Manager - 01 \n",
    "2. Team Leader - 01\n",
    "3. Business Analyst - 01\n",
    "4. Data Engineer - 01 or 02 >> Data Engineer fetches data from the database.\n",
    "5. Data Scientist - 01 \n",
    "6. ML Engineer - 01 \n",
    "7. Python Developer - 01 >> For web framework(Flask , Django , FastApi , gRPC)\n",
    "8. Power Bi developers - 01 >> Data Visualization\n",
    "9. Devops Engineer - 01 or 02 >> For project deployment(Cloud platform : AWS ,Azure(Azure ML),GCP)\n",
    "10. Front End Developer : UX Designer >> Create Design\n",
    "                        : UI Developer >> Integrate API \n",
    "* \n",
    "> Project Planning\n",
    "* Project Planning tools \n",
    "1. JIRA \n",
    "2. Zoho \n",
    "3. Trello \n",
    "4. Asana \n",
    "> Pipeline : In Industry we are getting data seperate for training and seperate for testing.We will write single functions in which we carry out necessary steps (EDA ,Feature Engineering,Feature Selection) and then we call the same function for testing data.So in this way we are creating a pipeline for all steps.\n",
    "> Docker : It is used for creating Pipeline(Flow/COnnection).It Works as a ship carrying different containers.\n",
    "* Generally when we create a model and deploy it , we will do some observations.In Observations we will see how our model is performing on new data and we will monitor this process.This process is called as CICD pipeline(Continous Integration and Continous Development).\n",
    "* During call with Hr ask him about the Job description(JD).\n",
    "* 1st round is generalized round (Introduction ,end of the introduction will deicde the further interview)\n",
    "> Project \n",
    "* First step is project planning and Timeline.So for this purpose we use project planning tools (JIRA).\n",
    "* In Big Industry we will use JIRA and in small scale Industry we will use Excel file for project planning.\n",
    "> Kickoff meeting : The first meeting after the project is assigned we will discuss project agenda , project team , introduction. OnBoarding of project / Project Kick Off \n",
    "* CLient Meet \n",
    "* Project problem discussion\n",
    "* PPT / Deck \n",
    "* Main Work \n",
    "* Project Management : JIRA , ZOHO , Trello , Asana \n",
    "> Project Duration / Project Timeline >> It will be decided by higher authorities (Project Manager , Team Lead , Technical Lead)\n",
    "* \n",
    "> VDE (Virtual desktop Environment)\n",
    "1. Local System(Laptop / Computer) : System available with you physically and you are working on it.\n",
    "2. Virtual System(VmWare) : System where you can login online and it has interface as Local System.\n",
    "> While scheduling the interview with Hr tell him let me check my calendar wall and then we ill schedule the interview.\n",
    "> ScrumMaster(Big Companies) : Person who handles the JIRA software gives updates.In Small companies Team Lead will be the Scrum Master.\n",
    "* \n",
    "> Epic(JIRA) >> Project Plan >> Suppose 5 months \n",
    "* Data Gathering \n",
    "* EDA \n",
    "* Feature Engineering\n",
    "* Feature selection\n",
    "* Model Building\n",
    "> Subpart of Epic >> Sprint(1 Week or 2 Weeks)\n",
    "* Within SPrint we will have subparts as story1 and story2.Supose we have Story1 as Data Gathering and Story2 as EDA.\n",
    "* We can have multiple sprints.And in that sprint we can have multiple stories.\n",
    "> From where do we get the data?\n",
    "* We will get the data from different databases(MySQL, postgresql, MongoDB(NoSql), Company database(Product based) , Client database , CloudS3 (Sagemaker) , Data Engineer) in the form of CSV ,Excel , JSON , text.\n",
    "* Data Engineer will fetch the data from databases and will send to us by mail on CLient Mailid which is shared with us.\n",
    "> Web Scrapping (Sentiment analysis , NLP , movie reviews) for data.\n",
    "* Sometimes the client can only provide the feature names.For data we can use Kaggle or other resources.\n",
    "> Synthetic data Generation >> Hugging Face \n",
    "* \n",
    "> DataBase access : Read and write access \n",
    "* \n",
    "> Environment access(DOne After project completion) \n",
    "1. Production Environment \n",
    "2. Testing Environment (Sandbox)(UAT)\n",
    "* First we will deploy our model on Testing Environment and after we are convinced it is performing good then we will deploy it on Production Environment.\n",
    "* \n",
    "> What was your data size?\n",
    "* 0.5 million to 2 million >> rows\n",
    "* 10 to 40 >> Features \n",
    "> Features \n",
    "1. Original features\n",
    "2. Derived features  >> New features that can be introduced.\n",
    "> Celonis : Process Mining tool , it uses a vertica sequel same as SQL.\n",
    "> How frequently was your data was updating?\n",
    "* Suppose we have 100000 rows and 25 features and the data was updating to 200000 rows and 30 features.\n",
    "> How frequently was your meeting with your clients?\n",
    "* Weekly , 15 Days , Biweekly , Alternate days \n",
    "> What was your role in CLient meeting?\n",
    "* As a Data Scientist i would discuss technical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After creating resume when we upload the resume on Naukri\n",
    "1. Hr Dynamic >> Resume >> Naukri >> Call Hr >> Discussion >> Interview schedule \n",
    "2. InCompany \n",
    "* After joining they will use you as a shadow resource.\n",
    "> Shadow resource : Some part of existing work will be allocated to you.\n",
    "> Probation Period >> 3 to 6 months (Temporary) >> After 3 to 6 months we will be taken as On Role in Industry.\n",
    "* New project >> Team Finalize >> Internal Meeting : Introduction to team Members >> Project Kickoff : With Client >> OnBoarding >> VM Credentials >> Userid , Password , Token Generation >> Project Management tools >> JIRA >> NDA (Non Disclosure agreement)\n",
    "* List given to client for required features.\n",
    "* Then the client will add the required features in Database.\n",
    "* We will get the required features from different tables and after joining these tables we will get a MAsterTable or MasterData.\n",
    "* MasterTable will be given to DataScientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "> Note : EDA and Feature Engineering will be different for different Algorithms.\n",
    "* Basic EDA \n",
    "1. Data Obesrvations \n",
    "* Independence \n",
    "* Balancing \n",
    "* Anomaly Detection (Disturbance in the data >> Duplicate rows (df.drop_duplicate()))\n",
    "* Distribution of the data\n",
    "* Data Sanity >> Coefficient of correlation , VIF \n",
    "* EDA has 2 types \n",
    "> Univariate Analysis and Multivariate Analysis\n",
    "* Univariate Analysis is where only one feature is taken in consideration and in Multivariate Analysis we consider more than one feature.\n",
    "* Handling Missing values \n",
    "* Delete the Observations \n",
    "* Imputation >> Continous data >> Mean , Median , KNN Imputer (Nan Eucledian distance) , MICE\n",
    "             >> Categorical data >> Mode , KNN Imputer(K=1)\n",
    "> What is difference between Normal Distribution and Binomial distribution?\n",
    "1. Normal Distribution \n",
    "* Used in situations where the data follows a continous , symmetric distribution.\n",
    "* Commonly used in statistical interference and hypothesis testing.\n",
    "2. Binomail Distribution \n",
    "* Used when dealing with discrete set of trials with two possible outcomes.\n",
    "* Commonly used in scenarios involving binary events(Success / Failure) , such as coin flips or pass/fail situations.\n",
    "> Normal Distribution is more appropriate for continous variables , while binomial distribution is more appropriate for dicrete variables with a fixed number of trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What made you switch your career to DataScience?\n",
    "> Why you want to resign from current job?\n",
    "> What package are you expecting (In Hr Round)?\n",
    "* Whichever company has called you search about its history whterher it is a Product based ar Service based Industry.\n",
    "# Why Projects actually exist(Service Based)? \n",
    "1. Time Management \n",
    "2. Automation \n",
    "3. Process Management\n",
    "4. Revenue Generation >> Maximization\n",
    "5. Revenue Loss >> Optimization\n",
    "> Proof of Concept >> POC >> Paid or Unpaid >> Duration >> 2-3 months , 6 months , 1 Year \n",
    "* \n",
    "> In POC we get a problem statement / Business Problem / Business Requirements / What was the Business Scenario?\n",
    "* Approach towards Project > Python > Machine Learning > NLP > Deep Learning > Time Series Anlaysis \n",
    "> Product based Industry have their own Database and we can retrieve data from the Database.\n",
    "> Who are the People involved in the Project?\n",
    "1. Project Manager - 01 \n",
    "2. Team Leader - 01 \n",
    "3. Business Analyst - 01 \n",
    "4. Data Engineer - 01 or 02 >> Data Engineer fetched data from DataBase.\n",
    "5. Data SCientist - 01 \n",
    "6. ML Engineer - 01 \n",
    "7. Python Developer - 01 >> For Web Framework(Flask , Django , FastAPI , gRPC)\n",
    "8. PowerBI Engineer - 01 >> For Data Visualization \n",
    "9. Devops Engineer - 01 or 02 >> For Project Deployment (Cloud Platform : AWS , Azure(Azure ML), GCP)\n",
    "10. Front End Developer - 01 >>\n",
    "* UX Designer >> Creates Design\n",
    "* UI Developer >> Integrates API \n",
    "> Project Planning \n",
    "* Project Planning Tools \n",
    "1. JIRA \n",
    "2. ZOHA \n",
    "3. TRELLO \n",
    "4. Asana \n",
    "> Pipeline : In Industry we are getting seperate data for Training and separate data for testing. We will write single functions in which we carry out necessary steps(EDA , Feature Engineering , Feature Selection) and then we will call the same function for testing data.So in this way we are creating a pipeline(flow) for all steps. \n",
    "> Docker : It is used for creating the pipeline(Flow/Connection).It works as ship carrying different Containers.\n",
    "* Generally when we create a model and deploy it we will do some Observations.In Observations we will see how our model is performing on new data and we will monitor this process.This Process Is called as CICD pipeline(Continous Integration and Continuous Development)\n",
    "* During call with Hr ask him about the Job Description.\n",
    "* 1st round is generalized round (Introduction , end of the instruction will decide the further interview)\n",
    "> Project \n",
    "* First Step is project planning and Timeline.So for this purpose we use project planning tools(JIRA).\n",
    "* In Big Industries we will use JIRA and in small companies we will use Excel file for Project Planning.\n",
    "> Kickoff Meeting : First meeting when the project is assigned , we will discuss project agenda , project team , introduction. Onboarding of Project / Project Kickoff \n",
    "* Client Meet \n",
    "* Project Problem Discussion\n",
    "* PPT / Deck\n",
    "* Main Work \n",
    "* Project Management : JIRA , ZOHO ,TRELLO , Asana \n",
    "> Project Duration / Project Timeline >> It will be decided by the higher authorities(Project Manager , Team Lead , Technical Lead)\n",
    "* \n",
    "> VDE (Virtual Desktop Environment) :\n",
    "1. Local System(Laptop / COmputer) : System available with you physiaclly and you are working on it.\n",
    "2. Virtual System(VMware) : System where you can login online and it has interface same as local system.\n",
    "> While scheduling the interview with Hr tell him let me check my calendar wall and then we will schedule the interview.\n",
    "> Scrum Master(Big Companies) : Person who handles the JIRA software and give updates. In small companies team lead will be the Scrum Master.\n",
    "* \n",
    "> Epic(JIRA) >> Project Plan >> Suppose 5 months \n",
    "* Data Gathering \n",
    "* EDA \n",
    "* Feature Engineering\n",
    "* Feature selection\n",
    "* Model Building\n",
    "> Subpart of Epic >> SPrint (1 Week or 2 Weeks) : \n",
    "* Within sprint we will have subparts as story1 and story2.Suppose we have story1 as data gathering and story2 as EDA.\n",
    "* We can have mutiple sprint . And in that sprint we can have multiple stories.\n",
    "* \n",
    "> From where do we get the data?\n",
    "* We will get data from different databases(MySql , PostgreSQL , MongoDB(NoSQL) , Company Database(Product based) , Cient Database , CLoudS3 Bucket(Sagemaker) , Data Engineer) in the form of CSV ,EXcel , JSON , Text.\n",
    "* Data Engineer fetches data from the database and will send it to us by mail on CLient emailid.\n",
    "> Web Scrapping (Sentiment Analysis , NLP , Movie reviews) for data.\n",
    "* Sometimes the client can provide only feature names.For data we can use Kaggle or other resources.\n",
    "> Synthetic data Generation : Hugging face \n",
    "* \n",
    "> DataBase access : Read and write access \n",
    "* \n",
    "> Environment access(Done after project completion)\n",
    "1. Production environment \n",
    "2. Testing environment(Sandbox) \n",
    "* First we will deploy our model on Testing environment and after it is performing Good we will go for Production Environment.\n",
    "> What was your data Size?\n",
    "* 0.5 Million to 2 million >> Rows \n",
    "* 10 to 40 >> Features \n",
    "> Features \n",
    "1. Original Features \n",
    "2. Derived Features >> New features that we can introduce \n",
    "> Celonis : Process Mining tool , it uses vertica sequel same as sql .\n",
    "> How frequently was your data is updating?\n",
    "* Suppose we have 100000 rows and 25 features and the data may update to 200000 rows and 30 features.\n",
    "> How Frequently was your meeting with Clients?\n",
    "* Weekly , 15 days , Biweekly , alternate days \n",
    "> What was your role in CLient Meeting?\n",
    "* As a Data Scientist i would discuss more technical issues with the project.\n",
    "* \n",
    "# After creating resume when we upload resume on Naukri \n",
    "1. Hr Dynamic >> Resume >> Naukri >> Call with Hr >> Discussion >> Interview schedule \n",
    "2. In Company >\n",
    "* After joining >> They will use you as a shadow resource at the beginning.\n",
    "> Shadow Resource : Some part of the existing work will be alocated to you.\n",
    "> Probabtion period : Starting period >> 3 to 6 months >> After that we will be taken On role in Industry.\n",
    "* New project >> Team FInalize \n",
    "              >> Internal Meeting : Introduction to team members \n",
    "              >> Project Kickoff : with client \n",
    "              >> On Boarding >> VM credentials >> Userid , password , token generation \n",
    "              >> Project management : tools >> (JIRA)\n",
    "              >> NDA >> Non Disclosure Agreement\n",
    "* List >> Required features from the client \n",
    "* Then the client will add the required features in the database.\n",
    "* We will get the required features in the form of different tables and after joining all tables we will get the Master Table or Master Data.\n",
    "* Master table will given to the Data Scientist.\n",
    "* \n",
    "# EDA \n",
    "> Note : EDA and Feature Engineering will be different for different Algorithms.\n",
    "> Basic EDA \n",
    "1. Data Observations\n",
    "* Independence \n",
    "* Balancing \n",
    "* Anomaly detection (disturbance in the data >> Duplicate rows(df.drop_duplicates()))\n",
    "* Distribution of the data \n",
    "* Data Sanity >> Coefficient of correlation , VIF \n",
    "* EDA has 2 types \n",
    "> Univariate analysis and multivariate analysis \n",
    "* Univariate analysis is where we consider only one feature and in multivariate analysis we will consider more than one feature.\n",
    "* Handling Missing values\n",
    "* Delete the obseravations \n",
    "* Imputation >> Continous data >> mean , median , knn imputer(Nan Eucledian distance),MICE\n",
    "             >> Categorical data >> mode , KNn imputer(k=1) \n",
    "> What is the difference between normal distribution and Binomial distribution?\n",
    "1. Normal Distribution \n",
    "* Used in situations where the data follows continous , symmteric distribution \n",
    "* Commonly used in statustical interference and hypothesis testing\n",
    "2. Binomial Distribution \n",
    "* Used when dealing with discrete set of trials with 2 possible outcomes.\n",
    "* Commonly used in Scenarios involving Binary events (Success/ Failure) , such as coin flips , or pass/Fail Situatons.\n",
    "> Normal Distribution is more appropriate for Continous variables , while Binomial distribution is more appropriate for discrete variables with a fixed number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
