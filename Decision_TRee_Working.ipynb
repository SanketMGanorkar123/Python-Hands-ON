{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree >>\n",
    "# First Case >> Independent variable >> categorical values\n",
    "#            >> Dependent variable   >> categorical values\n",
    "* First Step we have to select root node >> we have 2 options \n",
    "  > Information gain >> entropy \n",
    "  > Gini index / impurity\n",
    "\n",
    "> Information gain\n",
    "* Feature having maximum information gain amongst all features will be selected as root node.\n",
    "> formula for information gain\n",
    "> Information gain of a feature \n",
    ">> Entropy of feature - sumation[weighted average of each category * entropy of each category]\n",
    "\n",
    "* entropy >> It is a measure of impurity/uncertainity/randomness\n",
    "> formula >> entropy of each category = -prob(yes)*log2prob(yes) - prob(no)*log2prob(no)\n",
    "* After calculating Information gain of each feature from the dataset we will select the maximum information gain\n",
    "* as our root node.\n",
    "\n",
    "> Gini Index >> It is a measure of impurity\n",
    "> formula for gini index of a feature>>\n",
    "> gini index of a category = 1 - prob(yes)2 - prob(no)2\n",
    "> gini index of a feature = sumation[weighted average of each category * gini index of each category]\n",
    "\n",
    "# Second Case >> Independent variable >> continous values\n",
    "#             >> Dependent variable   >> categorical values\n",
    "\n",
    "* we will divide the feature having continous values in different thresholds(sort the values in independent  variable * in ascending order)\n",
    "* example >> 1,2,3,4,5,6,7,8,9,10  threshold for 1,2 will be 1.5 ,for 2,3 will be 2.5 and likewise for rest\n",
    "* for above example we will have 9 thresholds\n",
    "* we will calculate Information gain for each threshold \n",
    "* Maximum Information gain among all thresholds will be selected the information gain for that feature.\n",
    "* Likewise all information gain values of all features will be calculated.\n",
    "* Maximum Information gain among all features will be selected as root node.\n",
    "* In case of Gini Index gini index of each thresholds will be calculated.\n",
    "* Minimum Gini index among all thresholds will be selected as gini index of that feature.\n",
    "* Likewise all gini index of all features will be calculated.\n",
    "* Minimum Gini index among all features will be selected as root node.\n",
    " \n",
    "\n",
    "# Third Case >> Independent variable >> continous values\n",
    "#             >> Dependent variable >> continuous values\n",
    "* As we have continous values in target column we cannot use entropy,gini index to find out the root node.\n",
    "* Suppose we have plotted our values on graph where we have age on x-axis and cibil score on y-axis.\n",
    "* we will use a splitter to divide the data into 2 parts .\n",
    "* values on the left side of the splitter to be predicted will be the mean of the values on the left side of splitter.\n",
    "* same goes for the right side of the splitter\n",
    "* How to find the Best Splitter ?\n",
    "* Sort the Independent variables in ascending order.\n",
    "* Calculate the threshold for each 2 data points ,Suppose we have 1,2,3,4,5,6,7,8,9,10 we will calculate threshold \n",
    "* for 1,2 then 2,3 then for 3,4 likewise we will get 9 thresholds\n",
    "* For every threshold we will calculate MSE/MAE \n",
    "* Every threshold will be consirded as splitter.\n",
    "* Data points on either side of the splitter will be predicted by mean of the data points on that side of the splitter.\n",
    "* So our mean will be the predicted value and we will also have actual values .\n",
    "* So we will calculate MSE for Each threshold or splitter for that feature.\n",
    "* Minimum MSE amongst all thresholds will be selected as the splitter or best threshold for that feature.\n",
    "* In this way we will calculate Best MSE for each feature and minimum MSE amongst all features wiil be selected as the root node .\n",
    "\n",
    "\n",
    "# Fourth Case >> Independent variable >> categorical values\n",
    "#            >> Dependent variable  >> continuous values\n",
    "* As we have continous values in target column we cannot use entropy,gini index to find out the root node.\n",
    "* Suppose we have have 2 categories as independent variables male and female\n",
    "* we will split the data by splitter in 2 parts \n",
    "* Take the mean of all data points on either side of the splitter \n",
    "* We will calculate MSE as we have actual values and predicted value will be the mean of all data points on either side of the splitter.\n",
    "* Now we have MSE value of that feature.\n",
    "* If this value of MSE is lowest amongst all features it will be selected as root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages >> \n",
    "1. No Assumptions\n",
    "2. No Scaling \n",
    "3. Not Sensitive to outliers\n",
    "\n",
    "* Disadvantages >>\n",
    "1. Overfitting\n",
    "2. Unstable \n",
    "\n",
    "* How to handle overfitting ?\n",
    "1. Hyperparameter tuning\n",
    "2. Pruning \n",
    "3. Use Ensemble technique \n",
    "\n",
    "> Ensemble Technique >> Combining multiple models to avoid overfitting\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "\n",
    "1.Bagging >> random forest >> parallel approach\n",
    ">> Bagging >> Bootstrapping + aggregation\n",
    "* suppose we have 1000 rows \n",
    "* In bootstrapping if we want to have 5 different data sets from the original data set keeping the no of rows \n",
    "  as original data and also data in 5 different data sets to be unique .\n",
    "* After this we will apply decision tree algorithm to each 5 data sets.\n",
    "* We will get output from each decision tree model and we will have 5 different outputs.\n",
    "* In case of classification we will consider voting classifier >> majority wins\n",
    "* In case of regression we will consider mean of voting classifier\n",
    "\n",
    "2.Boosting >> AdaBoost \n",
    "           >> gradient boost     >>  sequential approach \n",
    "           >> XG Boost\n",
    "           >> Cat Boost\n",
    "* In Boosting  we will have sequential approach which says that our dataset will be apllied on different algorithms \n",
    "  sequentially .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised machine learning algorithm\n",
    "* used for both classification and regression\n",
    "\n",
    "* Decision node >\n",
    "1. Internal nodes >> feature of datasets\n",
    "2. Branches >> Decision rule\n",
    "3. Leaf >> Outcome\n",
    "\n",
    "* Root node \n",
    "* ASM >> Attribute selection measure\n",
    "1. Entropy\n",
    "2. Gini Index / Gini Impurity\n",
    "\n",
    "\n",
    "> Working of Decision TRee\n",
    "> First case >> Independent variable >> categorical variable\n",
    "             >> Dependent variable   >> categorical variable\\\n",
    "* First step we have to select root node >> we have 2 options\n",
    "* Information gain >> entropy,gini index\n",
    "* feature having maximum information gain amongst all features will be selected as root node.\n",
    "* Formula for information gain of a single feature \n",
    "> Entropy of a single feature - sumation[weighted average of each category * entropy of each category]\n",
    "* entropy >> It is a measure of impurity/uncertainty/randomness\n",
    "* Formula for entropy of each category is >\n",
    "> entropy of each category = -prob(yes)*log2prob(yes) - prob(no)*log2prob(no)\n",
    "* After calculating information gain of each feature,maximum information gain amongst all features will be selected as root node.\n",
    "* Gini index >> it is a measure of impurity .\n",
    "* Formula for gini index of a category \n",
    "> 1 - prob(yes)2 - prob(no)2\n",
    "* formula for gini index of a feature\n",
    "> sumation[weighted average of each category * gini index of each category]\n",
    "\n",
    "\n",
    "> Second case >> Independent variables >> continous values \n",
    "              >> Dependent variables   >> categorical values\n",
    "* we will sort the values in independent variable in ascending order.\n",
    "* we will divide the feature having continous values in different thresholds.\n",
    "* Example >> 1,2,3,4,5,6,7,8,9,10 for this values we will have thresholds for 1,2 as mean of 1,2 which will be 1.5\n",
    "  for 2,3 threshold will be 2.5 and likewise for others \n",
    "* For 10 independent variables we will have 9 thresholds \n",
    "* We will calculate Information gain for each threshold\n",
    "* Maximum Information gain among all thresholds will be selected as information gain of that feature.\n",
    "* Maximum Information gain of a feature among all other information gain will be selected as root node.\n",
    "* In case of gini index ,gini index of each threshold will be calculated.\n",
    "* Minimum gini index among all thresholds will be selected as gini index of that feature.\n",
    "* likewise all gini index of all features will be calculated.\n",
    "* minimum gini index among all features will be selected as root node.\n",
    "\n",
    "\n",
    "> Third case >> Independent variables >> continuous values\n",
    "             >> dependent variable    >> continuous values\n",
    "\n",
    "* As we have continous values in target column we cannot use entropy,gini index to find out the root node.\n",
    "* Suppose we have plotted our values on graph where we have age on x-axis and cibil score on y-axis.\n",
    "* we will use splitter to divide the data in two parts.\n",
    "* values on the left side of the splitter to be predicted will be the mean of the values on the left side of the     splitter,same will work for the right side of the splitter.\n",
    "* How to find the best splitter?\n",
    "* Sort the independent variables in ascending order .\n",
    "* Calculate the threshold for each 2 data points ,suppose we have 1,2,3,4,5,6,7,8,9,10 we will calculate threshold \n",
    "  for 1,2 then 2,3 then for 3,4 likwise we will get 9 thresholds for 10 data points.\n",
    "* For every threshold we will calculate MSE/MAE\n",
    "* Every threshold will be considered as splitter \n",
    "* Data points on left side of the splitter will be predicted by mean of all data points on the left side of the splitter.\n",
    "* So our mean will be the predicted value for either side of the splitter and we will also have actual data points.\n",
    "* So we will calculate MSE/MAE for every threshold or splitter.\n",
    "* Minimum MSE amongst all thresholds will be selected as the best splitter or best threshold for that feature.\n",
    "* In this way we will calculate minmum MSE amongst all features .\n",
    "* Minimum MSE among all features will be selected as root node.\n",
    "\n",
    "\n",
    "> Fourth case >> Independent variable >> categorical values\n",
    "              >> Dependent variable   >> continuous values\n",
    "\n",
    "* As we have continous values in dependent variable we cannot calculate entropy,gini index\n",
    "* Suppose we have 2 categories as independent variables male and female\n",
    "* we will split the data by splitter in 2 parts\n",
    "* Take the mean of all data points on either side of the splitter\n",
    "* We will calculate MSE as we have actual values and predicted value will be the mean on either side of the splitter.\n",
    "* Now we have MSE of that feature\n",
    "* If this values of MSE is lowest amongst all features it will be selected as root node.\n",
    "\n",
    "\n",
    "> Advantages \n",
    "1. No assumotions \n",
    "2. No Scaling \n",
    "3. Not sensitive to outliers \n",
    "\n",
    "> Disadvantages\n",
    "1. Overfitting\n",
    "2. Unstable \n",
    "\n",
    "> How to handle Overfitting\n",
    "1. Hyperparameter tuning\n",
    "2. Pruning\n",
    "3. Use Ensemble technique\n",
    "\n",
    "* Ensemble technique >> combining multiple models to avoid overfitting\n",
    "1. Bagging \n",
    "2. Boosting\n",
    "\n",
    "1. Bagging >> Bootstrapping + aggregation\n",
    "* Suppose we have 1000 rows in our dataset\n",
    "* In bootstrapping we will create 5(suppose) datasets from original dataset with unique values .\n",
    "* These 5 datasets each will have 1000 rows and will have unique values.\n",
    "* After this we will apply decision tree algorithm to each dataset .\n",
    "* we will get 5 outputs from each decision tree \n",
    "* In case of classification we will consider the majority class among the outputs\n",
    "* In case of regression we will consider the mean of all outputs\n",
    "\n",
    "2. Boosting >> sequential approach \n",
    "* adaboost\n",
    "* gradient boost\n",
    "* XG boost\n",
    "* In Boosting we will have sequential approach which says that our dataset will be applied on different decision tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
