{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (NAtural Language processing)\n",
    "* It is a branch of Artificial Intelligence.\n",
    "* Whenever text data will come NLP will come into picture.\n",
    "* The human capability of understanding a language will be given to model by NLP.\n",
    "# SUbSections \n",
    "1. NLU (Natural language understanding)\n",
    "2. NLG (Natural language generation) >> we generate new text \n",
    "* Example > If we have watched a movie and if somebody ask us about the movie.If we tell about the script of the movie then this is called as NLU.If we are telling about the movie in our own language then that it is called NLG.\n",
    "* If we are taking as it is from the original data then it is called as NLU and if we are generating new text from the original data then it is called as NLG.\n",
    "# Basic terms of any language\n",
    "1. Phonemes >> Smallest unit of any language ,characters , speech , sound\n",
    "2. Morphemes(words) & lexemes(run - running , swim - swimming)\n",
    "3. Syntax >> Phrases , Sentences \n",
    "4. Context \n",
    "* Meaning \n",
    "* Combination of Syntaxes \n",
    "> NLP applications\n",
    "1. Sentiment analysis >> It is a classification problem (Text classification)\n",
    "* Tweets >> Positive tweets , Negative tweets , Neutral tweets \n",
    "* Movie reviews >> Positive reviews , Negative reviews , Neutral reviews\n",
    "* In sentiment analysis we analyze the sentiment of the text data whethr it is positive , negative or neutral.\n",
    "* Sentiment analysis is quite important as we are dealing with product based or service based Industry interactions between customers is crucial and we need to understand the review of the product or service which we are providing.We should be able to understand the entiment of the text data which will be helpful for the industry to make necessary changes to satisfy the Customers.\n",
    "* Surveys , google forms , audio files\n",
    "2. Document classification \n",
    "* In this type we will get documents to classify.\n",
    "* In case of 2 documents it will be binary classification and in case of multiple documents it will be Multiclass classification.\n",
    "* Adhar card , PAN card , Driving license , Voter ID \n",
    "3. Text summarization\n",
    "* If we get 50 page data and we want to summarize it in a single page.\n",
    "> Extractive text summarization\n",
    "* In extractive text summarization we extract important lines/sentences and will return the summary of the original data.\n",
    "> Abstractive text summarization \n",
    "* Creating new summary (NLG)\n",
    "4. Topic modelling / Topic Identification \n",
    "* Suppose we have 100 documents of data and we ahve 20 documents related to sports , 30 documents related to geography and 50 documents realted to ethics and we are not aware of that.if we build a model on Topic modelling and this will find out the hidden patterns between the text from the 100 documents.\n",
    "* This model will return output as 20 documents related to 0 Class , 30 documents to 1 class and 50 documents related to 2 class.\n",
    "* And then we will ned to manually classify the headings of the documents.\n",
    "* Model will classify the documents based on similar words or sentences in different clusters.\n",
    "5. Chatbot \n",
    "* Automatic response generation\n",
    "* \n",
    "# NLP Pipeline\n",
    "1. Data Extraction \n",
    "2. EDA \n",
    "3. Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Modelling \n",
    "6. Evaluation \n",
    "7. Deployment \n",
    "* \n",
    "1. Data Extraction \n",
    "* We can get data in different data formats >> JSON , Text , CSV , Images(OCR , Pytessaract , Amazon textract , Google vision)\n",
    "> Data Types \n",
    "* Public data : easily available \n",
    "* Private data : belongs to some organiozation \n",
    "> If Data is not available \n",
    "* WE take data from Public options >> Webscrapping \n",
    "> If data is available \n",
    "* Then we will download the data \n",
    "* In this way we get the data.\n",
    "> What is noise in text data?\n",
    "* \"We >>> loved $@ /\\ # the product will ___ defientlri recommend\"\n",
    "* Noise is available in above text.\n",
    "> Quality of data\n",
    "> If there is noise in the text data then what would we do?\n",
    "* We will process on extreme data processing.\n",
    "> If there is minimal noise then what would we do?\n",
    "* Minimal data preprocessing\n",
    "* \n",
    "* After this we will get clean data.\n",
    "> Quantity of data?\n",
    "* In case of POC suppose we have 1GB of data then that is a good thing.\n",
    "* \n",
    "> Problems regarding data?\n",
    "* Quantity of data \n",
    "* Quality of data \n",
    "* Exact data / Specific data is not available for our usecase.\n",
    "* Donot have continous flow of data.\n",
    "* In Industry we will not get data in one time we will get data in cycles.\n",
    "* Montly cycle , quarterly cycle , yearly cycle \n",
    "* \n",
    "2. EDA \n",
    "> Ngram \n",
    "> Wordcloud \n",
    "> Keyphrase Extraction \n",
    "* \n",
    "> Ngram\n",
    "* Unigram \n",
    "* Bigram \n",
    "* Trigram \n",
    "* Quadragram \n",
    "> Suppose we have a example of \"Rajesh is hardworking guy\"\n",
    "* Unigram : [Rajesh , is , hardworking , guy]\n",
    "* Bigram : [Rajesh is , is hardworking , hardworking guy]\n",
    "* Trigram : [Rajesh is hardworking, is hardworking guy]\n",
    "> Why Ngram?\n",
    "* It is a part of EDA.\n",
    "* To get insights from the data(Understanding words)\n",
    "* Suppose we are considering positive reviews we will get >> Positive words by Ngram \n",
    "* Also when we are considering negative reviews we will >> Negative words by Ngram \n",
    "* \n",
    "* To get domain specific stopwords.\n",
    "* \n",
    "> Word cloud \n",
    "* If a frequency of any word is higher than the word font will be higher in that word cloud.\n",
    "* If a frequency of any word is lower than the word font will be lower in that word cloud.\n",
    "* \n",
    "> Key phrase extraction \n",
    "* To extract important keyphrase or keywords.\n",
    "* RAKE , YAKE algorithm used for key phrase extraction.\n",
    "* \"We are learning NLP\" and we apply key phrase extraction.\n",
    "* \"learning NLP\" will be important keyword.\n",
    "* \n",
    "4. Preprocessing \n",
    "> Tokenization \n",
    "* Sentence tokenization \n",
    "* Word tokenization \n",
    "* Suppose we get text \"We are learning NLP.NLP is a huge domain.\"\n",
    "* Sentence tokenization : [We are learning NLP. , NLP is a huge domain.]\n",
    "* Word tokenization : [We, are , learning , NLP , . , NLP , is , a , huge , domain , .]\n",
    "> How does sentence tokenization works or how does it knows it is a sentence?\n",
    "* It will check Syntax.\n",
    "* It will check for punctuaution marks : ! , . , ?\n",
    "* It will check for Consumptions : and , but \n",
    "* \n",
    "> Normalization \n",
    "* Suppose we have words \"G R E A T\" and another word \"great\" these 2 words have the same meaning but our model doesnot understamd them. Model will allocate number for \"G R E A T\" and another number for \"great\" which is not good.\n",
    "* So i NLP we will convert this into single case depends upon us we keep it in lowercase or Uppercase. In Industry standard parctise is to keep it in lowecase.\n",
    "* \n",
    "> Remove punctuation marks\n",
    "* We have string library for this purpose and we import punctuation.\n",
    "* In this punctuation string we have punctuation symbols.\n",
    "* And we can use this to remove punctuation.\n",
    "* \n",
    "> Remove stopwords \n",
    "* Language specific stopwords : Words in text which contributes grammatically but doenot have any impact on the sentence. >> is , has , we , him \n",
    "* Domain Specific stopwords : Words in Domain text which contributes grammatically but doenot have any impact on the sentence. >> doctor , tablet , capsule , treatment.\n",
    "* \"Rajesh is suffering from cancer.Right now Doctor Pravin is treating him.We have given him XYZ tablet.\"\n",
    "* \n",
    "> Lemmatization(lemma) & stemming(stem) : Words pruining \n",
    "* Lemma and stem are greek words.\n",
    "* lemma >> meaningful root word >> has dictionary in backend(Word net dictionary) :\n",
    "* Example >> Running >> it will check the word in word net dictionary whether it is meaningful or not >> Is there any other root word available in word net dictionary >> Output = Run\n",
    "* \n",
    "* Stem >> Root word >> aggresive pruiner : example >> running = run , swimming = swim.\n",
    "* Believe = Beli as it is a aggresive pruiner but it is wrong.\n",
    "* \n",
    "* We should use both lemma and stem and compare the results.\n",
    "* \n",
    "> Contraction mapping : Text expanding \n",
    "* didn't >> did not , doesn't >> does not , haven't >> have not \n",
    "* Supose we have review : \"I didn't like the movie\" , \"I liked the movie\" and we remove the stopwords then we will get as \"like movie\" and \"liked movie\".\n",
    "* In first case we are getting the wrong interpreatation which is not good so we are contract mapping.\n",
    "* We can remove stopwords from stopwords list.\n",
    "* \n",
    "> Handling accented characters \n",
    "* We have unidecode library to handle accented characters.\n",
    "* Accented characters >> s : $ , a : @ \n",
    "* We will convert $ to s and @ to a\n",
    "* \n",
    "> Auto correction\n",
    "* Correct speeling of words \n",
    "* We have autocorrect library and text blob library.\n",
    "* \n",
    "5. Feature Engineering \n",
    "* We convert text to numeric format/Vectors in Feature Engineering of NLP.\n",
    "* Converting text to numeric format/Vectors in Feature Engineering is called as Word Embeddings.\n",
    "> Word Embedding\n",
    "* Frequency based word embedding : It focuses on word frequency and returns in numeric format.\n",
    "* Count Vectorizer \n",
    "* TFIDF \n",
    "* \n",
    "* Prediction based word embedding : It uses algorithm in backend and converts text to numeric format.\n",
    "* Word to weight \n",
    "* Fast Text \n",
    "* Doc2ve \n",
    "* \n",
    "6. Modelling \n",
    "* We have data in Numeric format.\n",
    "* We can use all classification algorithms.\n",
    "* Logistic Regression , Naive Bayes , Random Forest , SVM , Decision Tree , AdaBoost , RNN , LSTM.\n",
    "* \n",
    "7. Evaluation \n",
    "* Confusion Matrix , Recall , Precision , accuracy score , F1-score \n",
    "* If we have low accuracy then we will jump on preprocessing or go for feature engineering.\n",
    "* Suppose we have used frequency based first then we will use prediction based for better result.\n",
    "* \n",
    "8. Deployment \n",
    "* If we get good accuracy then we will go for Deployment.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Natural Language Processing\n",
    "* It is a branch of Artificial Intelligence.\n",
    "* Whenever text data will come NLP will come into picture.\n",
    "* The Human capability of understanding language will be given to the model by NLP.\n",
    "# Subsections \n",
    "1. NLU (Natural Language understanding)\n",
    "2. NLG (Natural language generation) >> We generate new text \n",
    "* Example > If we have watched a movie and if somebody ask us about the movie. If we tell about the script of the movie as it is then it is called as NLU.If we tell about the movie in our own language then that is called as NLG.\n",
    "* If we are taking as it is from the original data then that is called as NLU and if we are generating new text from the original data then that is called as NLG.\n",
    "# Basic terms of any language \n",
    "1. Phonemes >> smallest unit of any language , characters , speech , sound \n",
    "2. Morphemes(Words) & lexemes( run : running , swim : swimming)\n",
    "3. Synatx >> Phrases ,sentences \n",
    "4. Context \n",
    "* Meaning\n",
    "* Combination of Syntaxes\n",
    "> NLP applications \n",
    "1. Sentiment analysis >> It is a classification problem (text classification)\n",
    "* Tweets >> Positive tweets , Negative tweets , Neutral tweets\n",
    "* Movie reviews >> Positive reviews , Negative reviews , Neutral reviews\n",
    "* In sentiment analysis we analyze the sentiment of the text data whether it is positive or negative or neutral.\n",
    "* Sentiment analysis is quite important as we are dealing with product based and service based Industry , Interactions between customers is crucial and we need to understand the review of the product or service which we are providing.We should be able to understand the sentiment of the text data which will be helpful for the industry to make necessary changes to satify the customers.\n",
    "* Surveys , google forms , audio files\n",
    "2. Document classification\n",
    "* In this type we will get documents to classify.\n",
    "* In case of 2 documents it will be binary classification nad in case of multiple documents we will get Multi class classification.\n",
    "* Adhar card , PAN card , Driving License , Voter ID \n",
    "3. Text Summarization\n",
    "* If we get 50 page data and we want to summarize it into a single page.\n",
    "> Extractive text summarization \n",
    "* In Extractive text summarization we extract important lines/sentences and wil return the summary of the original data.\n",
    "> Abstractive text summarization \n",
    "* Creating new summary (NLG)\n",
    "4. Topic Modelling / Topic Identification \n",
    "* Suppose we have 100 documents of data and we have 20 documents related to sports , 30 documents related to geography , 50 documents related to ethics and we are not aware of that.If we build a model on Topic modelling and this will find out the hidden patterns between the text from the 100 documents.\n",
    "* This model will return output as 20 documnets realted to 0 class , 30 documents related to 1 class , 50 documents related to 2 class.\n",
    "* And then we will need to manually classify the headings of the documents.\n",
    "* Model will classify the documents based on similar words or sentences in different clusters.\n",
    "5. Chatbot \n",
    "* Automatic response generation\n",
    "* \n",
    "# NLP pipeline \n",
    "1. Data Extraction \n",
    "2. EDA \n",
    "3. Preprocessing \n",
    "4. Feature Engineering \n",
    "5. Modelling \n",
    "6. Evaluation \n",
    "7. Deployment \n",
    "* \n",
    "1. Data Extraction \n",
    "* We can get data in different formats >> JSON , text , CSV , Images(OCR , Pytessarct , Amazon Textract , Google vision)\n",
    "> Data Types \n",
    "* Public data >> Easily avaialable \n",
    "* private data >> belongs to some orginization \n",
    "> If data is not available \n",
    "* We take data from Public options >> Webscrapping \n",
    "> If data is available \n",
    "* We will download the data.\n",
    "* In this way we get the data.\n",
    "> What is noise in text data?\n",
    "* \"We >>> loved $@ /\\ # the product will ___ defientlri recommend\"\n",
    "* Noise is available in above example.\n",
    "> Quality of the data \n",
    "> If there is noise in text data then what we would do?\n",
    "* We will process on extreme data preprocessing.\n",
    "> If there is minimal noise then what we would do?\n",
    "* Minimal data preprocessing\n",
    "* \n",
    "* After these we will get clean data.\n",
    "> Qunatity of the data?\n",
    "* In case of POC suppose we have 1Gb of data then that is a good thing.\n",
    "* \n",
    "> Problems regarding the data?\n",
    "* Quantity of the data\n",
    "* Quality of the data \n",
    "* Exact data / specific data is not available for our use case.\n",
    "* Donot have continous flow of data.\n",
    "* In Industry we will not get data in one time we will get data in cycles.\n",
    "* Monthly cycle , quarterly cycle , yearly cycle \n",
    "* \n",
    "2. EDA \n",
    "> Ngram \n",
    "> Word Cloud \n",
    "> Keyphrase extraction \n",
    "* \n",
    "> Ngram \n",
    "* Unigram \n",
    "* Bigram \n",
    "* Trigram\n",
    "* Quadragram\n",
    "> Suppose we have a example \"Rajesh is a hardworking guy.\"\n",
    "* Unigram = [Rajesh , is , a , hardworking , guy]\n",
    "* Bigram = [Rajesh is , is a , a hardworking ,hardworking guy]\n",
    "* Trigram = [Rajesh is a , is a hardworking , a hardworking guy]\n",
    "> Why Ngram?\n",
    "* It is a part of EDA.\n",
    "* To get insights from the data(Understanding words)\n",
    "* Suppose we are considering positive reviews we will get >> positive words by Ngram \n",
    "* Also when we are considering negative reviews we will get >> negative words by Ngram\n",
    "* \n",
    "* To get domain specific stopwords.\n",
    "* \n",
    "> Word Cloud \n",
    "* If a frequency of any word is higher than the word font will be higher in that word cloud.\n",
    "* If a frequency of any word is lower than the word font will be lower in that word cloud.\n",
    "* \n",
    "> Key phrase extraction \n",
    "* To extract important keyphrases or Keywords.\n",
    "* RAKE , YAKE algorithm used for key phrase extraction.\n",
    "* \"We are learning NLP\" and we apply key phrase extraction.\n",
    "* \"learning NLP\" will be important keyword.\n",
    "* \n",
    "4. Preprocessing\n",
    "> Tokenization :\n",
    "* Sentence tokenization\n",
    "* Word tokenization\n",
    "* Suppose we get text \"We are learning NLP.NLP is a huge domain.\"\n",
    "* Sentence tokenization : [We are learning NLP. , NLP is a huge domain.]\n",
    "* Word tokenization : [We , are , learning , NLP, . , NLP , is , a , huge , domain , .]\n",
    "> How does Sentence tokenization works or how does it know ot is a sentence?\n",
    "* It will check Syntax.\n",
    "* It will check for punctuation marks >> ! , . , ?\n",
    "* It will check for consumtions >> and , but \n",
    "* \n",
    "> Normalization\n",
    "* Suppose we have words \"G R E A T\" and another word \"great\" these 2 words have the same meaning but our model doesnot understand them.Model will allocate number for \"G R E A T\" and another number for \"great\" which is not good.\n",
    "* So in NLP we will convert this in single case depends upon us we keep it in lowecase or uppercase. In Industry standard practise is in lowercase.\n",
    "* \n",
    "> remove punctuation / symbols \n",
    "* we have string library we import punctuation \n",
    "* In this punctuation string we will have multiple symbols.\n",
    "* And we can use this to remove punctuation.\n",
    "* \n",
    "> remove stopwords\n",
    "* Language specific stopwords : Words in text which contributes grammatically buty doenot have any impact on the sentence. >> is , has , we , him \n",
    "* Domain specific stopwords : Words in domain text which contributes grammatically buty doenot have any impact on the sentence. >> doctor , tablet , capsule , treatment\n",
    "* \"Rajesh is suffering from cancer.Right now Doctor Pravin is treating him.We have given him XYZ tablet.\"\n",
    "* \n",
    "> Lemmatization(lemma) & stemming(stem) : words pruining \n",
    "* Lemma and stem are greak words.\n",
    "* lemma >> meaningful root word >> has dictionary in backend(word net dictionary) \n",
    "* example >> Running >> It will check the word in wordnet dictionary whether it is meaningful or not >> Is there any other root word available in word net dictionary >> Output = Run \n",
    "* \n",
    "* Stem >> Root word >> aggresive prunner >> example : running >> run , swimming >> swim\n",
    "* Believe >> Beli which is wrong as it is aggresive prunner \n",
    "* \n",
    "* We should use both lemma and stem and compare the results.\n",
    "* \n",
    "> Contraction mapping : text expanding\n",
    "* didn't >> did not , doesn't >> does not , haven't >> have not \n",
    "* Suppose we have review : \"I didn't like the movie\" , \"I liked the movie\" and we romve the stopwords then we will get as \"like movie\" and \"liked movie\".\n",
    "* In first case we are getting the wrong interpretation of the original sentence.which is wrong so we are using contraction mapping.\n",
    "* We can remove stopwords from stopwords list.\n",
    "* \n",
    "> Handling accented characters: \n",
    "* We have unidecode library to handle accented characters.\n",
    "* Accented characters : s >> $ , a >> @ \n",
    "* We will convert $ to s and @ to a\n",
    "* \n",
    "> Autocorrection \n",
    "* Correct spellings of word \n",
    "* We have autocorrect library and text blob library.\n",
    "* \n",
    "5. Feature Engineering \n",
    "* We convert text to numeric format/ Vectors in feature engineering of NLP.\n",
    "* Converting text to numeric format/ Vectors in feature engineering is called as Word embedding.\n",
    "> Word embedding \n",
    "* Frequency based word embedding : It focuses on word frequency and returns numeric format.\n",
    "* Count vectorizer \n",
    "* TFIDF \n",
    "* \n",
    "* Prediction based word embedding : It uses algorithm in backend and convert text to numeric format.\n",
    "* Word to weight\n",
    "* fast text \n",
    "* Doc2Ve\n",
    "* \n",
    "6. Modelling \n",
    "* We now have data in numeric format.\n",
    "* We can use all classification algorithms.\n",
    "* Logistic regression , SVM ,Random Forest , AdaBoost , Decision Tree , Naive Bayes , LSTM , RNN.\n",
    "* \n",
    "7. Evaluation \n",
    "* Confusion matrix , accuracy score , precision score , recall score , F1 score\n",
    "* If we have low accuracy we will jump on preprocessing or go for feature engineering.\n",
    "* Suppose we have frequent based first then we will go for prediction based for better accuracy.\n",
    "* \n",
    "8. Deployment \n",
    "* If we get good accuracy then we will go for Deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nltk \n",
    "* gensim \n",
    "* spacy \n",
    "* most used libraries in NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize , WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer , LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ants are found everywhere in the world. They make their home in buildings, gardens\n",
    "etc. They live in anthills. Ants are very hardworking insects. Throughout the summers they\n",
    "collect food for the winter season. Whenever they find a sweet lying on the floor they stick\n",
    "to the sweet and carry it to their home. Thus, in this way, they clean the floor.Ants are \n",
    "generally red and black in colour. They have two eyes and six legs. They are social insects. They\n",
    "live in groups or colonies. Most ants are scavengers they collect whatever food they can find.\n",
    "They are usually wingless but they develop wings when they reproduce. Their bites are quite \n",
    "painful.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants are found everywhere in the world.',\n",
       " 'They make their home in buildings, gardens\\netc.',\n",
       " 'They live in anthills.',\n",
       " 'Ants are very hardworking insects.',\n",
       " 'Throughout the summers they\\ncollect food for the winter season.',\n",
       " 'Whenever they find a sweet lying on the floor they stick\\nto the sweet and carry it to their home.',\n",
       " 'Thus, in this way, they clean the floor.Ants are \\ngenerally red and black in colour.',\n",
       " 'They have two eyes and six legs.',\n",
       " 'They are social insects.',\n",
       " 'They\\nlive in groups or colonies.',\n",
       " 'Most ants are scavengers they collect whatever food they can find.',\n",
       " 'They are usually wingless but they develop wings when they reproduce.',\n",
       " 'Their bites are quite \\npainful.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings,',\n",
       " 'gardens',\n",
       " 'etc.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home.',\n",
       " 'Thus,',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way,',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whitespace tokenization\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization \n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "text_without_punc = [word for word in lower_text if word not in punctuation]\n",
    "text_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'summers',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'floor',\n",
       " 'stick',\n",
       " 'sweet',\n",
       " 'carry',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'way',\n",
       " 'clean',\n",
       " 'floor.ants',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'live',\n",
       " 'groups',\n",
       " 'colonies',\n",
       " 'ants',\n",
       " 'scavengers',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'find',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'reproduce',\n",
       " 'bites',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stopwords = [word for word in text_without_punc if word not in stopword_list]\n",
    "text_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not like the movie'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conttraction mapping \n",
    "text1 = \"I didn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I does not like the movie'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I doesn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot like the movie'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I can't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original word >> found\n",
      "Stemmed word >> found\n",
      "Lemmetized word >> found\n",
      "********************************************************************************\n",
      "Original word >> everywhere\n",
      "Stemmed word >> everywh\n",
      "Lemmetized word >> everywhere\n",
      "********************************************************************************\n",
      "Original word >> world\n",
      "Stemmed word >> world\n",
      "Lemmetized word >> world\n",
      "********************************************************************************\n",
      "Original word >> make\n",
      "Stemmed word >> mak\n",
      "Lemmetized word >> make\n",
      "********************************************************************************\n",
      "Original word >> home\n",
      "Stemmed word >> hom\n",
      "Lemmetized word >> home\n",
      "********************************************************************************\n",
      "Original word >> buildings\n",
      "Stemmed word >> build\n",
      "Lemmetized word >> building\n",
      "********************************************************************************\n",
      "Original word >> gardens\n",
      "Stemmed word >> gard\n",
      "Lemmetized word >> garden\n",
      "********************************************************************************\n",
      "Original word >> etc\n",
      "Stemmed word >> etc\n",
      "Lemmetized word >> etc\n",
      "********************************************************************************\n",
      "Original word >> live\n",
      "Stemmed word >> liv\n",
      "Lemmetized word >> live\n",
      "********************************************************************************\n",
      "Original word >> anthills\n",
      "Stemmed word >> anthil\n",
      "Lemmetized word >> anthill\n",
      "********************************************************************************\n",
      "Original word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original word >> hardworking\n",
      "Stemmed word >> hardwork\n",
      "Lemmetized word >> hardworking\n",
      "********************************************************************************\n",
      "Original word >> insects\n",
      "Stemmed word >> insect\n",
      "Lemmetized word >> insect\n",
      "********************************************************************************\n",
      "Original word >> throughout\n",
      "Stemmed word >> throughout\n",
      "Lemmetized word >> throughout\n",
      "********************************************************************************\n",
      "Original word >> summers\n",
      "Stemmed word >> sum\n",
      "Lemmetized word >> summer\n",
      "********************************************************************************\n",
      "Original word >> collect\n",
      "Stemmed word >> collect\n",
      "Lemmetized word >> collect\n",
      "********************************************************************************\n",
      "Original word >> food\n",
      "Stemmed word >> food\n",
      "Lemmetized word >> food\n",
      "********************************************************************************\n",
      "Original word >> winter\n",
      "Stemmed word >> wint\n",
      "Lemmetized word >> winter\n",
      "********************************************************************************\n",
      "Original word >> season\n",
      "Stemmed word >> season\n",
      "Lemmetized word >> season\n",
      "********************************************************************************\n",
      "Original word >> whenever\n",
      "Stemmed word >> whenev\n",
      "Lemmetized word >> whenever\n",
      "********************************************************************************\n",
      "Original word >> find\n",
      "Stemmed word >> find\n",
      "Lemmetized word >> find\n",
      "********************************************************************************\n",
      "Original word >> sweet\n",
      "Stemmed word >> sweet\n",
      "Lemmetized word >> sweet\n",
      "********************************************************************************\n",
      "Original word >> lying\n",
      "Stemmed word >> lying\n",
      "Lemmetized word >> lying\n",
      "********************************************************************************\n",
      "Original word >> floor\n",
      "Stemmed word >> flo\n",
      "Lemmetized word >> floor\n",
      "********************************************************************************\n",
      "Original word >> stick\n",
      "Stemmed word >> stick\n",
      "Lemmetized word >> stick\n",
      "********************************************************************************\n",
      "Original word >> sweet\n",
      "Stemmed word >> sweet\n",
      "Lemmetized word >> sweet\n",
      "********************************************************************************\n",
      "Original word >> carry\n",
      "Stemmed word >> carry\n",
      "Lemmetized word >> carry\n",
      "********************************************************************************\n",
      "Original word >> home\n",
      "Stemmed word >> hom\n",
      "Lemmetized word >> home\n",
      "********************************************************************************\n",
      "Original word >> thus\n",
      "Stemmed word >> thu\n",
      "Lemmetized word >> thus\n",
      "********************************************************************************\n",
      "Original word >> way\n",
      "Stemmed word >> way\n",
      "Lemmetized word >> way\n",
      "********************************************************************************\n",
      "Original word >> clean\n",
      "Stemmed word >> cle\n",
      "Lemmetized word >> clean\n",
      "********************************************************************************\n",
      "Original word >> floor.ants\n",
      "Stemmed word >> floor.ants\n",
      "Lemmetized word >> floor.ants\n",
      "********************************************************************************\n",
      "Original word >> generally\n",
      "Stemmed word >> gen\n",
      "Lemmetized word >> generally\n",
      "********************************************************************************\n",
      "Original word >> red\n",
      "Stemmed word >> red\n",
      "Lemmetized word >> red\n",
      "********************************************************************************\n",
      "Original word >> black\n",
      "Stemmed word >> black\n",
      "Lemmetized word >> black\n",
      "********************************************************************************\n",
      "Original word >> colour\n",
      "Stemmed word >> colo\n",
      "Lemmetized word >> colour\n",
      "********************************************************************************\n",
      "Original word >> two\n",
      "Stemmed word >> two\n",
      "Lemmetized word >> two\n",
      "********************************************************************************\n",
      "Original word >> eyes\n",
      "Stemmed word >> ey\n",
      "Lemmetized word >> eye\n",
      "********************************************************************************\n",
      "Original word >> six\n",
      "Stemmed word >> six\n",
      "Lemmetized word >> six\n",
      "********************************************************************************\n",
      "Original word >> legs\n",
      "Stemmed word >> leg\n",
      "Lemmetized word >> leg\n",
      "********************************************************************************\n",
      "Original word >> social\n",
      "Stemmed word >> soc\n",
      "Lemmetized word >> social\n",
      "********************************************************************************\n",
      "Original word >> insects\n",
      "Stemmed word >> insect\n",
      "Lemmetized word >> insect\n",
      "********************************************************************************\n",
      "Original word >> live\n",
      "Stemmed word >> liv\n",
      "Lemmetized word >> live\n",
      "********************************************************************************\n",
      "Original word >> groups\n",
      "Stemmed word >> group\n",
      "Lemmetized word >> group\n",
      "********************************************************************************\n",
      "Original word >> colonies\n",
      "Stemmed word >> colony\n",
      "Lemmetized word >> colony\n",
      "********************************************************************************\n",
      "Original word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original word >> scavengers\n",
      "Stemmed word >> scaveng\n",
      "Lemmetized word >> scavenger\n",
      "********************************************************************************\n",
      "Original word >> collect\n",
      "Stemmed word >> collect\n",
      "Lemmetized word >> collect\n",
      "********************************************************************************\n",
      "Original word >> whatever\n",
      "Stemmed word >> whatev\n",
      "Lemmetized word >> whatever\n",
      "********************************************************************************\n",
      "Original word >> food\n",
      "Stemmed word >> food\n",
      "Lemmetized word >> food\n",
      "********************************************************************************\n",
      "Original word >> find\n",
      "Stemmed word >> find\n",
      "Lemmetized word >> find\n",
      "********************************************************************************\n",
      "Original word >> usually\n",
      "Stemmed word >> us\n",
      "Lemmetized word >> usually\n",
      "********************************************************************************\n",
      "Original word >> wingless\n",
      "Stemmed word >> wingless\n",
      "Lemmetized word >> wingless\n",
      "********************************************************************************\n",
      "Original word >> develop\n",
      "Stemmed word >> develop\n",
      "Lemmetized word >> develop\n",
      "********************************************************************************\n",
      "Original word >> wings\n",
      "Stemmed word >> wing\n",
      "Lemmetized word >> wing\n",
      "********************************************************************************\n",
      "Original word >> reproduce\n",
      "Stemmed word >> reproduc\n",
      "Lemmetized word >> reproduce\n",
      "********************************************************************************\n",
      "Original word >> bites\n",
      "Stemmed word >> bit\n",
      "Lemmetized word >> bite\n",
      "********************************************************************************\n",
      "Original word >> quite\n",
      "Stemmed word >> quit\n",
      "Lemmetized word >> quite\n",
      "********************************************************************************\n",
      "Original word >> painful\n",
      "Stemmed word >> pain\n",
      "Lemmetized word >> painful\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# We are facing issue for can't at such times we need to do it manually using replace \n",
    "# Stemming and lemmatization \n",
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# We cannot give direct data to prunners ,we need to give it to word.\n",
    "for word in text_without_stopwords:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmetized_word = lemmatizer.lemmatize(word)\n",
    "    print(f\"Original word >> {word}\")\n",
    "    print(f\"Stemmed word >> {stemmed_word}\")\n",
    "    print(f\"Lemmetized word >> {lemmetized_word}\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O,o,O,o,N,n,O,o'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling accented characters\n",
    "accented_characters = \"Ô,ô,Ò,ò,Ñ,ñ,Ö,ö\"\n",
    "fixed_words = unidecode(accented_characters)\n",
    "fixed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"Mussage\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"Survice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"mussage\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"Survice\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are not effective because mussage can be corrected to massage also \n",
    "# We should use trial and error for autocorrect libraries.\n",
    "# We can try different combinations in preprocessing and can check result in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize , WhitespaceTokenizer\n",
    "from nltk.stem import LancasterStemmer , WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ants are found everywhere in the world. They make their home in buildings, gardens\n",
    "etc. They live in anthills. Ants are very hardworking insects. Throughout the summers they\n",
    "collect food for the winter season. Whenever they find a sweet lying on the floor they stick\n",
    "to the sweet and carry it to their home. Thus, in this way, they clean the floor.Ants are \n",
    "generally red and black in colour. They have two eyes and six legs. They are social insects. They\n",
    "live in groups or colonies. Most ants are scavengers they collect whatever food they can find.\n",
    "They are usually wingless but they develop wings when they reproduce. Their bites are quite \n",
    "painful.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants are found everywhere in the world.',\n",
       " 'They make their home in buildings, gardens\\netc.',\n",
       " 'They live in anthills.',\n",
       " 'Ants are very hardworking insects.',\n",
       " 'Throughout the summers they\\ncollect food for the winter season.',\n",
       " 'Whenever they find a sweet lying on the floor they stick\\nto the sweet and carry it to their home.',\n",
       " 'Thus, in this way, they clean the floor.Ants are \\ngenerally red and black in colour.',\n",
       " 'They have two eyes and six legs.',\n",
       " 'They are social insects.',\n",
       " 'They\\nlive in groups or colonies.',\n",
       " 'Most ants are scavengers they collect whatever food they can find.',\n",
       " 'They are usually wingless but they develop wings when they reproduce.',\n",
       " 'Their bites are quite \\npainful.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings,',\n",
       " 'gardens',\n",
       " 'etc.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home.',\n",
       " 'Thus,',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way,',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whitespace tokenization\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization \n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "text_without_punc = [word for word in lower_text if word not in punctuation]\n",
    "text_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'summers',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'floor',\n",
       " 'stick',\n",
       " 'sweet',\n",
       " 'carry',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'way',\n",
       " 'clean',\n",
       " 'floor.ants',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'live',\n",
       " 'groups',\n",
       " 'colonies',\n",
       " 'ants',\n",
       " 'scavengers',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'find',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'reproduce',\n",
       " 'bites',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "text_without_stopwords = [word for word in text_without_punc if word not in stopword_list]\n",
    "text_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not like the movie'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contraction mapping \n",
    "text1 = \"I didn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I does not like the movie'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I doesn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot like the movie'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I can't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original_word >> found\n",
      "Stemmed word >> found\n",
      "Lemmetized word >> found\n",
      "********************************************************************************\n",
      "Original_word >> everywhere\n",
      "Stemmed word >> everywh\n",
      "Lemmetized word >> everywhere\n",
      "********************************************************************************\n",
      "Original_word >> world\n",
      "Stemmed word >> world\n",
      "Lemmetized word >> world\n",
      "********************************************************************************\n",
      "Original_word >> make\n",
      "Stemmed word >> mak\n",
      "Lemmetized word >> make\n",
      "********************************************************************************\n",
      "Original_word >> home\n",
      "Stemmed word >> hom\n",
      "Lemmetized word >> home\n",
      "********************************************************************************\n",
      "Original_word >> buildings\n",
      "Stemmed word >> build\n",
      "Lemmetized word >> building\n",
      "********************************************************************************\n",
      "Original_word >> gardens\n",
      "Stemmed word >> gard\n",
      "Lemmetized word >> garden\n",
      "********************************************************************************\n",
      "Original_word >> etc\n",
      "Stemmed word >> etc\n",
      "Lemmetized word >> etc\n",
      "********************************************************************************\n",
      "Original_word >> live\n",
      "Stemmed word >> liv\n",
      "Lemmetized word >> live\n",
      "********************************************************************************\n",
      "Original_word >> anthills\n",
      "Stemmed word >> anthil\n",
      "Lemmetized word >> anthill\n",
      "********************************************************************************\n",
      "Original_word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original_word >> hardworking\n",
      "Stemmed word >> hardwork\n",
      "Lemmetized word >> hardworking\n",
      "********************************************************************************\n",
      "Original_word >> insects\n",
      "Stemmed word >> insect\n",
      "Lemmetized word >> insect\n",
      "********************************************************************************\n",
      "Original_word >> throughout\n",
      "Stemmed word >> throughout\n",
      "Lemmetized word >> throughout\n",
      "********************************************************************************\n",
      "Original_word >> summers\n",
      "Stemmed word >> sum\n",
      "Lemmetized word >> summer\n",
      "********************************************************************************\n",
      "Original_word >> collect\n",
      "Stemmed word >> collect\n",
      "Lemmetized word >> collect\n",
      "********************************************************************************\n",
      "Original_word >> food\n",
      "Stemmed word >> food\n",
      "Lemmetized word >> food\n",
      "********************************************************************************\n",
      "Original_word >> winter\n",
      "Stemmed word >> wint\n",
      "Lemmetized word >> winter\n",
      "********************************************************************************\n",
      "Original_word >> season\n",
      "Stemmed word >> season\n",
      "Lemmetized word >> season\n",
      "********************************************************************************\n",
      "Original_word >> whenever\n",
      "Stemmed word >> whenev\n",
      "Lemmetized word >> whenever\n",
      "********************************************************************************\n",
      "Original_word >> find\n",
      "Stemmed word >> find\n",
      "Lemmetized word >> find\n",
      "********************************************************************************\n",
      "Original_word >> sweet\n",
      "Stemmed word >> sweet\n",
      "Lemmetized word >> sweet\n",
      "********************************************************************************\n",
      "Original_word >> lying\n",
      "Stemmed word >> lying\n",
      "Lemmetized word >> lying\n",
      "********************************************************************************\n",
      "Original_word >> floor\n",
      "Stemmed word >> flo\n",
      "Lemmetized word >> floor\n",
      "********************************************************************************\n",
      "Original_word >> stick\n",
      "Stemmed word >> stick\n",
      "Lemmetized word >> stick\n",
      "********************************************************************************\n",
      "Original_word >> sweet\n",
      "Stemmed word >> sweet\n",
      "Lemmetized word >> sweet\n",
      "********************************************************************************\n",
      "Original_word >> carry\n",
      "Stemmed word >> carry\n",
      "Lemmetized word >> carry\n",
      "********************************************************************************\n",
      "Original_word >> home\n",
      "Stemmed word >> hom\n",
      "Lemmetized word >> home\n",
      "********************************************************************************\n",
      "Original_word >> thus\n",
      "Stemmed word >> thu\n",
      "Lemmetized word >> thus\n",
      "********************************************************************************\n",
      "Original_word >> way\n",
      "Stemmed word >> way\n",
      "Lemmetized word >> way\n",
      "********************************************************************************\n",
      "Original_word >> clean\n",
      "Stemmed word >> cle\n",
      "Lemmetized word >> clean\n",
      "********************************************************************************\n",
      "Original_word >> floor.ants\n",
      "Stemmed word >> floor.ants\n",
      "Lemmetized word >> floor.ants\n",
      "********************************************************************************\n",
      "Original_word >> generally\n",
      "Stemmed word >> gen\n",
      "Lemmetized word >> generally\n",
      "********************************************************************************\n",
      "Original_word >> red\n",
      "Stemmed word >> red\n",
      "Lemmetized word >> red\n",
      "********************************************************************************\n",
      "Original_word >> black\n",
      "Stemmed word >> black\n",
      "Lemmetized word >> black\n",
      "********************************************************************************\n",
      "Original_word >> colour\n",
      "Stemmed word >> colo\n",
      "Lemmetized word >> colour\n",
      "********************************************************************************\n",
      "Original_word >> two\n",
      "Stemmed word >> two\n",
      "Lemmetized word >> two\n",
      "********************************************************************************\n",
      "Original_word >> eyes\n",
      "Stemmed word >> ey\n",
      "Lemmetized word >> eye\n",
      "********************************************************************************\n",
      "Original_word >> six\n",
      "Stemmed word >> six\n",
      "Lemmetized word >> six\n",
      "********************************************************************************\n",
      "Original_word >> legs\n",
      "Stemmed word >> leg\n",
      "Lemmetized word >> leg\n",
      "********************************************************************************\n",
      "Original_word >> social\n",
      "Stemmed word >> soc\n",
      "Lemmetized word >> social\n",
      "********************************************************************************\n",
      "Original_word >> insects\n",
      "Stemmed word >> insect\n",
      "Lemmetized word >> insect\n",
      "********************************************************************************\n",
      "Original_word >> live\n",
      "Stemmed word >> liv\n",
      "Lemmetized word >> live\n",
      "********************************************************************************\n",
      "Original_word >> groups\n",
      "Stemmed word >> group\n",
      "Lemmetized word >> group\n",
      "********************************************************************************\n",
      "Original_word >> colonies\n",
      "Stemmed word >> colony\n",
      "Lemmetized word >> colony\n",
      "********************************************************************************\n",
      "Original_word >> ants\n",
      "Stemmed word >> ant\n",
      "Lemmetized word >> ant\n",
      "********************************************************************************\n",
      "Original_word >> scavengers\n",
      "Stemmed word >> scaveng\n",
      "Lemmetized word >> scavenger\n",
      "********************************************************************************\n",
      "Original_word >> collect\n",
      "Stemmed word >> collect\n",
      "Lemmetized word >> collect\n",
      "********************************************************************************\n",
      "Original_word >> whatever\n",
      "Stemmed word >> whatev\n",
      "Lemmetized word >> whatever\n",
      "********************************************************************************\n",
      "Original_word >> food\n",
      "Stemmed word >> food\n",
      "Lemmetized word >> food\n",
      "********************************************************************************\n",
      "Original_word >> find\n",
      "Stemmed word >> find\n",
      "Lemmetized word >> find\n",
      "********************************************************************************\n",
      "Original_word >> usually\n",
      "Stemmed word >> us\n",
      "Lemmetized word >> usually\n",
      "********************************************************************************\n",
      "Original_word >> wingless\n",
      "Stemmed word >> wingless\n",
      "Lemmetized word >> wingless\n",
      "********************************************************************************\n",
      "Original_word >> develop\n",
      "Stemmed word >> develop\n",
      "Lemmetized word >> develop\n",
      "********************************************************************************\n",
      "Original_word >> wings\n",
      "Stemmed word >> wing\n",
      "Lemmetized word >> wing\n",
      "********************************************************************************\n",
      "Original_word >> reproduce\n",
      "Stemmed word >> reproduc\n",
      "Lemmetized word >> reproduce\n",
      "********************************************************************************\n",
      "Original_word >> bites\n",
      "Stemmed word >> bit\n",
      "Lemmetized word >> bite\n",
      "********************************************************************************\n",
      "Original_word >> quite\n",
      "Stemmed word >> quit\n",
      "Lemmetized word >> quite\n",
      "********************************************************************************\n",
      "Original_word >> painful\n",
      "Stemmed word >> pain\n",
      "Lemmetized word >> painful\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# We are facing issues for can't at such times we need to do it manually using replace \n",
    "# Stemming and Lemmatization \n",
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# We cannot give direct data to prunners , we nned to give it word by word \n",
    "for word in text_without_stopwords:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmetized_word = lemmatizer.lemmatize(word)\n",
    "    print(f\"Original_word >> {word}\")\n",
    "    print(f\"Stemmed word >> {stemmed_word}\")\n",
    "    print(f\"Lemmetized word >> {lemmetized_word}\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O,o,O,o,N,n,O,o'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling accented characters \n",
    "accented_characters = \"Ô,ô,Ò,ò,Ñ,ñ,Ö,ö\"\n",
    "fixed_words = unidecode(accented_characters)\n",
    "fixed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"Muusage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"Survice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"mussage\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"Survice\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraraies are not as effective because mussage can be corrected to massage also.\n",
    "# We should use trial and error for autocorrect libraries \n",
    "# We can try different combinations in preprocessing and we can check result in eva;uation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In Feature enginnering we saw about frequency based word embeddings and prediction based word embeddings. We will study about frequency based word embeddings in which 1st one is Count Vectorizer.\n",
    "# Count vectorizer\n",
    "* every count of word in every document will be calculated.\n",
    "> Suppose we get 2 documents \n",
    "1. We are learning Data Science.\n",
    "2. Data Science is a combination of Deep Learning and Machine Learning.\n",
    "* The data we get in feature enginnering will be preprocessed.So above data will become \n",
    "1. learning Data Science\n",
    "2. Data Science combination Deep Learning Machine Learning\n",
    "* After getting this preprocessed data to Count Vectorizer then the Count Vectorizer will find out unique words.\n",
    "* Count Vectorizer will make a list of unique words.\n",
    "* [learning , Data , Science , combination , Deep , Machine]\n",
    "* Then it will create a structure where every unique word will be a independent variable.\n",
    "* Number of rows will be equal to number of documents.\n",
    "* It will take count of every unique word(Indepndent variable) in every document.\n",
    "* And the structure or dataframe we got by Count Vectorizer will be commonly known as Document term matrix/Bag of words/Bag of tokens.\n",
    "> When our count vectorizer gets test data \n",
    "* Test data = NLP needs both Machine learning and Deep learning.\n",
    "* Preprocessing is also done on testing data so after preprocessing we will get test data as \n",
    "* test data = nlp needs machine learning deep learning \n",
    "* But as we compare with 1 and 2 documents as training data we dont have(nlp and needs) as unique words/ independent variable , so for this purpose we have inbuilt feature in count vectorizer.\n",
    "> OOV (Out of Vocabulary)\n",
    "* In OOV , Unseen words are taken into account , as we have 2 unseen words our OOV score will be 2.\n",
    "> Drawbacks of Count Vectorizer \n",
    "1. Curse of Dimensionality\n",
    "2. Order is not maintained\n",
    "3. It is not considering actual meaning of words.\n",
    "* \n",
    "# TF-IDF \n",
    "* It is a combination of 2 things \n",
    "1. Term frequency \n",
    "2. Inverse term frequency \n",
    "* \n",
    "1. Term frequency \n",
    "* Formula : Frequency of term \"t\" in a document / Total words in that document\n",
    "> Example : Data Science is a combination of Machine learning and Deep learning.\n",
    "* TF-IDF is a feature enginnering technique so we will get preprocessed data.\n",
    "> So we will get : Data Science combination Machine learning Deep Learning.\n",
    "* Term Frequency (Learning) : 2 / 7 \n",
    "* Term Frequency (Data) : 1/ 7 \n",
    "2. IDF (Inverse document frequency)\n",
    "> Documents frequency \n",
    "* Formula : Number of documents containing term \"t\" / Total number of documents \n",
    "* Suppose we have 2 documents\n",
    "> NLP needs deep learning and machine learning.\n",
    "> data science contains lots of things like NLP.\n",
    "* Document frequency(NLP) : 2 / 2 = 1 \n",
    "* Document frequency(learning) : 1 / 2 = 0.5\n",
    "> IDF (Inverse document frequency)\n",
    "* Formula : log(1 / document frequency)\n",
    "* Formula : log(Total number of documents / Number of documents containing term \"t\")\n",
    "> TF-IDF \n",
    "* Formula : TF * IDF \n",
    "> Working of TF-IDF \n",
    "* Document1 : We are learning NLP.\n",
    "* Document2 : Data Science is a combination of Deep learning and Machine learning.\n",
    "* Document3 : NLP needs both Machine learning and Deep learning.\n",
    "* After preprocessing :\n",
    "* Document1 : learning nlp\n",
    "* Document2 : data science combination deep learning machine learning\n",
    "* Document3 : nlp needs machine learning deep learning\n",
    "* It will find out unique words.\n",
    "* Every unique word will be a independent varaiable.\n",
    "* Unique words : learning nlp data science combination deep machine needs\n",
    "* We have 3 documents so we will have 3 rows.\n",
    "* We will calculate TF-IDF of every word in every document.\n",
    "* TF-IDF is calculating weighatge.\n",
    "* The more the frequency of the word less will be the weightage of the word.\n",
    "* \n",
    "* Example of Movie reviews : \n",
    "* Bad Movie ,Awesome Movie , Fabulous Movie likewise we have 100b reviews.\n",
    "* Inn Above example Movie word is not important . In sentiment analysis bad , awesome , fabulous words are more important.\n",
    "> Drawbacks of TF-IDF \n",
    "1. Curse of Dimensionality\n",
    "2. Order is not maintained\n",
    "3. It is not considering actual meaning of words.\n",
    "> Similarities between Count Vectorizer and TF-IDF\n",
    "* They have same drawbacks.\n",
    "* When we initiate models of Count Vectorizer and TF-IDF there parameters used are same.\n",
    "> Difference between Count Vectorizer and TF-IDF \n",
    "* Count Vectorizer counts frequency of each word in each document.\n",
    "* TF-IDF counts weigthage of each word in each document.\n",
    "* Difference of accuarcy of models of Count Vectorizer and TF-IDF is low.But this difference matters in Industry.Whichever model will give the highest accuracy that model will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nltk \n",
    "* gensim \n",
    "* spacy \n",
    "* Most used libraries in NLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize , WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer , LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions \n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ants are found everywhere in the world. They make their home in buildings, gardens\n",
    "etc. They live in anthills. Ants are very hardworking insects. Throughout the summers they\n",
    "collect food for the winter season. Whenever they find a sweet lying on the floor they stick\n",
    "to the sweet and carry it to their home. Thus, in this way, they clean the floor.Ants are \n",
    "generally red and black in colour. They have two eyes and six legs. They are social insects. They\n",
    "live in groups or colonies. Most ants are scavengers they collect whatever food they can find.\n",
    "They are usually wingless but they develop wings when they reproduce. Their bites are quite \n",
    "painful.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants are found everywhere in the world.',\n",
       " 'They make their home in buildings, gardens\\netc.',\n",
       " 'They live in anthills.',\n",
       " 'Ants are very hardworking insects.',\n",
       " 'Throughout the summers they\\ncollect food for the winter season.',\n",
       " 'Whenever they find a sweet lying on the floor they stick\\nto the sweet and carry it to their home.',\n",
       " 'Thus, in this way, they clean the floor.Ants are \\ngenerally red and black in colour.',\n",
       " 'They have two eyes and six legs.',\n",
       " 'They are social insects.',\n",
       " 'They\\nlive in groups or colonies.',\n",
       " 'Most ants are scavengers they collect whatever food they can find.',\n",
       " 'They are usually wingless but they develop wings when they reproduce.',\n",
       " 'Their bites are quite \\npainful.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent tokenization \n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenize \n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world.',\n",
       " 'They',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings,',\n",
       " 'gardens',\n",
       " 'etc.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills.',\n",
       " 'Ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects.',\n",
       " 'Throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season.',\n",
       " 'Whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home.',\n",
       " 'Thus,',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way,',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.Ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour.',\n",
       " 'They',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects.',\n",
       " 'They',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies.',\n",
       " 'Most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce.',\n",
       " 'Their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whitesoace tokenization\n",
    "tokens1 = WhitespaceTokenizer().tokenize(text)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " ',',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " '.',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " '.',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " '.',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " '.',\n",
       " 'thus',\n",
       " ',',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " ',',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " '.',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " '.',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " '.',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " '.',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "lower_text = [word.lower() for word in tokens]\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their',\n",
       " 'home',\n",
       " 'in',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'very',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'for',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'they',\n",
       " 'find',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'they',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'carry',\n",
       " 'it',\n",
       " 'to',\n",
       " 'their',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'in',\n",
       " 'this',\n",
       " 'way',\n",
       " 'they',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'floor.ants',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'and',\n",
       " 'black',\n",
       " 'in',\n",
       " 'colour',\n",
       " 'they',\n",
       " 'have',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'they',\n",
       " 'are',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'groups',\n",
       " 'or',\n",
       " 'colonies',\n",
       " 'most',\n",
       " 'ants',\n",
       " 'are',\n",
       " 'scavengers',\n",
       " 'they',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'they',\n",
       " 'can',\n",
       " 'find',\n",
       " 'they',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'but',\n",
       " 'they',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'when',\n",
       " 'they',\n",
       " 'reproduce',\n",
       " 'their',\n",
       " 'bites',\n",
       " 'are',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation \n",
    "text_without_punc = [ word for word in lower_text if word not in punctuation]\n",
    "text_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live',\n",
       " 'anthills',\n",
       " 'ants',\n",
       " 'hardworking',\n",
       " 'insects',\n",
       " 'throughout',\n",
       " 'summers',\n",
       " 'collect',\n",
       " 'food',\n",
       " 'winter',\n",
       " 'season',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'sweet',\n",
       " 'lying',\n",
       " 'floor',\n",
       " 'stick',\n",
       " 'sweet',\n",
       " 'carry',\n",
       " 'home',\n",
       " 'thus',\n",
       " 'way',\n",
       " 'clean',\n",
       " 'floor.ants',\n",
       " 'generally',\n",
       " 'red',\n",
       " 'black',\n",
       " 'colour',\n",
       " 'two',\n",
       " 'eyes',\n",
       " 'six',\n",
       " 'legs',\n",
       " 'social',\n",
       " 'insects',\n",
       " 'live',\n",
       " 'groups',\n",
       " 'colonies',\n",
       " 'ants',\n",
       " 'scavengers',\n",
       " 'collect',\n",
       " 'whatever',\n",
       " 'food',\n",
       " 'find',\n",
       " 'usually',\n",
       " 'wingless',\n",
       " 'develop',\n",
       " 'wings',\n",
       " 'reproduce',\n",
       " 'bites',\n",
       " 'quite',\n",
       " 'painful']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords \n",
    "text_without_stopwords = [word for word in text_without_punc if word not in stopword_list]\n",
    "text_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'are',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'they',\n",
       " 'make',\n",
       " 'their']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_punc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants',\n",
       " 'found',\n",
       " 'everywhere',\n",
       " 'world',\n",
       " 'make',\n",
       " 'home',\n",
       " 'buildings',\n",
       " 'gardens',\n",
       " 'etc',\n",
       " 'live']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not like the movie'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contraction mapping \n",
    "text1 = \"I didn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I does not like the movie'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I doesn't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot like the movie'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I can't like the movie\"\n",
    "expanded_text = contractions.fix(text1)\n",
    "expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string >> ants\n",
      "Lemmetized word >> ant\n",
      "Stemmed word >> ant\n",
      "********************************************************************************\n",
      "Original string >> found\n",
      "Lemmetized word >> found\n",
      "Stemmed word >> found\n",
      "********************************************************************************\n",
      "Original string >> everywhere\n",
      "Lemmetized word >> everywhere\n",
      "Stemmed word >> everywh\n",
      "********************************************************************************\n",
      "Original string >> world\n",
      "Lemmetized word >> world\n",
      "Stemmed word >> world\n",
      "********************************************************************************\n",
      "Original string >> make\n",
      "Lemmetized word >> make\n",
      "Stemmed word >> mak\n",
      "********************************************************************************\n",
      "Original string >> home\n",
      "Lemmetized word >> home\n",
      "Stemmed word >> hom\n",
      "********************************************************************************\n",
      "Original string >> buildings\n",
      "Lemmetized word >> building\n",
      "Stemmed word >> build\n",
      "********************************************************************************\n",
      "Original string >> gardens\n",
      "Lemmetized word >> garden\n",
      "Stemmed word >> gard\n",
      "********************************************************************************\n",
      "Original string >> etc\n",
      "Lemmetized word >> etc\n",
      "Stemmed word >> etc\n",
      "********************************************************************************\n",
      "Original string >> live\n",
      "Lemmetized word >> live\n",
      "Stemmed word >> liv\n",
      "********************************************************************************\n",
      "Original string >> anthills\n",
      "Lemmetized word >> anthill\n",
      "Stemmed word >> anthil\n",
      "********************************************************************************\n",
      "Original string >> ants\n",
      "Lemmetized word >> ant\n",
      "Stemmed word >> ant\n",
      "********************************************************************************\n",
      "Original string >> hardworking\n",
      "Lemmetized word >> hardworking\n",
      "Stemmed word >> hardwork\n",
      "********************************************************************************\n",
      "Original string >> insects\n",
      "Lemmetized word >> insect\n",
      "Stemmed word >> insect\n",
      "********************************************************************************\n",
      "Original string >> throughout\n",
      "Lemmetized word >> throughout\n",
      "Stemmed word >> throughout\n",
      "********************************************************************************\n",
      "Original string >> summers\n",
      "Lemmetized word >> summer\n",
      "Stemmed word >> sum\n",
      "********************************************************************************\n",
      "Original string >> collect\n",
      "Lemmetized word >> collect\n",
      "Stemmed word >> collect\n",
      "********************************************************************************\n",
      "Original string >> food\n",
      "Lemmetized word >> food\n",
      "Stemmed word >> food\n",
      "********************************************************************************\n",
      "Original string >> winter\n",
      "Lemmetized word >> winter\n",
      "Stemmed word >> wint\n",
      "********************************************************************************\n",
      "Original string >> season\n",
      "Lemmetized word >> season\n",
      "Stemmed word >> season\n",
      "********************************************************************************\n",
      "Original string >> whenever\n",
      "Lemmetized word >> whenever\n",
      "Stemmed word >> whenev\n",
      "********************************************************************************\n",
      "Original string >> find\n",
      "Lemmetized word >> find\n",
      "Stemmed word >> find\n",
      "********************************************************************************\n",
      "Original string >> sweet\n",
      "Lemmetized word >> sweet\n",
      "Stemmed word >> sweet\n",
      "********************************************************************************\n",
      "Original string >> lying\n",
      "Lemmetized word >> lying\n",
      "Stemmed word >> lying\n",
      "********************************************************************************\n",
      "Original string >> floor\n",
      "Lemmetized word >> floor\n",
      "Stemmed word >> flo\n",
      "********************************************************************************\n",
      "Original string >> stick\n",
      "Lemmetized word >> stick\n",
      "Stemmed word >> stick\n",
      "********************************************************************************\n",
      "Original string >> sweet\n",
      "Lemmetized word >> sweet\n",
      "Stemmed word >> sweet\n",
      "********************************************************************************\n",
      "Original string >> carry\n",
      "Lemmetized word >> carry\n",
      "Stemmed word >> carry\n",
      "********************************************************************************\n",
      "Original string >> home\n",
      "Lemmetized word >> home\n",
      "Stemmed word >> hom\n",
      "********************************************************************************\n",
      "Original string >> thus\n",
      "Lemmetized word >> thus\n",
      "Stemmed word >> thu\n",
      "********************************************************************************\n",
      "Original string >> way\n",
      "Lemmetized word >> way\n",
      "Stemmed word >> way\n",
      "********************************************************************************\n",
      "Original string >> clean\n",
      "Lemmetized word >> clean\n",
      "Stemmed word >> cle\n",
      "********************************************************************************\n",
      "Original string >> floor.ants\n",
      "Lemmetized word >> floor.ants\n",
      "Stemmed word >> floor.ants\n",
      "********************************************************************************\n",
      "Original string >> generally\n",
      "Lemmetized word >> generally\n",
      "Stemmed word >> gen\n",
      "********************************************************************************\n",
      "Original string >> red\n",
      "Lemmetized word >> red\n",
      "Stemmed word >> red\n",
      "********************************************************************************\n",
      "Original string >> black\n",
      "Lemmetized word >> black\n",
      "Stemmed word >> black\n",
      "********************************************************************************\n",
      "Original string >> colour\n",
      "Lemmetized word >> colour\n",
      "Stemmed word >> colo\n",
      "********************************************************************************\n",
      "Original string >> two\n",
      "Lemmetized word >> two\n",
      "Stemmed word >> two\n",
      "********************************************************************************\n",
      "Original string >> eyes\n",
      "Lemmetized word >> eye\n",
      "Stemmed word >> ey\n",
      "********************************************************************************\n",
      "Original string >> six\n",
      "Lemmetized word >> six\n",
      "Stemmed word >> six\n",
      "********************************************************************************\n",
      "Original string >> legs\n",
      "Lemmetized word >> leg\n",
      "Stemmed word >> leg\n",
      "********************************************************************************\n",
      "Original string >> social\n",
      "Lemmetized word >> social\n",
      "Stemmed word >> soc\n",
      "********************************************************************************\n",
      "Original string >> insects\n",
      "Lemmetized word >> insect\n",
      "Stemmed word >> insect\n",
      "********************************************************************************\n",
      "Original string >> live\n",
      "Lemmetized word >> live\n",
      "Stemmed word >> liv\n",
      "********************************************************************************\n",
      "Original string >> groups\n",
      "Lemmetized word >> group\n",
      "Stemmed word >> group\n",
      "********************************************************************************\n",
      "Original string >> colonies\n",
      "Lemmetized word >> colony\n",
      "Stemmed word >> colony\n",
      "********************************************************************************\n",
      "Original string >> ants\n",
      "Lemmetized word >> ant\n",
      "Stemmed word >> ant\n",
      "********************************************************************************\n",
      "Original string >> scavengers\n",
      "Lemmetized word >> scavenger\n",
      "Stemmed word >> scaveng\n",
      "********************************************************************************\n",
      "Original string >> collect\n",
      "Lemmetized word >> collect\n",
      "Stemmed word >> collect\n",
      "********************************************************************************\n",
      "Original string >> whatever\n",
      "Lemmetized word >> whatever\n",
      "Stemmed word >> whatev\n",
      "********************************************************************************\n",
      "Original string >> food\n",
      "Lemmetized word >> food\n",
      "Stemmed word >> food\n",
      "********************************************************************************\n",
      "Original string >> find\n",
      "Lemmetized word >> find\n",
      "Stemmed word >> find\n",
      "********************************************************************************\n",
      "Original string >> usually\n",
      "Lemmetized word >> usually\n",
      "Stemmed word >> us\n",
      "********************************************************************************\n",
      "Original string >> wingless\n",
      "Lemmetized word >> wingless\n",
      "Stemmed word >> wingless\n",
      "********************************************************************************\n",
      "Original string >> develop\n",
      "Lemmetized word >> develop\n",
      "Stemmed word >> develop\n",
      "********************************************************************************\n",
      "Original string >> wings\n",
      "Lemmetized word >> wing\n",
      "Stemmed word >> wing\n",
      "********************************************************************************\n",
      "Original string >> reproduce\n",
      "Lemmetized word >> reproduce\n",
      "Stemmed word >> reproduc\n",
      "********************************************************************************\n",
      "Original string >> bites\n",
      "Lemmetized word >> bite\n",
      "Stemmed word >> bit\n",
      "********************************************************************************\n",
      "Original string >> quite\n",
      "Lemmetized word >> quite\n",
      "Stemmed word >> quit\n",
      "********************************************************************************\n",
      "Original string >> painful\n",
      "Lemmetized word >> painful\n",
      "Stemmed word >> pain\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# We are facing the issue for can't at such times we need to do it manually using replace.\n",
    "# stemming and lemmatization \n",
    "stemming = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# We cannot give direct data  to prunners , we need to give it word by word.\n",
    "for word in text_without_stopwords:\n",
    "    stemmed_word = stemming.stem(word)\n",
    "    lemmetized_word = lemmatizer.lemmatize(word)\n",
    "    print(f\"Original string >> {word}\")\n",
    "    print(f\"Lemmetized word >> {lemmetized_word}\")\n",
    "    print(f\"Stemmed word >> {stemmed_word}\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O,o,O,o,N,n,O,o'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling accented characters\n",
    "accented_characters = \"Ô,ô,Ò,ò,Ñ,ñ,Ö,ö\"\n",
    "fixed_words = unidecode(accented_characters)\n",
    "fixed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"Mussage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service\n"
     ]
    }
   ],
   "source": [
    "print(spell(\"Survice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"mussage\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "a = \"survice\"\n",
    "b = TextBlob(a)\n",
    "print(b.correct())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are not as effective because mussage can be corrected to massage also.\n",
    "# We should use trial and error for autocorrect libraries\n",
    "# We can try different combinations in preprocessing and we can check result in evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In Feature enginnering we saw about frequency based word embeddings and prediction based word embeddings.We wil study about frequency based word embeddings in which 1st one is Count Vectorizer.\n",
    "# Count vectorizer\n",
    "* Every count of word in every document will be calculated.\n",
    "> Suppose we get 2 documents \n",
    "1. We are learning Data Science.\n",
    "2. Data Science is a combination of Deep Learning and Machine Learning.\n",
    "* The data we will get in Feature enginnering will be preprocessed. So above data will become \n",
    "1. learning Data Science\n",
    "2. Data Science combination Deep Learning Machine Learning\n",
    "* After getting this preprocessed data to Count Vectorizer thenk the Count Vectorizer will find the unique words.\n",
    "* Count vectorizer will make the list of unique words.\n",
    "* [learning , Data , Science , combination , Deep , Machine]\n",
    "* Then it will create a structure where every unique word will be a independent variable.\n",
    "* Number of rows will be equal to the number of documents.\n",
    "* It will take count of every unique word(Independent variable) in every document.\n",
    "* And the structure or dataframe we got by Count Vectorizer can be commonly called as Document Term Matrix/Bag of Words / Bag of tokens.\n",
    "> When our count vectorizer gets texting data\n",
    "* test data = NLP needs both Machine learning and Deep learning.\n",
    "* Preprocessing is also done on testing data so after preprocessing we will get data as \n",
    "* test data = nlp needs machine learning deep learning\n",
    "* But as we compare with 1 and 2 documents as training data we dont have (nlp and needs) as unique words/ Independent variables , so for this purpose we have inbuilt feature in count vectorizer.\n",
    "> OOV (Out of Vocabulary)\n",
    "* In OOV unseen words are taken into account , as we have 2 unseen words or OOV score will be 2.\n",
    "> Drawbacks of Count Vectorizer \n",
    "* Curse of Dimensionality \n",
    "* Order is not maintained \n",
    "* It is not considering actual meaning of words.\n",
    "* \n",
    "# TF-IDF \n",
    "* It is a combination of 2 things \n",
    "1. Term frequency\n",
    "2. Inverse documents frequency\n",
    "* \n",
    "1. Term frequency \n",
    "* Formula : Frequency of term \"t\" in a document / Total number of words in that document\n",
    "> Example : Data Science is a combination of Machine learning and Deep learning.\n",
    "* TF-IDF is a feature enginnering technique so we will get preprocessed data.\n",
    "> So we will get : Data Science combination Machine learning Deep Learning \n",
    "* Term Frequency (learning) : 2 / 7\n",
    "* Term Frequency (Data) : 1 / 7 \n",
    "2. IDF (Inverse document frequency)\n",
    "> Document frequency \n",
    "* Formula : Number of documents containing term \"t\" / Total number of documents\n",
    "* Suppose we have 2 documents \n",
    "> NLP needs deep learning and machine learning.\n",
    "> data science contains lots of things like NLP.\n",
    "* Document frequency (NLP) : 2 / 2 : 1 \n",
    "* Document frequency (learning) : 1 / 2 : 0.5 \n",
    "> IDF (Inverse document frequency)\n",
    "* Formula : log(1 / Document frequency)\n",
    "* Formula : log(Total number of documnets / Number of documents containing term \"t\")\n",
    "* IDF (NLP) : log(1 / 1) = log(1) = 0 \n",
    "* IDF (learning) : log(1 / (1/2)) = log(2) \n",
    "> TF-IDF \n",
    "* Formula : TF * IDF \n",
    "> Working of TF-IDF\n",
    "* Document1 : We are learning NLP.\n",
    "* Document2 : Data Science is a combination of Deep learning and Machine learning.\n",
    "* Document3 : NLP needs both Machine learning and Deep learning.\n",
    "* After preprocessing :\n",
    "* Document1 : learning nlp\n",
    "* Document2 : data science combination deep learning machine learning\n",
    "* Document3 : nlp needs machine learning deep learning\n",
    "* It will find out the unique words.\n",
    "* Every unique word will now be a Independent Variable.\n",
    "* Unique words : learning nlp data science combination deep machine needs\n",
    "* We have 3 documents so we will have 3 rows.\n",
    "* We will calculate TF-IDF of every word in every document.\n",
    "* \n",
    "* TF-IDF is calculating weightage.\n",
    "* The more the frequency of the word less will be the weightage of the word.\n",
    "* \n",
    "* Example of Movie reviews:\n",
    "* Bad movie , Awesome movie , Fabulous Movie likewise we have 100 reviews.\n",
    "* In above example Movie word is not important. In sentiment analysis bad , awesome , Fabulous words are more important.\n",
    "> Drawbacks of TF-IDF \n",
    "* Curse of Dimensionality \n",
    "* Order is not maintained \n",
    "* It is not considering actual meaning of the words.\n",
    "> Simialrities between Count Vectorizer and TF-IDF\n",
    "* They have same drawbacks.\n",
    "* When we initiate models of Count Vectorizer and TF-IDF there parameters used are same.\n",
    "* \n",
    "> Difference between Count Vectorizer and TF-IDF \n",
    "* Count Vectorizer counts frequency of each word in every document.\n",
    "* TF-IDF counts weightage of each word in every document.\n",
    "* Difference of accuracy of models of count vectorizer and TF-IDF is low.But this difference matters in industry. Whichever model will give the highest accuracy that model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autocorrect import Speller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "#1 . remove spaces , newlines \n",
    "def remove_spaces(data):\n",
    "    clean_text = data.replace(\"\\\\n\" , \" \").replace(\"\\t\" , \" \").replace(\"\\\\\" , \" \")\n",
    "    return clean_text\n",
    "#2. Contraction mapping\n",
    "def expand_test(data):\n",
    "    expanded_text = contractions.fix(data)\n",
    "    return expanded_text\n",
    "\n",
    "#3. Handling accented characters \n",
    "def handling_accented(data):\n",
    "    fixed_text = unidecode(data)\n",
    "    return fixed_text\n",
    "\n",
    "#4. cleaning \n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.remove(\"no\")\n",
    "stopword_list.remove(\"nor\")\n",
    "stopword_list.remove(\"not\")\n",
    "\n",
    "def clean_data(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    clean_text = [word.lower() for word in tokens if (word not in punctuation) and (word.lower() not in stopword_list) and (len(word)>2) and (word.isalpha())]\n",
    "    return clean_text\n",
    "\n",
    "#5. Autocorrect\n",
    "def autocorrection(data):\n",
    "    spell = Speller(lang=\"en\")\n",
    "    corrected_text = spell(data)\n",
    "    return corrected_text\n",
    "\n",
    "# Lemmetization \n",
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_data = []\n",
    "    for word in data:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        final_data.append(lemmatized_word)\n",
    "    return \" \".join(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer and TF-IDF >>> String Format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I grew up (b. 1965) watching and loving the Th...\n",
       "1        When I put this movie in my DVD player, and sa...\n",
       "2        Why do people who do not know what a particula...\n",
       "3        Even though I have great interest in Biblical ...\n",
       "4        Im a die hard Dads Army fan and nothing will e...\n",
       "                               ...                        \n",
       "39995    \"Western Union\" is something of a forgotten cl...\n",
       "39996    This movie is an incredible piece of work. It ...\n",
       "39997    My wife and I watched this movie because we pl...\n",
       "39998    When I first watched Flatliners, I was amazed....\n",
       "39999    Why would this film be so good, but only gross...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "39995    1\n",
       "39996    1\n",
       "39997    0\n",
       "39998    1\n",
       "39999    1\n",
       "Name: label, Length: 40000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage \n",
    "x_train,x_test,y_train,y_test = train_test_split(data.text,data.label , test_size=0.25 , random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_train = x_train.apply(remove_spaces)\n",
    "clean_text_test = x_test.apply(remove_spaces)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(expand_test)\n",
    "clean_text_test = clean_text_test.apply(expand_test)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(handling_accented)\n",
    "clean_text_test = clean_text_test.apply(handling_accented)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(clean_data)\n",
    "clean_text_test = clean_text_test.apply(clean_data)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(lemmatization)\n",
    "clean_text_test = clean_text_test.apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26898    fifth grade language art teacher read book stu...\n",
       "27635    low budget brit pop melodrama focus girl want ...\n",
       "3036     well watched movie little year ago pulled dust...\n",
       "5604     would almost give however confusing part well ...\n",
       "36111    full length feature film world bridge found fi...\n",
       "                               ...                        \n",
       "6265     movie one worst movie ever seen life waste tim...\n",
       "11284    movie inspiring anyone tough jam whether finan...\n",
       "38158    east side story documentary musical comedy sta...\n",
       "860      one boot one point doctor assistant refers wor...\n",
       "15795    movie horrible lighting terrible camera moveme...\n",
       "Name: text, Length: 30000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer \n",
    "count = CountVectorizer(max_df=0.95 , max_features=1000 , ngram_range=(2,2))\n",
    "count_val_train = count.fit_transform(clean_text_train)\n",
    "count_val_test = count.fit_transform(clean_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 248311 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolutely nothing', 'academy award', 'act like', 'acting bad',\n",
       "       'acting good', 'acting not', 'acting poor', 'action film',\n",
       "       'action movie', 'action scene', 'action sequence', 'actor actress',\n",
       "       'actor film', 'actor like', 'actor not', 'actually not',\n",
       "       'actually quite', 'almost every', 'along line', 'along way',\n",
       "       'also good', 'also not', 'also one', 'although not',\n",
       "       'another film', 'another movie', 'another one', 'anyone could',\n",
       "       'anyone else', 'anyone not', 'anyone would', 'anything else',\n",
       "       'aspect film', 'audience not', 'avoid cost', 'awful movie',\n",
       "       'bad acting', 'bad bad', 'bad enough', 'bad film', 'bad good',\n",
       "       'bad guy', 'bad movie', 'bad not', 'bad one', 'bad thing',\n",
       "       'based true', 'beginning end', 'beginning film', 'beginning movie',\n",
       "       'believe movie', 'believe not', 'best actor', 'best film',\n",
       "       'best friend', 'best movie', 'best part', 'best performance',\n",
       "       'best picture', 'best thing', 'best work', 'better film',\n",
       "       'better movie', 'better not', 'big budget', 'big fan',\n",
       "       'big screen', 'black white', 'blah blah', 'book not',\n",
       "       'bottom line', 'box office', 'brad pitt', 'bring back',\n",
       "       'came across', 'camera work', 'car chase', 'cast crew',\n",
       "       'cast member', 'cast not', 'certainly not', 'chance see',\n",
       "       'character actor', 'character could', 'character development',\n",
       "       'character film', 'character like', 'character movie',\n",
       "       'character not', 'character one', 'character played',\n",
       "       'character really', 'character well', 'civil war', 'come across',\n",
       "       'come back', 'come close', 'come mind', 'comedy not', 'comic book',\n",
       "       'comic relief', 'completely different', 'could better',\n",
       "       'could done', 'could easily', 'could get', 'could made',\n",
       "       'could make', 'could much', 'could not', 'could see', 'course not',\n",
       "       'david lynch', 'death scene', 'definitely not', 'definitely one',\n",
       "       'definitely worth', 'despite fact', 'director not', 'done better',\n",
       "       'done well', 'edge seat', 'effect not', 'either way', 'end film',\n",
       "       'end movie', 'end not', 'ending not', 'enjoy film', 'enjoy movie',\n",
       "       'enjoyed movie', 'enough make', 'entire film', 'entire movie',\n",
       "       'even better', 'even get', 'even good', 'even know', 'even not',\n",
       "       'even think', 'even though', 'even worse', 'ever made', 'ever see',\n",
       "       'ever seen', 'ever since', 'every day', 'every episode',\n",
       "       'every scene', 'every single', 'every time', 'everyone else',\n",
       "       'everything else', 'excellent film', 'eye candy',\n",
       "       'facial expression', 'fact not', 'fall flat', 'fall love',\n",
       "       'falling love', 'far away', 'far superior', 'favorite movie',\n",
       "       'feature film', 'feel like', 'feel sorry', 'felt like',\n",
       "       'fight scene', 'film actually', 'film also', 'film bad',\n",
       "       'film based', 'film begin', 'film best', 'film certainly',\n",
       "       'film character', 'film come', 'film could', 'film definitely',\n",
       "       'film director', 'film end', 'film especially', 'film even',\n",
       "       'film ever', 'film feel', 'film festival', 'film film',\n",
       "       'film first', 'film get', 'film good', 'film great',\n",
       "       'film however', 'film like', 'film little', 'film look',\n",
       "       'film lot', 'film made', 'film make', 'film maker', 'film making',\n",
       "       'film many', 'film may', 'film might', 'film movie', 'film much',\n",
       "       'film must', 'film never', 'film noir', 'film not', 'film nothing',\n",
       "       'film one', 'film people', 'film plot', 'film quite',\n",
       "       'film really', 'film see', 'film seems', 'film seen', 'film set',\n",
       "       'film show', 'film start', 'film still', 'film story', 'film take',\n",
       "       'film think', 'film though', 'film time', 'film two',\n",
       "       'film version', 'film watch', 'film way', 'film well', 'film work',\n",
       "       'film worth', 'film would', 'film year', 'final scene',\n",
       "       'find movie', 'find way', 'first episode', 'first film',\n",
       "       'first half', 'first minute', 'first movie', 'first not',\n",
       "       'first one', 'first place', 'first saw', 'first scene',\n",
       "       'first thing', 'first time', 'first two', 'five minute',\n",
       "       'five year', 'found film', 'found movie', 'fun movie', 'fun watch',\n",
       "       'funny moment', 'funny movie', 'funny not', 'get away', 'get back',\n",
       "       'get better', 'get chance', 'get good', 'get killed', 'get know',\n",
       "       'get movie', 'get not', 'get see', 'get wrong', 'girl not',\n",
       "       'give film', 'give good', 'give movie', 'give one', 'going get',\n",
       "       'going happen', 'good acting', 'good actor', 'good bad',\n",
       "       'good enough', 'good film', 'good guy', 'good idea', 'good job',\n",
       "       'good laugh', 'good look', 'good looking', 'good movie',\n",
       "       'good not', 'good old', 'good one', 'good performance',\n",
       "       'good point', 'good reason', 'good story', 'good thing',\n",
       "       'good time', 'good way', 'great acting', 'great actor',\n",
       "       'great deal', 'great film', 'great job', 'great movie',\n",
       "       'great performance', 'great see', 'great story', 'guess not',\n",
       "       'guy not', 'half hour', 'half movie', 'happy ending',\n",
       "       'hard believe', 'high school', 'highly recommend',\n",
       "       'highly recommended', 'hong kong', 'horror fan', 'horror film',\n",
       "       'horror flick', 'horror movie', 'hour half', 'however not',\n",
       "       'huge fan', 'human being', 'independent film',\n",
       "       'interesting character', 'james bond', 'joke not', 'kid movie',\n",
       "       'kid not', 'kind film', 'kind like', 'kind movie', 'know going',\n",
       "       'know movie', 'know not', 'last minute', 'last night', 'last one',\n",
       "       'last scene', 'last year', 'late night', 'laugh loud',\n",
       "       'lead actor', 'lead character', 'leading lady', 'leading man',\n",
       "       'least not', 'least one', 'let alone', 'let know', 'let say',\n",
       "       'let see', 'life not', 'like film', 'like good', 'like many',\n",
       "       'like movie', 'like not', 'like one', 'like people', 'like real',\n",
       "       'like see', 'like something', 'like watching', 'like would',\n",
       "       'liked movie', 'line not', 'little bit', 'little boy',\n",
       "       'little film', 'little girl', 'little kid', 'long time',\n",
       "       'look good', 'look like', 'looked like', 'looking forward',\n",
       "       'looking like', 'los angeles', 'lot better', 'lot fun',\n",
       "       'lot movie', 'lot people', 'love film', 'love interest',\n",
       "       'love movie', 'love not', 'love story', 'loved movie',\n",
       "       'low budget', 'made film', 'made laugh', 'made movie', 'made not',\n",
       "       'made sense', 'main character', 'main reason', 'make feel',\n",
       "       'make film', 'make good', 'make great', 'make laugh', 'make look',\n",
       "       'make movie', 'make much', 'make one', 'make sense', 'make sure',\n",
       "       'make think', 'make want', 'making film', 'making movie',\n",
       "       'man not', 'many film', 'many many', 'many movie', 'many people',\n",
       "       'many scene', 'many thing', 'many time', 'many way', 'many year',\n",
       "       'martial art', 'may not', 'may well', 'maybe not', 'men woman',\n",
       "       'michael caine', 'might not', 'might well', 'million dollar',\n",
       "       'minute film', 'minute long', 'minute movie', 'minute not',\n",
       "       'modern day', 'moment film', 'motion picture', 'movie acting',\n",
       "       'movie actually', 'movie almost', 'movie also', 'movie anyone',\n",
       "       'movie bad', 'movie based', 'movie best', 'movie come',\n",
       "       'movie could', 'movie end', 'movie even', 'movie ever',\n",
       "       'movie fan', 'movie feel', 'movie film', 'movie first',\n",
       "       'movie get', 'movie give', 'movie going', 'movie good',\n",
       "       'movie got', 'movie great', 'movie however', 'movie kid',\n",
       "       'movie kind', 'movie know', 'movie least', 'movie like',\n",
       "       'movie little', 'movie look', 'movie lot', 'movie made',\n",
       "       'movie make', 'movie many', 'movie may', 'movie might',\n",
       "       'movie movie', 'movie much', 'movie must', 'movie never',\n",
       "       'movie not', 'movie nothing', 'movie one', 'movie people',\n",
       "       'movie plot', 'movie pretty', 'movie probably', 'movie quite',\n",
       "       'movie real', 'movie really', 'movie saw', 'movie say',\n",
       "       'movie see', 'movie seems', 'movie seen', 'movie show',\n",
       "       'movie star', 'movie start', 'movie still', 'movie story',\n",
       "       'movie take', 'movie terrible', 'movie theater', 'movie think',\n",
       "       'movie thought', 'movie time', 'movie try', 'movie want',\n",
       "       'movie watch', 'movie way', 'movie well', 'movie without',\n",
       "       'movie worth', 'movie would', 'movie year', 'much better',\n",
       "       'much film', 'much fun', 'much le', 'much like', 'much movie',\n",
       "       'much not', 'much time', 'murder mystery', 'music not',\n",
       "       'music video', 'musical number', 'musical score', 'must admit',\n",
       "       'must say', 'must see', 'near end', 'need see', 'needle say',\n",
       "       'never get', 'never got', 'never heard', 'never made',\n",
       "       'never really', 'never see', 'never seen', 'new york', 'next time',\n",
       "       'nice see', 'not able', 'not act', 'not actually', 'not always',\n",
       "       'not anything', 'not bad', 'not believe', 'not best', 'not better',\n",
       "       'not big', 'not bother', 'not buy', 'not care', 'not come',\n",
       "       'not disappointed', 'not done', 'not easy', 'not enjoy',\n",
       "       'not enough', 'not entirely', 'not even', 'not ever',\n",
       "       'not everyone', 'not exactly', 'not exist', 'not expect',\n",
       "       'not expecting', 'not fan', 'not far', 'not feel', 'not film',\n",
       "       'not find', 'not fit', 'not forget', 'not funny', 'not get',\n",
       "       'not give', 'not going', 'not good', 'not great', 'not help',\n",
       "       'not imagine', 'not know', 'not laugh', 'not least', 'not let',\n",
       "       'not like', 'not long', 'not look', 'not love', 'not made',\n",
       "       'not make', 'not many', 'not matter', 'not mean', 'not mention',\n",
       "       'not mind', 'not miss', 'not movie', 'not much', 'not nearly',\n",
       "       'not necessarily', 'not need', 'not not', 'not one',\n",
       "       'not particularly', 'not pay', 'not perfect', 'not play',\n",
       "       'not put', 'not quite', 'not read', 'not real', 'not really',\n",
       "       'not recommend', 'not remember', 'not save', 'not say',\n",
       "       'not saying', 'not scary', 'not see', 'not seem', 'not seen',\n",
       "       'not show', 'not single', 'not stand', 'not stop', 'not sure',\n",
       "       'not surprised', 'not take', 'not taken', 'not tell', 'not think',\n",
       "       'not try', 'not understand', 'not wait', 'not want', 'not waste',\n",
       "       'not watch', 'not well', 'not without', 'not work', 'not worst',\n",
       "       'not worth', 'not yet', 'nothing else', 'nothing like',\n",
       "       'nothing new', 'nothing special', 'obviously not', 'old man',\n",
       "       'one another', 'one best', 'one better', 'one big',\n",
       "       'one character', 'one could', 'one day', 'one episode',\n",
       "       'one favorite', 'one film', 'one first', 'one get', 'one good',\n",
       "       'one great', 'one greatest', 'one last', 'one like', 'one man',\n",
       "       'one many', 'one might', 'one movie', 'one night', 'one not',\n",
       "       'one one', 'one person', 'one point', 'one really', 'one reason',\n",
       "       'one scene', 'one see', 'one thing', 'one time', 'one two',\n",
       "       'one way', 'one wonder', 'one worst', 'one would',\n",
       "       'opening credit', 'opening scene', 'opening sequence',\n",
       "       'original film', 'original movie', 'outer space', 'painful watch',\n",
       "       'part film', 'part movie', 'part not', 'pay attention',\n",
       "       'people get', 'people like', 'people not', 'people say',\n",
       "       'people think', 'people would', 'performance not', 'piece crap',\n",
       "       'pleasantly surprised', 'please not', 'plot hole', 'plot line',\n",
       "       'plot movie', 'plot not', 'plot point', 'plot twist', 'point film',\n",
       "       'point movie', 'point not', 'point view', 'police officer',\n",
       "       'pretty bad', 'pretty good', 'pretty much', 'probably best',\n",
       "       'probably not', 'probably one', 'probably would', 'problem film',\n",
       "       'problem movie', 'production value', 'put together', 'quite bit',\n",
       "       'quite good', 'quite well', 'read book', 'real life',\n",
       "       'real people', 'really bad', 'really care', 'really enjoyed',\n",
       "       'really funny', 'really get', 'really good', 'really great',\n",
       "       'really know', 'really like', 'really liked', 'really make',\n",
       "       'really not', 'really really', 'really think', 'really want',\n",
       "       'reason not', 'recommend anyone', 'recommend film',\n",
       "       'recommend movie', 'redeeming quality', 'rest cast', 'rest film',\n",
       "       'rest movie', 'revolves around', 'robin williams', 'role not',\n",
       "       'romantic comedy', 'run away', 'running around', 'running time',\n",
       "       'said not', 'saturday night', 'saw film', 'saw movie', 'say film',\n",
       "       'say least', 'say movie', 'say not', 'say one', 'scary movie',\n",
       "       'scene film', 'scene movie', 'scene not', 'scene one',\n",
       "       'science fiction', 'screen time', 'script not', 'second half',\n",
       "       'second time', 'see film', 'see movie', 'see not', 'see one',\n",
       "       'see something', 'seeing film', 'seeing movie', 'seem like',\n",
       "       'seemed like', 'seems like', 'seen film', 'seen many',\n",
       "       'seen movie', 'seen not', 'seen one', 'sense humor',\n",
       "       'serial killer', 'series not', 'several time', 'several year',\n",
       "       'sex scene', 'short film', 'show like', 'show not', 'silent film',\n",
       "       'simply not', 'since not', 'slasher film', 'small town',\n",
       "       'soap opera', 'someone else', 'something else', 'something like',\n",
       "       'something not', 'something would', 'sound effect', 'sound like',\n",
       "       'special effect', 'star trek', 'star war', 'start finish',\n",
       "       'stay away', 'still not', 'story character', 'story film',\n",
       "       'story line', 'story movie', 'story not', 'story one',\n",
       "       'story told', 'subject matter', 'supporting cast',\n",
       "       'supporting character', 'supporting role', 'sure not', 'take away',\n",
       "       'take place', 'take seriously', 'tell story', 'ten minute',\n",
       "       'ten year', 'terrible movie', 'thank god', 'thing film',\n",
       "       'thing get', 'thing happen', 'thing like', 'thing movie',\n",
       "       'thing not', 'thing really', 'thing would', 'think could',\n",
       "       'think film', 'think movie', 'think not', 'think one',\n",
       "       'think would', 'though film', 'though not', 'thought movie',\n",
       "       'thought would', 'three time', 'throughout film',\n",
       "       'throughout movie', 'time around', 'time film', 'time money',\n",
       "       'time movie', 'time not', 'time one', 'time saw', 'time see',\n",
       "       'time still', 'time time', 'time watch', 'time watching',\n",
       "       'time would', 'tom hank', 'took place', 'top notch', 'towards end',\n",
       "       'true story', 'try get', 'try hard', 'try make', 'trying find',\n",
       "       'trying get', 'trying make', 'twenty minute', 'twist turn',\n",
       "       'two character', 'two film', 'two hour', 'two lead', 'two main',\n",
       "       'two people', 'two year', 'type movie', 'unfortunately not',\n",
       "       'united state', 'van damme', 'video game', 'video store',\n",
       "       'viewer not', 'visual effect', 'want get', 'want know', 'want see',\n",
       "       'want watch', 'wanted see', 'war movie', 'waste money',\n",
       "       'waste time', 'watch film', 'watch movie', 'watch not',\n",
       "       'watch one', 'watch show', 'watched film', 'watched movie',\n",
       "       'watching film', 'watching movie', 'way film', 'way movie',\n",
       "       'way not', 'well acted', 'well done', 'well known', 'well made',\n",
       "       'well not', 'well one', 'well worth', 'well written', 'went see',\n",
       "       'whether not', 'whole film', 'whole lot', 'whole movie',\n",
       "       'whole thing', 'wish could', 'wish would', 'without doubt',\n",
       "       'woman not', 'work art', 'work not', 'work well', 'world not',\n",
       "       'world war', 'worst film', 'worst movie', 'worth seeing',\n",
       "       'worth time', 'worth watching', 'would better', 'would ever',\n",
       "       'would expect', 'would get', 'would give', 'would like',\n",
       "       'would love', 'would made', 'would make', 'would much',\n",
       "       'would never', 'would not', 'would probably', 'would rather',\n",
       "       'would recommend', 'would say', 'would take', 'would think',\n",
       "       'would want', 'writer director', 'written directed', 'year ago',\n",
       "       'year earlier', 'year later', 'year not', 'year old',\n",
       "       'yet another', 'yet not', 'york city', 'young boy', 'young girl',\n",
       "       'young man', 'young woman'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely nothing</th>\n",
       "      <th>academy award</th>\n",
       "      <th>act like</th>\n",
       "      <th>acting bad</th>\n",
       "      <th>acting good</th>\n",
       "      <th>acting not</th>\n",
       "      <th>acting poor</th>\n",
       "      <th>action film</th>\n",
       "      <th>action movie</th>\n",
       "      <th>action scene</th>\n",
       "      <th>...</th>\n",
       "      <th>year later</th>\n",
       "      <th>year not</th>\n",
       "      <th>year old</th>\n",
       "      <th>yet another</th>\n",
       "      <th>yet not</th>\n",
       "      <th>york city</th>\n",
       "      <th>young boy</th>\n",
       "      <th>young girl</th>\n",
       "      <th>young man</th>\n",
       "      <th>young woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolutely nothing  academy award  act like  acting bad  acting good  \\\n",
       "0                       0              0         0           0            0   \n",
       "1                       0              0         0           0            0   \n",
       "2                       0              0         0           0            0   \n",
       "3                       0              0         0           0            0   \n",
       "4                       0              0         0           0            0   \n",
       "...                   ...            ...       ...         ...          ...   \n",
       "29995                   0              0         0           0            0   \n",
       "29996                   0              0         0           0            0   \n",
       "29997                   0              0         0           0            0   \n",
       "29998                   0              0         0           0            0   \n",
       "29999                   0              0         0           0            0   \n",
       "\n",
       "       acting not  acting poor  action film  action movie  action scene  ...  \\\n",
       "0               0            0            0             0             0  ...   \n",
       "1               0            0            0             0             0  ...   \n",
       "2               0            0            0             0             0  ...   \n",
       "3               0            0            0             0             0  ...   \n",
       "4               0            0            0             0             0  ...   \n",
       "...           ...          ...          ...           ...           ...  ...   \n",
       "29995           0            0            0             0             0  ...   \n",
       "29996           0            0            0             0             0  ...   \n",
       "29997           0            0            0             0             0  ...   \n",
       "29998           0            0            0             0             0  ...   \n",
       "29999           0            0            0             0             0  ...   \n",
       "\n",
       "       year later  year not  year old  yet another  yet not  york city  \\\n",
       "0               0         0         0            0        0          0   \n",
       "1               0         0         0            0        0          0   \n",
       "2               0         0         0            0        0          0   \n",
       "3               0         0         0            0        0          0   \n",
       "4               0         0         0            0        0          0   \n",
       "...           ...       ...       ...          ...      ...        ...   \n",
       "29995           0         0         0            0        0          0   \n",
       "29996           0         0         0            0        0          0   \n",
       "29997           0         0         0            0        0          0   \n",
       "29998           0         0         0            0        0          0   \n",
       "29999           0         0         1            0        0          0   \n",
       "\n",
       "       young boy  young girl  young man  young woman  \n",
       "0              0           0          0            0  \n",
       "1              0           0          0            0  \n",
       "2              0           0          0            0  \n",
       "3              0           0          0            0  \n",
       "4              0           0          0            0  \n",
       "...          ...         ...        ...          ...  \n",
       "29995          0           0          0            0  \n",
       "29996          0           0          0            0  \n",
       "29997          0           0          0            0  \n",
       "29998          0           0          0            0  \n",
       "29999          0           0          0            0  \n",
       "\n",
       "[30000 rows x 1000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(count_val_train.A,columns=count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.120000000000005"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mnb = MultinomialNB()\n",
    "count_mnb.fit(count_val_train.A,y_train)\n",
    "predict_count = count_mnb.predict(count_val_test.A)\n",
    "accuracy_count = accuracy_score(y_test , predict_count)*100\n",
    "accuracy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf \n",
    "tfidf = TfidfVectorizer(max_df=0.95 , max_features=1000)\n",
    "tfidf_train = tfidf.fit_transform(clean_text_train)\n",
    "tfidf_test = tfidf.fit_transform(clean_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1617756 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'absolutely', 'accent', 'across', 'act',\n",
       "       'acted', 'acting', 'action', 'actor', 'actress', 'actual',\n",
       "       'actually', 'adaptation', 'add', 'admit', 'adult', 'adventure',\n",
       "       'affair', 'age', 'agent', 'ago', 'agree', 'air', 'alien', 'almost',\n",
       "       'alone', 'along', 'already', 'also', 'although', 'always',\n",
       "       'amazing', 'america', 'american', 'among', 'amount', 'amusing',\n",
       "       'animal', 'animated', 'animation', 'annoying', 'another', 'answer',\n",
       "       'anyone', 'anything', 'anyway', 'apart', 'apparently', 'appear',\n",
       "       'appearance', 'appears', 'appreciate', 'army', 'around', 'art',\n",
       "       'aside', 'ask', 'aspect', 'atmosphere', 'attempt', 'attention',\n",
       "       'audience', 'average', 'avoid', 'award', 'away', 'awesome',\n",
       "       'awful', 'baby', 'back', 'background', 'bad', 'badly', 'band',\n",
       "       'based', 'basic', 'basically', 'battle', 'beat', 'beautiful',\n",
       "       'beauty', 'became', 'become', 'becomes', 'bed', 'begin',\n",
       "       'beginning', 'behind', 'believable', 'believe', 'ben', 'best',\n",
       "       'better', 'beyond', 'big', 'biggest', 'bill', 'bit', 'bizarre',\n",
       "       'black', 'blood', 'blue', 'body', 'book', 'bored', 'boring',\n",
       "       'bother', 'bought', 'box', 'boy', 'brain', 'break', 'brilliant',\n",
       "       'bring', 'brings', 'british', 'brother', 'brought', 'budget',\n",
       "       'building', 'bunch', 'business', 'buy', 'call', 'called', 'came',\n",
       "       'camera', 'camp', 'car', 'care', 'career', 'cartoon', 'case',\n",
       "       'cast', 'casting', 'cat', 'catch', 'caught', 'century', 'certain',\n",
       "       'certainly', 'chance', 'change', 'channel', 'character', 'chase',\n",
       "       'cheap', 'check', 'cheesy', 'chemistry', 'child', 'choice',\n",
       "       'cinema', 'cinematography', 'city', 'class', 'classic', 'clear',\n",
       "       'clearly', 'clever', 'cliche', 'close', 'cold', 'color', 'come',\n",
       "       'comedy', 'comic', 'coming', 'comment', 'common', 'company',\n",
       "       'compared', 'complete', 'completely', 'computer', 'concept',\n",
       "       'consider', 'considering', 'control', 'convincing', 'cool', 'cop',\n",
       "       'copy', 'costume', 'could', 'country', 'couple', 'course', 'cover',\n",
       "       'crap', 'crazy', 'create', 'created', 'creature', 'credit',\n",
       "       'creepy', 'crew', 'crime', 'critic', 'cry', 'culture', 'cut',\n",
       "       'cute', 'dad', 'dance', 'dancing', 'dark', 'daughter', 'david',\n",
       "       'day', 'dead', 'deal', 'death', 'decent', 'decide', 'decided',\n",
       "       'decides', 'deep', 'definitely', 'depth', 'deserves', 'despite',\n",
       "       'detail', 'development', 'dialog', 'dialogue', 'die', 'died',\n",
       "       'different', 'difficult', 'directed', 'directing', 'direction',\n",
       "       'director', 'disappointed', 'disney', 'doctor', 'documentary',\n",
       "       'dog', 'done', 'door', 'doubt', 'drama', 'dramatic', 'dream',\n",
       "       'drive', 'drug', 'due', 'dull', 'dumb', 'dvd', 'earlier', 'early',\n",
       "       'earth', 'easily', 'easy', 'editing', 'effect', 'effective',\n",
       "       'effort', 'either', 'element', 'else', 'emotion', 'emotional',\n",
       "       'end', 'ended', 'ending', 'english', 'enjoy', 'enjoyable',\n",
       "       'enjoyed', 'enough', 'entertaining', 'entertainment', 'entire',\n",
       "       'entirely', 'episode', 'era', 'escape', 'especially', 'etc',\n",
       "       'even', 'event', 'eventually', 'ever', 'every', 'everyone',\n",
       "       'everything', 'evil', 'exactly', 'example', 'excellent', 'except',\n",
       "       'expect', 'expected', 'expecting', 'experience', 'explain',\n",
       "       'extra', 'extremely', 'eye', 'face', 'fact', 'fails', 'fair',\n",
       "       'fairly', 'fake', 'fall', 'familiar', 'family', 'famous', 'fan',\n",
       "       'fantastic', 'fantasy', 'far', 'fast', 'father', 'favorite',\n",
       "       'fear', 'feature', 'feel', 'feeling', 'felt', 'female', 'fiction',\n",
       "       'fight', 'fighting', 'figure', 'film', 'filmed', 'filmmaker',\n",
       "       'final', 'finally', 'find', 'fine', 'fire', 'first', 'fit', 'five',\n",
       "       'flat', 'flick', 'focus', 'follow', 'following', 'follows',\n",
       "       'footage', 'force', 'forced', 'forget', 'form', 'former',\n",
       "       'forward', 'found', 'four', 'free', 'french', 'friend', 'front',\n",
       "       'full', 'fun', 'funny', 'future', 'game', 'gang', 'garbage',\n",
       "       'gave', 'gay', 'general', 'genius', 'genre', 'george', 'german',\n",
       "       'get', 'getting', 'ghost', 'girl', 'girlfriend', 'give', 'given',\n",
       "       'giving', 'go', 'god', 'going', 'gone', 'good', 'gore', 'got',\n",
       "       'government', 'great', 'greatest', 'group', 'guess', 'gun', 'guy',\n",
       "       'hair', 'half', 'hand', 'happen', 'happened', 'happens', 'happy',\n",
       "       'hard', 'hardly', 'hate', 'head', 'hear', 'heard', 'heart', 'hell',\n",
       "       'help', 'hero', 'high', 'highly', 'hilarious', 'history', 'hit',\n",
       "       'hold', 'hole', 'hollywood', 'home', 'honestly', 'hope',\n",
       "       'horrible', 'horror', 'hot', 'hour', 'house', 'however', 'huge',\n",
       "       'human', 'humor', 'husband', 'idea', 'image', 'imagine', 'imdb',\n",
       "       'important', 'impossible', 'including', 'incredible', 'incredibly',\n",
       "       'indeed', 'indian', 'inside', 'instead', 'interest', 'interested',\n",
       "       'interesting', 'involved', 'island', 'issue', 'italian', 'jack',\n",
       "       'james', 'jane', 'japanese', 'job', 'joe', 'john', 'joke', 'keep',\n",
       "       'kelly', 'kept', 'kid', 'kill', 'killed', 'killer', 'killing',\n",
       "       'kind', 'king', 'knew', 'know', 'known', 'lack', 'lady', 'lame',\n",
       "       'land', 'language', 'large', 'last', 'late', 'later', 'laugh',\n",
       "       'laughing', 'law', 'le', 'lead', 'leading', 'leaf', 'learn',\n",
       "       'least', 'leave', 'leaving', 'lee', 'left', 'let', 'level', 'lie',\n",
       "       'life', 'light', 'like', 'liked', 'line', 'list', 'literally',\n",
       "       'little', 'live', 'living', 'local', 'location', 'long', 'look',\n",
       "       'looked', 'looking', 'lost', 'lot', 'love', 'loved', 'lover',\n",
       "       'low', 'machine', 'mad', 'made', 'main', 'major', 'make', 'maker',\n",
       "       'making', 'male', 'man', 'manages', 'many', 'mark', 'married',\n",
       "       'master', 'masterpiece', 'match', 'material', 'matter', 'may',\n",
       "       'maybe', 'mean', 'meaning', 'meant', 'meet', 'member', 'memorable',\n",
       "       'memory', 'men', 'mention', 'mentioned', 'mess', 'message',\n",
       "       'michael', 'middle', 'might', 'million', 'mind', 'minute', 'miss',\n",
       "       'missed', 'missing', 'mistake', 'modern', 'moment', 'money',\n",
       "       'monster', 'mood', 'mostly', 'mother', 'move', 'movie', 'moving',\n",
       "       'much', 'murder', 'music', 'musical', 'must', 'mystery', 'naked',\n",
       "       'name', 'named', 'nature', 'near', 'nearly', 'need', 'needed',\n",
       "       'neither', 'never', 'new', 'next', 'nice', 'night', 'none', 'nor',\n",
       "       'not', 'note', 'nothing', 'notice', 'novel', 'nudity', 'number',\n",
       "       'obvious', 'obviously', 'odd', 'offer', 'office', 'often', 'okay',\n",
       "       'old', 'older', 'one', 'open', 'opening', 'opinion', 'order',\n",
       "       'original', 'oscar', 'others', 'otherwise', 'outside', 'overall',\n",
       "       'pace', 'parent', 'park', 'part', 'particular', 'particularly',\n",
       "       'party', 'past', 'paul', 'pay', 'people', 'perfect', 'perfectly',\n",
       "       'performance', 'perhaps', 'period', 'person', 'personal', 'peter',\n",
       "       'pick', 'picture', 'piece', 'place', 'plain', 'plan', 'play',\n",
       "       'played', 'player', 'playing', 'please', 'plenty', 'plot', 'plus',\n",
       "       'point', 'pointless', 'police', 'political', 'poor', 'poorly',\n",
       "       'popular', 'portrayal', 'portrayed', 'positive', 'possible',\n",
       "       'possibly', 'potential', 'power', 'powerful', 'predictable',\n",
       "       'premise', 'present', 'pretty', 'previous', 'prison', 'probably',\n",
       "       'problem', 'produced', 'producer', 'production', 'project',\n",
       "       'public', 'pull', 'pure', 'purpose', 'put', 'quality', 'question',\n",
       "       'quickly', 'quite', 'rate', 'rather', 'rating', 'read', 'reading',\n",
       "       'real', 'realistic', 'reality', 'realize', 'really', 'reason',\n",
       "       'recently', 'recommend', 'recommended', 'red', 'reference',\n",
       "       'relationship', 'release', 'released', 'remains', 'remake',\n",
       "       'remember', 'rent', 'respect', 'rest', 'result', 'return',\n",
       "       'review', 'rich', 'richard', 'ridiculous', 'right', 'road',\n",
       "       'robert', 'rock', 'role', 'romance', 'romantic', 'room', 'run',\n",
       "       'running', 'sad', 'sadly', 'said', 'save', 'saw', 'say', 'saying',\n",
       "       'scary', 'scene', 'school', 'science', 'scientist', 'score',\n",
       "       'scott', 'screen', 'screenplay', 'script', 'season', 'second',\n",
       "       'secret', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen',\n",
       "       'sense', 'sequel', 'sequence', 'series', 'serious', 'seriously',\n",
       "       'set', 'setting', 'several', 'sex', 'sexual', 'shame', 'shoot',\n",
       "       'short', 'shot', 'show', 'showing', 'shown', 'sick', 'side',\n",
       "       'silent', 'silly', 'similar', 'simple', 'simply', 'since',\n",
       "       'single', 'sister', 'sit', 'situation', 'slasher', 'slightly',\n",
       "       'slow', 'small', 'social', 'society', 'soldier', 'somehow',\n",
       "       'someone', 'something', 'sometimes', 'somewhat', 'son', 'song',\n",
       "       'soon', 'sorry', 'sort', 'soul', 'sound', 'soundtrack', 'south',\n",
       "       'space', 'speak', 'special', 'spent', 'spirit', 'spoiler', 'stage',\n",
       "       'stand', 'standard', 'star', 'start', 'started', 'state', 'stay',\n",
       "       'steal', 'stick', 'still', 'stop', 'store', 'story', 'storyline',\n",
       "       'straight', 'strange', 'street', 'strong', 'struggle', 'student',\n",
       "       'studio', 'stuff', 'stupid', 'style', 'subject', 'success',\n",
       "       'successful', 'suddenly', 'superb', 'supporting', 'supposed',\n",
       "       'sure', 'surprise', 'surprised', 'suspense', 'sweet', 'take',\n",
       "       'taken', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking',\n",
       "       'taste', 'team', 'teen', 'teenager', 'television', 'tell',\n",
       "       'telling', 'ten', 'tension', 'term', 'terrible', 'thanks',\n",
       "       'theater', 'theme', 'thing', 'think', 'thinking', 'third',\n",
       "       'though', 'thought', 'three', 'thriller', 'throughout', 'throw',\n",
       "       'time', 'title', 'today', 'together', 'told', 'tom', 'tone',\n",
       "       'took', 'top', 'total', 'totally', 'touch', 'towards', 'town',\n",
       "       'track', 'train', 'trash', 'tried', 'trip', 'trouble', 'true',\n",
       "       'truly', 'truth', 'try', 'trying', 'turn', 'turned', 'twist',\n",
       "       'two', 'type', 'typical', 'understand', 'unfortunately', 'unique',\n",
       "       'unless', 'unlike', 'upon', 'us', 'use', 'used', 'using', 'usual',\n",
       "       'usually', 'value', 'vampire', 'van', 'various', 'version',\n",
       "       'victim', 'video', 'view', 'viewer', 'viewing', 'villain',\n",
       "       'violence', 'violent', 'visual', 'voice', 'wait', 'waiting',\n",
       "       'walk', 'want', 'wanted', 'war', 'waste', 'wasted', 'watch',\n",
       "       'watched', 'watching', 'water', 'way', 'weak', 'week', 'weird',\n",
       "       'well', 'went', 'western', 'whatever', 'whether', 'white', 'whole',\n",
       "       'whose', 'wife', 'william', 'win', 'wish', 'within', 'without',\n",
       "       'woman', 'wonder', 'wonderful', 'wood', 'word', 'work', 'worked',\n",
       "       'working', 'world', 'worse', 'worst', 'worth', 'would', 'write',\n",
       "       'writer', 'writing', 'written', 'wrong', 'yeah', 'year', 'yes',\n",
       "       'yet', 'york', 'young', 'younger', 'zombie'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.189091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  absolutely    accent  across       act  acted    acting  \\\n",
       "0          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.182468   \n",
       "1          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "2          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "3          0.0   0.0    0.000000  0.137926     0.0  0.000000    0.0  0.000000   \n",
       "4          0.0   0.0    0.321531  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "...        ...   ...         ...       ...     ...       ...    ...       ...   \n",
       "29995      0.0   0.0    0.000000  0.000000     0.0  0.135608    0.0  0.088057   \n",
       "29996      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "29997      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "29998      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.050570   \n",
       "29999      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.097604   \n",
       "\n",
       "       action  actor  ...  written     wrong  yeah      year  yes  yet  york  \\\n",
       "0         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "1         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "2         0.0    0.0  ...  0.00000  0.000000   0.0  0.055835  0.0  0.0   0.0   \n",
       "3         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "4         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "...       ...    ...  ...      ...       ...   ...       ...  ...  ...   ...   \n",
       "29995     0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29996     0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29997     0.0    0.0  ...  0.00000  0.000000   0.0  0.074143  0.0  0.0   0.0   \n",
       "29998     0.0    0.0  ...  0.07508  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29999     0.0    0.0  ...  0.00000  0.189091   0.0  0.102630  0.0  0.0   0.0   \n",
       "\n",
       "       young  younger  zombie  \n",
       "0        0.0      0.0     0.0  \n",
       "1        0.0      0.0     0.0  \n",
       "2        0.0      0.0     0.0  \n",
       "3        0.0      0.0     0.0  \n",
       "4        0.0      0.0     0.0  \n",
       "...      ...      ...     ...  \n",
       "29995    0.0      0.0     0.0  \n",
       "29996    0.0      0.0     0.0  \n",
       "29997    0.0      0.0     0.0  \n",
       "29998    0.0      0.0     0.0  \n",
       "29999    0.0      0.0     0.0  \n",
       "\n",
       "[30000 rows x 1000 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_train.A,columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.93"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mnb = MultinomialNB()\n",
    "tfidf_mnb.fit(tfidf_train.A,y_train)\n",
    "predict_tfidf = tfidf_mnb.predict(tfidf_test.A)\n",
    "accuracy_tfidf = accuracy_score(y_test,predict_tfidf)*100\n",
    "accuracy_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central, theme, movie, seems, confusion, rela...\n",
       "16298    [excellent, example, cowboy, noir, called, une...\n",
       "28505    [ending, made, heart, jump, throat, proceeded,...\n",
       "6689     [chosen, one, appreciate, quality, story, char...\n",
       "26893    [really, funny, film, especially, second, thir...\n",
       "                               ...                        \n",
       "29415    [film, came, gift, offering, blue, unlike, rev...\n",
       "11359    [first, started, watching, movie, looking, kin...\n",
       "575      [big, mark, music, neil, young, glowing, prais...\n",
       "17398    [watching, lady, ermine, wondering, betty, gra...\n",
       "4189     [crappy, miserably, acted, movie, based, subli...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ngrams \n",
    "from nltk.util import ngrams\n",
    "def splitting_dataframe(data):\n",
    "    tokens = data.split()\n",
    "    return tokens \n",
    "\n",
    "data = clean_text_test.apply(splitting_dataframe)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_list(data , ngram_range):\n",
    "    ngram = ngrams(data , ngram_range) #Zip file \n",
    "    ngram_list1 = []\n",
    "    for ngram1 in ngram: # opening zip file \n",
    "        ngram_list1.append(\" \".join(ngram1))\n",
    "    return ngram_list1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central, theme, movie, seems, confusion, rela...\n",
       "16298    [excellent, example, cowboy, noir, called, une...\n",
       "28505    [ending, made, heart, jump, throat, proceeded,...\n",
       "6689     [chosen, one, appreciate, quality, story, char...\n",
       "26893    [really, funny, film, especially, second, thir...\n",
       "                               ...                        \n",
       "29415    [film, came, gift, offering, blue, unlike, rev...\n",
       "11359    [first, started, watching, movie, looking, kin...\n",
       "575      [big, mark, music, neil, young, glowing, prais...\n",
       "17398    [watching, lady, ermine, wondering, betty, gra...\n",
       "4189     [crappy, miserably, acted, movie, based, subli...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams = data.apply(lambda x : ngram_list(x,1))\n",
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central theme, theme movie, movie seems, seem...\n",
       "16298    [excellent example, example cowboy, cowboy noi...\n",
       "28505    [ending made, made heart, heart jump, jump thr...\n",
       "6689     [chosen one, one appreciate, appreciate qualit...\n",
       "26893    [really funny, funny film, film especially, es...\n",
       "                               ...                        \n",
       "29415    [film came, came gift, gift offering, offering...\n",
       "11359    [first started, started watching, watching mov...\n",
       "575      [big mark, mark music, music neil, neil young,...\n",
       "17398    [watching lady, lady ermine, ermine wondering,...\n",
       "4189     [crappy miserably, miserably acted, acted movi...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = data.apply(lambda x : ngram_list(x,2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central theme movie, theme movie seems, movie...\n",
       "16298    [excellent example cowboy, example cowboy noir...\n",
       "28505    [ending made heart, made heart jump, heart jum...\n",
       "6689     [chosen one appreciate, one appreciate quality...\n",
       "26893    [really funny film, funny film especially, fil...\n",
       "                               ...                        \n",
       "29415    [film came gift, came gift offering, gift offe...\n",
       "11359    [first started watching, started watching movi...\n",
       "575      [big mark music, mark music neil, music neil y...\n",
       "17398    [watching lady ermine, lady ermine wondering, ...\n",
       "4189     [crappy miserably acted, miserably acted movie...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = data.apply(lambda x : ngram_list(x,3))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central theme movie seems confusion relations...\n",
       "16298    [excellent example cowboy noir called unemploy...\n",
       "28505    [ending made heart jump throat proceeded leave...\n",
       "6689     [chosen one appreciate quality story character...\n",
       "26893    [really funny film especially second third fou...\n",
       "                               ...                        \n",
       "29415    [film came gift offering blue unlike reviewer,...\n",
       "11359    [first started watching movie looking kind sub...\n",
       "575      [big mark music neil young glowing praise, mar...\n",
       "17398    [watching lady ermine wondering betty grable p...\n",
       "4189     [crappy miserably acted movie based sublimated...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_grams = data.apply(lambda x : ngram_list(x,7))\n",
    "seven_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "> In previous Unit Vectorizer and TF-IDF we were not understanding the meaning of the words.\n",
    "> Suppose we have example of KING , MAN , HORSE what comes to our mind.\n",
    "* For KING : Power,rich,male,Palace\n",
    "* For MAN : Low power , low rich , male,Home \n",
    "* For HORSE : Power,Male,tail.\n",
    "* We as a human being understand the menaning of words by properties of that word.\n",
    "* Same concept will be used for training the model using the properties of that word.\n",
    "* In Count Vectorizer and TF-IDF our output was in sparse matrix.\n",
    "> Sparse matrix : maximum values are of value 0.\n",
    "* Word2Vec uses dense vector representation.\n",
    "* In Dense Vectors there will be no value of Zero.\n",
    "* If it is alloted zero then it will take values close to zero.\n",
    "* Highest weight alloted is 1.98.\n",
    "* In Word2Vec we will see how weights are randomly assigned to vectors/data.\n",
    "> Word2Vec\n",
    "* It is a simple neural network.\n",
    "* It has a single hidden layer.\n",
    "* vector performance is great.\n",
    "* We dont know what properties it is using for calculating vector performance.\n",
    "> Assumptions of Word2Vec\n",
    "* Words with similar context will have similar vector representations.\n",
    "> Vector : which has magnitude and direction.\n",
    "> Similar context : For KING similar context >> Prince , Princess , Queen , Ruler , Sultan\n",
    "* In Above case representation of KING and Prince has similar context so representation of vectors of KING and Prince will be similar.\n",
    "> Cosine similarity : A.B / |A|*|B|\n",
    "* To check the similarities of vector representations we can use Cosine similarity.\n",
    "* Range -1 to +1.\n",
    "* If the value is close to -1 , it means vectors are not similar at all.\n",
    "* If the value is close to +1 , it means vectors are are similar.\n",
    "* Cos0 = 1 ,cos180 = -1 as the angle is increasing value is decreasing.\n",
    "* Cosine similarity shows similarity between cosine \n",
    "> If the representaition of two vectors are similar it means that the angle of vectors is low and so the values of similarity is high.\n",
    "> If two word vectors arec closer then there cosine similarity will increase and there will be similarity between vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import  stopwords\n",
    "from string import punctuation\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autocorrect import Speller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preporcessing \n",
    "#1. remove space and newline \n",
    "def remove_spaces(data):\n",
    "    clean_text = data.replace(\"\\\\n\", \" \").replace(\"\\t\",\" \").replace(\"\\\\\",\" \")\n",
    "    return clean_text\n",
    "\n",
    "#2. Contraction Mapping \n",
    "def expand_text(data):\n",
    "    expanded_text = contractions.fix(data)\n",
    "    return expanded_text\n",
    "\n",
    "#3. Handling accented characters\n",
    "def handling_accented(data):\n",
    "    fixed_text = unidecode(data)\n",
    "    return fixed_text\n",
    "\n",
    "#4. Cleaning \n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.remove(\"no\")\n",
    "stopword_list.remove(\"nor\")\n",
    "stopword_list.remove(\"not\")\n",
    "\n",
    "def clean_data(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    clean_text = [word.lower() for word in tokens if (word not in punctuation) and (word.lower() not in stopword_list) and (len(word)>2) and (word.isalpha())]\n",
    "    return clean_text\n",
    "\n",
    "#5. Auto correct \n",
    "def autocorrection(data):\n",
    "    spell = Speller(lang=\"en\")\n",
    "    corrected_text = spell(data)\n",
    "    return corrected_text\n",
    "\n",
    "# Lemmatization \n",
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_data = []\n",
    "    for word in data:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        final_data.append(lemmatized_word)\n",
    "    return \" \".join(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer and TF-IDF >> String format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Leakage \n",
    "x_train ,x_test , y_train , y_test = train_test_split(data.text,data.label,test_size=0.25,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_train = x_train.apply(remove_spaces)\n",
    "clean_text_test = x_test.apply(remove_spaces)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(expand_text)\n",
    "clean_text_test = clean_text_test.apply(expand_text)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(handling_accented)\n",
    "clean_text_test = clean_text_test.apply(handling_accented)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(clean_data)\n",
    "clean_text_test = clean_text_test.apply(clean_data)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(lemmatization)\n",
    "clean_text_test = clean_text_test.apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26898    fifth grade language art teacher read book stu...\n",
       "27635    low budget brit pop melodrama focus girl want ...\n",
       "3036     well watched movie little year ago pulled dust...\n",
       "5604     would almost give however confusing part well ...\n",
       "36111    full length feature film world bridge found fi...\n",
       "                               ...                        \n",
       "6265     movie one worst movie ever seen life waste tim...\n",
       "11284    movie inspiring anyone tough jam whether finan...\n",
       "38158    east side story documentary musical comedy sta...\n",
       "860      one boot one point doctor assistant refers wor...\n",
       "15795    movie horrible lighting terrible camera moveme...\n",
       "Name: text, Length: 30000, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer \n",
    "count = CountVectorizer(max_df=0.95 , max_features=1000 , ngram_range=(2,2))\n",
    "count_val_train = count.fit_transform(clean_text_train)\n",
    "count_val_test = count.fit_transform(clean_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 248311 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_val_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolutely nothing', 'academy award', 'act like', 'acting bad',\n",
       "       'acting good', 'acting not', 'acting poor', 'action film',\n",
       "       'action movie', 'action scene', 'action sequence', 'actor actress',\n",
       "       'actor film', 'actor like', 'actor not', 'actually not',\n",
       "       'actually quite', 'almost every', 'along line', 'along way',\n",
       "       'also good', 'also not', 'also one', 'although not',\n",
       "       'another film', 'another movie', 'another one', 'anyone could',\n",
       "       'anyone else', 'anyone not', 'anyone would', 'anything else',\n",
       "       'aspect film', 'audience not', 'avoid cost', 'awful movie',\n",
       "       'bad acting', 'bad bad', 'bad enough', 'bad film', 'bad good',\n",
       "       'bad guy', 'bad movie', 'bad not', 'bad one', 'bad thing',\n",
       "       'based true', 'beginning end', 'beginning film', 'beginning movie',\n",
       "       'believe movie', 'believe not', 'best actor', 'best film',\n",
       "       'best friend', 'best movie', 'best part', 'best performance',\n",
       "       'best picture', 'best thing', 'best work', 'better film',\n",
       "       'better movie', 'better not', 'big budget', 'big fan',\n",
       "       'big screen', 'black white', 'blah blah', 'book not',\n",
       "       'bottom line', 'box office', 'brad pitt', 'bring back',\n",
       "       'came across', 'camera work', 'car chase', 'cast crew',\n",
       "       'cast member', 'cast not', 'certainly not', 'chance see',\n",
       "       'character actor', 'character could', 'character development',\n",
       "       'character film', 'character like', 'character movie',\n",
       "       'character not', 'character one', 'character played',\n",
       "       'character really', 'character well', 'civil war', 'come across',\n",
       "       'come back', 'come close', 'come mind', 'comedy not', 'comic book',\n",
       "       'comic relief', 'completely different', 'could better',\n",
       "       'could done', 'could easily', 'could get', 'could made',\n",
       "       'could make', 'could much', 'could not', 'could see', 'course not',\n",
       "       'david lynch', 'death scene', 'definitely not', 'definitely one',\n",
       "       'definitely worth', 'despite fact', 'director not', 'done better',\n",
       "       'done well', 'edge seat', 'effect not', 'either way', 'end film',\n",
       "       'end movie', 'end not', 'ending not', 'enjoy film', 'enjoy movie',\n",
       "       'enjoyed movie', 'enough make', 'entire film', 'entire movie',\n",
       "       'even better', 'even get', 'even good', 'even know', 'even not',\n",
       "       'even think', 'even though', 'even worse', 'ever made', 'ever see',\n",
       "       'ever seen', 'ever since', 'every day', 'every episode',\n",
       "       'every scene', 'every single', 'every time', 'everyone else',\n",
       "       'everything else', 'excellent film', 'eye candy',\n",
       "       'facial expression', 'fact not', 'fall flat', 'fall love',\n",
       "       'falling love', 'far away', 'far superior', 'favorite movie',\n",
       "       'feature film', 'feel like', 'feel sorry', 'felt like',\n",
       "       'fight scene', 'film actually', 'film also', 'film bad',\n",
       "       'film based', 'film begin', 'film best', 'film certainly',\n",
       "       'film character', 'film come', 'film could', 'film definitely',\n",
       "       'film director', 'film end', 'film especially', 'film even',\n",
       "       'film ever', 'film feel', 'film festival', 'film film',\n",
       "       'film first', 'film get', 'film good', 'film great',\n",
       "       'film however', 'film like', 'film little', 'film look',\n",
       "       'film lot', 'film made', 'film make', 'film maker', 'film making',\n",
       "       'film many', 'film may', 'film might', 'film movie', 'film much',\n",
       "       'film must', 'film never', 'film noir', 'film not', 'film nothing',\n",
       "       'film one', 'film people', 'film plot', 'film quite',\n",
       "       'film really', 'film see', 'film seems', 'film seen', 'film set',\n",
       "       'film show', 'film start', 'film still', 'film story', 'film take',\n",
       "       'film think', 'film though', 'film time', 'film two',\n",
       "       'film version', 'film watch', 'film way', 'film well', 'film work',\n",
       "       'film worth', 'film would', 'film year', 'final scene',\n",
       "       'find movie', 'find way', 'first episode', 'first film',\n",
       "       'first half', 'first minute', 'first movie', 'first not',\n",
       "       'first one', 'first place', 'first saw', 'first scene',\n",
       "       'first thing', 'first time', 'first two', 'five minute',\n",
       "       'five year', 'found film', 'found movie', 'fun movie', 'fun watch',\n",
       "       'funny moment', 'funny movie', 'funny not', 'get away', 'get back',\n",
       "       'get better', 'get chance', 'get good', 'get killed', 'get know',\n",
       "       'get movie', 'get not', 'get see', 'get wrong', 'girl not',\n",
       "       'give film', 'give good', 'give movie', 'give one', 'going get',\n",
       "       'going happen', 'good acting', 'good actor', 'good bad',\n",
       "       'good enough', 'good film', 'good guy', 'good idea', 'good job',\n",
       "       'good laugh', 'good look', 'good looking', 'good movie',\n",
       "       'good not', 'good old', 'good one', 'good performance',\n",
       "       'good point', 'good reason', 'good story', 'good thing',\n",
       "       'good time', 'good way', 'great acting', 'great actor',\n",
       "       'great deal', 'great film', 'great job', 'great movie',\n",
       "       'great performance', 'great see', 'great story', 'guess not',\n",
       "       'guy not', 'half hour', 'half movie', 'happy ending',\n",
       "       'hard believe', 'high school', 'highly recommend',\n",
       "       'highly recommended', 'hong kong', 'horror fan', 'horror film',\n",
       "       'horror flick', 'horror movie', 'hour half', 'however not',\n",
       "       'huge fan', 'human being', 'independent film',\n",
       "       'interesting character', 'james bond', 'joke not', 'kid movie',\n",
       "       'kid not', 'kind film', 'kind like', 'kind movie', 'know going',\n",
       "       'know movie', 'know not', 'last minute', 'last night', 'last one',\n",
       "       'last scene', 'last year', 'late night', 'laugh loud',\n",
       "       'lead actor', 'lead character', 'leading lady', 'leading man',\n",
       "       'least not', 'least one', 'let alone', 'let know', 'let say',\n",
       "       'let see', 'life not', 'like film', 'like good', 'like many',\n",
       "       'like movie', 'like not', 'like one', 'like people', 'like real',\n",
       "       'like see', 'like something', 'like watching', 'like would',\n",
       "       'liked movie', 'line not', 'little bit', 'little boy',\n",
       "       'little film', 'little girl', 'little kid', 'long time',\n",
       "       'look good', 'look like', 'looked like', 'looking forward',\n",
       "       'looking like', 'los angeles', 'lot better', 'lot fun',\n",
       "       'lot movie', 'lot people', 'love film', 'love interest',\n",
       "       'love movie', 'love not', 'love story', 'loved movie',\n",
       "       'low budget', 'made film', 'made laugh', 'made movie', 'made not',\n",
       "       'made sense', 'main character', 'main reason', 'make feel',\n",
       "       'make film', 'make good', 'make great', 'make laugh', 'make look',\n",
       "       'make movie', 'make much', 'make one', 'make sense', 'make sure',\n",
       "       'make think', 'make want', 'making film', 'making movie',\n",
       "       'man not', 'many film', 'many many', 'many movie', 'many people',\n",
       "       'many scene', 'many thing', 'many time', 'many way', 'many year',\n",
       "       'martial art', 'may not', 'may well', 'maybe not', 'men woman',\n",
       "       'michael caine', 'might not', 'might well', 'million dollar',\n",
       "       'minute film', 'minute long', 'minute movie', 'minute not',\n",
       "       'modern day', 'moment film', 'motion picture', 'movie acting',\n",
       "       'movie actually', 'movie almost', 'movie also', 'movie anyone',\n",
       "       'movie bad', 'movie based', 'movie best', 'movie come',\n",
       "       'movie could', 'movie end', 'movie even', 'movie ever',\n",
       "       'movie fan', 'movie feel', 'movie film', 'movie first',\n",
       "       'movie get', 'movie give', 'movie going', 'movie good',\n",
       "       'movie got', 'movie great', 'movie however', 'movie kid',\n",
       "       'movie kind', 'movie know', 'movie least', 'movie like',\n",
       "       'movie little', 'movie look', 'movie lot', 'movie made',\n",
       "       'movie make', 'movie many', 'movie may', 'movie might',\n",
       "       'movie movie', 'movie much', 'movie must', 'movie never',\n",
       "       'movie not', 'movie nothing', 'movie one', 'movie people',\n",
       "       'movie plot', 'movie pretty', 'movie probably', 'movie quite',\n",
       "       'movie real', 'movie really', 'movie saw', 'movie say',\n",
       "       'movie see', 'movie seems', 'movie seen', 'movie show',\n",
       "       'movie star', 'movie start', 'movie still', 'movie story',\n",
       "       'movie take', 'movie terrible', 'movie theater', 'movie think',\n",
       "       'movie thought', 'movie time', 'movie try', 'movie want',\n",
       "       'movie watch', 'movie way', 'movie well', 'movie without',\n",
       "       'movie worth', 'movie would', 'movie year', 'much better',\n",
       "       'much film', 'much fun', 'much le', 'much like', 'much movie',\n",
       "       'much not', 'much time', 'murder mystery', 'music not',\n",
       "       'music video', 'musical number', 'musical score', 'must admit',\n",
       "       'must say', 'must see', 'near end', 'need see', 'needle say',\n",
       "       'never get', 'never got', 'never heard', 'never made',\n",
       "       'never really', 'never see', 'never seen', 'new york', 'next time',\n",
       "       'nice see', 'not able', 'not act', 'not actually', 'not always',\n",
       "       'not anything', 'not bad', 'not believe', 'not best', 'not better',\n",
       "       'not big', 'not bother', 'not buy', 'not care', 'not come',\n",
       "       'not disappointed', 'not done', 'not easy', 'not enjoy',\n",
       "       'not enough', 'not entirely', 'not even', 'not ever',\n",
       "       'not everyone', 'not exactly', 'not exist', 'not expect',\n",
       "       'not expecting', 'not fan', 'not far', 'not feel', 'not film',\n",
       "       'not find', 'not fit', 'not forget', 'not funny', 'not get',\n",
       "       'not give', 'not going', 'not good', 'not great', 'not help',\n",
       "       'not imagine', 'not know', 'not laugh', 'not least', 'not let',\n",
       "       'not like', 'not long', 'not look', 'not love', 'not made',\n",
       "       'not make', 'not many', 'not matter', 'not mean', 'not mention',\n",
       "       'not mind', 'not miss', 'not movie', 'not much', 'not nearly',\n",
       "       'not necessarily', 'not need', 'not not', 'not one',\n",
       "       'not particularly', 'not pay', 'not perfect', 'not play',\n",
       "       'not put', 'not quite', 'not read', 'not real', 'not really',\n",
       "       'not recommend', 'not remember', 'not save', 'not say',\n",
       "       'not saying', 'not scary', 'not see', 'not seem', 'not seen',\n",
       "       'not show', 'not single', 'not stand', 'not stop', 'not sure',\n",
       "       'not surprised', 'not take', 'not taken', 'not tell', 'not think',\n",
       "       'not try', 'not understand', 'not wait', 'not want', 'not waste',\n",
       "       'not watch', 'not well', 'not without', 'not work', 'not worst',\n",
       "       'not worth', 'not yet', 'nothing else', 'nothing like',\n",
       "       'nothing new', 'nothing special', 'obviously not', 'old man',\n",
       "       'one another', 'one best', 'one better', 'one big',\n",
       "       'one character', 'one could', 'one day', 'one episode',\n",
       "       'one favorite', 'one film', 'one first', 'one get', 'one good',\n",
       "       'one great', 'one greatest', 'one last', 'one like', 'one man',\n",
       "       'one many', 'one might', 'one movie', 'one night', 'one not',\n",
       "       'one one', 'one person', 'one point', 'one really', 'one reason',\n",
       "       'one scene', 'one see', 'one thing', 'one time', 'one two',\n",
       "       'one way', 'one wonder', 'one worst', 'one would',\n",
       "       'opening credit', 'opening scene', 'opening sequence',\n",
       "       'original film', 'original movie', 'outer space', 'painful watch',\n",
       "       'part film', 'part movie', 'part not', 'pay attention',\n",
       "       'people get', 'people like', 'people not', 'people say',\n",
       "       'people think', 'people would', 'performance not', 'piece crap',\n",
       "       'pleasantly surprised', 'please not', 'plot hole', 'plot line',\n",
       "       'plot movie', 'plot not', 'plot point', 'plot twist', 'point film',\n",
       "       'point movie', 'point not', 'point view', 'police officer',\n",
       "       'pretty bad', 'pretty good', 'pretty much', 'probably best',\n",
       "       'probably not', 'probably one', 'probably would', 'problem film',\n",
       "       'problem movie', 'production value', 'put together', 'quite bit',\n",
       "       'quite good', 'quite well', 'read book', 'real life',\n",
       "       'real people', 'really bad', 'really care', 'really enjoyed',\n",
       "       'really funny', 'really get', 'really good', 'really great',\n",
       "       'really know', 'really like', 'really liked', 'really make',\n",
       "       'really not', 'really really', 'really think', 'really want',\n",
       "       'reason not', 'recommend anyone', 'recommend film',\n",
       "       'recommend movie', 'redeeming quality', 'rest cast', 'rest film',\n",
       "       'rest movie', 'revolves around', 'robin williams', 'role not',\n",
       "       'romantic comedy', 'run away', 'running around', 'running time',\n",
       "       'said not', 'saturday night', 'saw film', 'saw movie', 'say film',\n",
       "       'say least', 'say movie', 'say not', 'say one', 'scary movie',\n",
       "       'scene film', 'scene movie', 'scene not', 'scene one',\n",
       "       'science fiction', 'screen time', 'script not', 'second half',\n",
       "       'second time', 'see film', 'see movie', 'see not', 'see one',\n",
       "       'see something', 'seeing film', 'seeing movie', 'seem like',\n",
       "       'seemed like', 'seems like', 'seen film', 'seen many',\n",
       "       'seen movie', 'seen not', 'seen one', 'sense humor',\n",
       "       'serial killer', 'series not', 'several time', 'several year',\n",
       "       'sex scene', 'short film', 'show like', 'show not', 'silent film',\n",
       "       'simply not', 'since not', 'slasher film', 'small town',\n",
       "       'soap opera', 'someone else', 'something else', 'something like',\n",
       "       'something not', 'something would', 'sound effect', 'sound like',\n",
       "       'special effect', 'star trek', 'star war', 'start finish',\n",
       "       'stay away', 'still not', 'story character', 'story film',\n",
       "       'story line', 'story movie', 'story not', 'story one',\n",
       "       'story told', 'subject matter', 'supporting cast',\n",
       "       'supporting character', 'supporting role', 'sure not', 'take away',\n",
       "       'take place', 'take seriously', 'tell story', 'ten minute',\n",
       "       'ten year', 'terrible movie', 'thank god', 'thing film',\n",
       "       'thing get', 'thing happen', 'thing like', 'thing movie',\n",
       "       'thing not', 'thing really', 'thing would', 'think could',\n",
       "       'think film', 'think movie', 'think not', 'think one',\n",
       "       'think would', 'though film', 'though not', 'thought movie',\n",
       "       'thought would', 'three time', 'throughout film',\n",
       "       'throughout movie', 'time around', 'time film', 'time money',\n",
       "       'time movie', 'time not', 'time one', 'time saw', 'time see',\n",
       "       'time still', 'time time', 'time watch', 'time watching',\n",
       "       'time would', 'tom hank', 'took place', 'top notch', 'towards end',\n",
       "       'true story', 'try get', 'try hard', 'try make', 'trying find',\n",
       "       'trying get', 'trying make', 'twenty minute', 'twist turn',\n",
       "       'two character', 'two film', 'two hour', 'two lead', 'two main',\n",
       "       'two people', 'two year', 'type movie', 'unfortunately not',\n",
       "       'united state', 'van damme', 'video game', 'video store',\n",
       "       'viewer not', 'visual effect', 'want get', 'want know', 'want see',\n",
       "       'want watch', 'wanted see', 'war movie', 'waste money',\n",
       "       'waste time', 'watch film', 'watch movie', 'watch not',\n",
       "       'watch one', 'watch show', 'watched film', 'watched movie',\n",
       "       'watching film', 'watching movie', 'way film', 'way movie',\n",
       "       'way not', 'well acted', 'well done', 'well known', 'well made',\n",
       "       'well not', 'well one', 'well worth', 'well written', 'went see',\n",
       "       'whether not', 'whole film', 'whole lot', 'whole movie',\n",
       "       'whole thing', 'wish could', 'wish would', 'without doubt',\n",
       "       'woman not', 'work art', 'work not', 'work well', 'world not',\n",
       "       'world war', 'worst film', 'worst movie', 'worth seeing',\n",
       "       'worth time', 'worth watching', 'would better', 'would ever',\n",
       "       'would expect', 'would get', 'would give', 'would like',\n",
       "       'would love', 'would made', 'would make', 'would much',\n",
       "       'would never', 'would not', 'would probably', 'would rather',\n",
       "       'would recommend', 'would say', 'would take', 'would think',\n",
       "       'would want', 'writer director', 'written directed', 'year ago',\n",
       "       'year earlier', 'year later', 'year not', 'year old',\n",
       "       'yet another', 'yet not', 'york city', 'young boy', 'young girl',\n",
       "       'young man', 'young woman'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely nothing</th>\n",
       "      <th>academy award</th>\n",
       "      <th>act like</th>\n",
       "      <th>acting bad</th>\n",
       "      <th>acting good</th>\n",
       "      <th>acting not</th>\n",
       "      <th>acting poor</th>\n",
       "      <th>action film</th>\n",
       "      <th>action movie</th>\n",
       "      <th>action scene</th>\n",
       "      <th>...</th>\n",
       "      <th>year later</th>\n",
       "      <th>year not</th>\n",
       "      <th>year old</th>\n",
       "      <th>yet another</th>\n",
       "      <th>yet not</th>\n",
       "      <th>york city</th>\n",
       "      <th>young boy</th>\n",
       "      <th>young girl</th>\n",
       "      <th>young man</th>\n",
       "      <th>young woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       absolutely nothing  academy award  act like  acting bad  acting good  \\\n",
       "0                       0              0         0           0            0   \n",
       "1                       0              0         0           0            0   \n",
       "2                       0              0         0           0            0   \n",
       "3                       0              0         0           0            0   \n",
       "4                       0              0         0           0            0   \n",
       "...                   ...            ...       ...         ...          ...   \n",
       "29995                   0              0         0           0            0   \n",
       "29996                   0              0         0           0            0   \n",
       "29997                   0              0         0           0            0   \n",
       "29998                   0              0         0           0            0   \n",
       "29999                   0              0         0           0            0   \n",
       "\n",
       "       acting not  acting poor  action film  action movie  action scene  ...  \\\n",
       "0               0            0            0             0             0  ...   \n",
       "1               0            0            0             0             0  ...   \n",
       "2               0            0            0             0             0  ...   \n",
       "3               0            0            0             0             0  ...   \n",
       "4               0            0            0             0             0  ...   \n",
       "...           ...          ...          ...           ...           ...  ...   \n",
       "29995           0            0            0             0             0  ...   \n",
       "29996           0            0            0             0             0  ...   \n",
       "29997           0            0            0             0             0  ...   \n",
       "29998           0            0            0             0             0  ...   \n",
       "29999           0            0            0             0             0  ...   \n",
       "\n",
       "       year later  year not  year old  yet another  yet not  york city  \\\n",
       "0               0         0         0            0        0          0   \n",
       "1               0         0         0            0        0          0   \n",
       "2               0         0         0            0        0          0   \n",
       "3               0         0         0            0        0          0   \n",
       "4               0         0         0            0        0          0   \n",
       "...           ...       ...       ...          ...      ...        ...   \n",
       "29995           0         0         0            0        0          0   \n",
       "29996           0         0         0            0        0          0   \n",
       "29997           0         0         0            0        0          0   \n",
       "29998           0         0         0            0        0          0   \n",
       "29999           0         0         1            0        0          0   \n",
       "\n",
       "       young boy  young girl  young man  young woman  \n",
       "0              0           0          0            0  \n",
       "1              0           0          0            0  \n",
       "2              0           0          0            0  \n",
       "3              0           0          0            0  \n",
       "4              0           0          0            0  \n",
       "...          ...         ...        ...          ...  \n",
       "29995          0           0          0            0  \n",
       "29996          0           0          0            0  \n",
       "29997          0           0          0            0  \n",
       "29998          0           0          0            0  \n",
       "29999          0           0          0            0  \n",
       "\n",
       "[30000 rows x 1000 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(count_val_train.A,columns=count.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.120000000000005"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mnb = MultinomialNB()\n",
    "count_mnb.fit(count_val_train.A,y_train)\n",
    "predict_count = count_mnb.predict(count_val_test.A)\n",
    "accuracy_count = accuracy_score(y_test , predict_count)*100\n",
    "accuracy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "tfidf = TfidfVectorizer(max_df=0.95,max_features=1000)\n",
    "tfidf_train = tfidf.fit_transform(clean_text_train)\n",
    "tfidf_test = tfidf.fit_transform(clean_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1617756 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'absolutely', 'accent', 'across', 'act',\n",
       "       'acted', 'acting', 'action', 'actor', 'actress', 'actual',\n",
       "       'actually', 'adaptation', 'add', 'admit', 'adult', 'adventure',\n",
       "       'affair', 'age', 'agent', 'ago', 'agree', 'air', 'alien', 'almost',\n",
       "       'alone', 'along', 'already', 'also', 'although', 'always',\n",
       "       'amazing', 'america', 'american', 'among', 'amount', 'amusing',\n",
       "       'animal', 'animated', 'animation', 'annoying', 'another', 'answer',\n",
       "       'anyone', 'anything', 'anyway', 'apart', 'apparently', 'appear',\n",
       "       'appearance', 'appears', 'appreciate', 'army', 'around', 'art',\n",
       "       'aside', 'ask', 'aspect', 'atmosphere', 'attempt', 'attention',\n",
       "       'audience', 'average', 'avoid', 'award', 'away', 'awesome',\n",
       "       'awful', 'baby', 'back', 'background', 'bad', 'badly', 'band',\n",
       "       'based', 'basic', 'basically', 'battle', 'beat', 'beautiful',\n",
       "       'beauty', 'became', 'become', 'becomes', 'bed', 'begin',\n",
       "       'beginning', 'behind', 'believable', 'believe', 'ben', 'best',\n",
       "       'better', 'beyond', 'big', 'biggest', 'bill', 'bit', 'bizarre',\n",
       "       'black', 'blood', 'blue', 'body', 'book', 'bored', 'boring',\n",
       "       'bother', 'bought', 'box', 'boy', 'brain', 'break', 'brilliant',\n",
       "       'bring', 'brings', 'british', 'brother', 'brought', 'budget',\n",
       "       'building', 'bunch', 'business', 'buy', 'call', 'called', 'came',\n",
       "       'camera', 'camp', 'car', 'care', 'career', 'cartoon', 'case',\n",
       "       'cast', 'casting', 'cat', 'catch', 'caught', 'century', 'certain',\n",
       "       'certainly', 'chance', 'change', 'channel', 'character', 'chase',\n",
       "       'cheap', 'check', 'cheesy', 'chemistry', 'child', 'choice',\n",
       "       'cinema', 'cinematography', 'city', 'class', 'classic', 'clear',\n",
       "       'clearly', 'clever', 'cliche', 'close', 'cold', 'color', 'come',\n",
       "       'comedy', 'comic', 'coming', 'comment', 'common', 'company',\n",
       "       'compared', 'complete', 'completely', 'computer', 'concept',\n",
       "       'consider', 'considering', 'control', 'convincing', 'cool', 'cop',\n",
       "       'copy', 'costume', 'could', 'country', 'couple', 'course', 'cover',\n",
       "       'crap', 'crazy', 'create', 'created', 'creature', 'credit',\n",
       "       'creepy', 'crew', 'crime', 'critic', 'cry', 'culture', 'cut',\n",
       "       'cute', 'dad', 'dance', 'dancing', 'dark', 'daughter', 'david',\n",
       "       'day', 'dead', 'deal', 'death', 'decent', 'decide', 'decided',\n",
       "       'decides', 'deep', 'definitely', 'depth', 'deserves', 'despite',\n",
       "       'detail', 'development', 'dialog', 'dialogue', 'die', 'died',\n",
       "       'different', 'difficult', 'directed', 'directing', 'direction',\n",
       "       'director', 'disappointed', 'disney', 'doctor', 'documentary',\n",
       "       'dog', 'done', 'door', 'doubt', 'drama', 'dramatic', 'dream',\n",
       "       'drive', 'drug', 'due', 'dull', 'dumb', 'dvd', 'earlier', 'early',\n",
       "       'earth', 'easily', 'easy', 'editing', 'effect', 'effective',\n",
       "       'effort', 'either', 'element', 'else', 'emotion', 'emotional',\n",
       "       'end', 'ended', 'ending', 'english', 'enjoy', 'enjoyable',\n",
       "       'enjoyed', 'enough', 'entertaining', 'entertainment', 'entire',\n",
       "       'entirely', 'episode', 'era', 'escape', 'especially', 'etc',\n",
       "       'even', 'event', 'eventually', 'ever', 'every', 'everyone',\n",
       "       'everything', 'evil', 'exactly', 'example', 'excellent', 'except',\n",
       "       'expect', 'expected', 'expecting', 'experience', 'explain',\n",
       "       'extra', 'extremely', 'eye', 'face', 'fact', 'fails', 'fair',\n",
       "       'fairly', 'fake', 'fall', 'familiar', 'family', 'famous', 'fan',\n",
       "       'fantastic', 'fantasy', 'far', 'fast', 'father', 'favorite',\n",
       "       'fear', 'feature', 'feel', 'feeling', 'felt', 'female', 'fiction',\n",
       "       'fight', 'fighting', 'figure', 'film', 'filmed', 'filmmaker',\n",
       "       'final', 'finally', 'find', 'fine', 'fire', 'first', 'fit', 'five',\n",
       "       'flat', 'flick', 'focus', 'follow', 'following', 'follows',\n",
       "       'footage', 'force', 'forced', 'forget', 'form', 'former',\n",
       "       'forward', 'found', 'four', 'free', 'french', 'friend', 'front',\n",
       "       'full', 'fun', 'funny', 'future', 'game', 'gang', 'garbage',\n",
       "       'gave', 'gay', 'general', 'genius', 'genre', 'george', 'german',\n",
       "       'get', 'getting', 'ghost', 'girl', 'girlfriend', 'give', 'given',\n",
       "       'giving', 'go', 'god', 'going', 'gone', 'good', 'gore', 'got',\n",
       "       'government', 'great', 'greatest', 'group', 'guess', 'gun', 'guy',\n",
       "       'hair', 'half', 'hand', 'happen', 'happened', 'happens', 'happy',\n",
       "       'hard', 'hardly', 'hate', 'head', 'hear', 'heard', 'heart', 'hell',\n",
       "       'help', 'hero', 'high', 'highly', 'hilarious', 'history', 'hit',\n",
       "       'hold', 'hole', 'hollywood', 'home', 'honestly', 'hope',\n",
       "       'horrible', 'horror', 'hot', 'hour', 'house', 'however', 'huge',\n",
       "       'human', 'humor', 'husband', 'idea', 'image', 'imagine', 'imdb',\n",
       "       'important', 'impossible', 'including', 'incredible', 'incredibly',\n",
       "       'indeed', 'indian', 'inside', 'instead', 'interest', 'interested',\n",
       "       'interesting', 'involved', 'island', 'issue', 'italian', 'jack',\n",
       "       'james', 'jane', 'japanese', 'job', 'joe', 'john', 'joke', 'keep',\n",
       "       'kelly', 'kept', 'kid', 'kill', 'killed', 'killer', 'killing',\n",
       "       'kind', 'king', 'knew', 'know', 'known', 'lack', 'lady', 'lame',\n",
       "       'land', 'language', 'large', 'last', 'late', 'later', 'laugh',\n",
       "       'laughing', 'law', 'le', 'lead', 'leading', 'leaf', 'learn',\n",
       "       'least', 'leave', 'leaving', 'lee', 'left', 'let', 'level', 'lie',\n",
       "       'life', 'light', 'like', 'liked', 'line', 'list', 'literally',\n",
       "       'little', 'live', 'living', 'local', 'location', 'long', 'look',\n",
       "       'looked', 'looking', 'lost', 'lot', 'love', 'loved', 'lover',\n",
       "       'low', 'machine', 'mad', 'made', 'main', 'major', 'make', 'maker',\n",
       "       'making', 'male', 'man', 'manages', 'many', 'mark', 'married',\n",
       "       'master', 'masterpiece', 'match', 'material', 'matter', 'may',\n",
       "       'maybe', 'mean', 'meaning', 'meant', 'meet', 'member', 'memorable',\n",
       "       'memory', 'men', 'mention', 'mentioned', 'mess', 'message',\n",
       "       'michael', 'middle', 'might', 'million', 'mind', 'minute', 'miss',\n",
       "       'missed', 'missing', 'mistake', 'modern', 'moment', 'money',\n",
       "       'monster', 'mood', 'mostly', 'mother', 'move', 'movie', 'moving',\n",
       "       'much', 'murder', 'music', 'musical', 'must', 'mystery', 'naked',\n",
       "       'name', 'named', 'nature', 'near', 'nearly', 'need', 'needed',\n",
       "       'neither', 'never', 'new', 'next', 'nice', 'night', 'none', 'nor',\n",
       "       'not', 'note', 'nothing', 'notice', 'novel', 'nudity', 'number',\n",
       "       'obvious', 'obviously', 'odd', 'offer', 'office', 'often', 'okay',\n",
       "       'old', 'older', 'one', 'open', 'opening', 'opinion', 'order',\n",
       "       'original', 'oscar', 'others', 'otherwise', 'outside', 'overall',\n",
       "       'pace', 'parent', 'park', 'part', 'particular', 'particularly',\n",
       "       'party', 'past', 'paul', 'pay', 'people', 'perfect', 'perfectly',\n",
       "       'performance', 'perhaps', 'period', 'person', 'personal', 'peter',\n",
       "       'pick', 'picture', 'piece', 'place', 'plain', 'plan', 'play',\n",
       "       'played', 'player', 'playing', 'please', 'plenty', 'plot', 'plus',\n",
       "       'point', 'pointless', 'police', 'political', 'poor', 'poorly',\n",
       "       'popular', 'portrayal', 'portrayed', 'positive', 'possible',\n",
       "       'possibly', 'potential', 'power', 'powerful', 'predictable',\n",
       "       'premise', 'present', 'pretty', 'previous', 'prison', 'probably',\n",
       "       'problem', 'produced', 'producer', 'production', 'project',\n",
       "       'public', 'pull', 'pure', 'purpose', 'put', 'quality', 'question',\n",
       "       'quickly', 'quite', 'rate', 'rather', 'rating', 'read', 'reading',\n",
       "       'real', 'realistic', 'reality', 'realize', 'really', 'reason',\n",
       "       'recently', 'recommend', 'recommended', 'red', 'reference',\n",
       "       'relationship', 'release', 'released', 'remains', 'remake',\n",
       "       'remember', 'rent', 'respect', 'rest', 'result', 'return',\n",
       "       'review', 'rich', 'richard', 'ridiculous', 'right', 'road',\n",
       "       'robert', 'rock', 'role', 'romance', 'romantic', 'room', 'run',\n",
       "       'running', 'sad', 'sadly', 'said', 'save', 'saw', 'say', 'saying',\n",
       "       'scary', 'scene', 'school', 'science', 'scientist', 'score',\n",
       "       'scott', 'screen', 'screenplay', 'script', 'season', 'second',\n",
       "       'secret', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen',\n",
       "       'sense', 'sequel', 'sequence', 'series', 'serious', 'seriously',\n",
       "       'set', 'setting', 'several', 'sex', 'sexual', 'shame', 'shoot',\n",
       "       'short', 'shot', 'show', 'showing', 'shown', 'sick', 'side',\n",
       "       'silent', 'silly', 'similar', 'simple', 'simply', 'since',\n",
       "       'single', 'sister', 'sit', 'situation', 'slasher', 'slightly',\n",
       "       'slow', 'small', 'social', 'society', 'soldier', 'somehow',\n",
       "       'someone', 'something', 'sometimes', 'somewhat', 'son', 'song',\n",
       "       'soon', 'sorry', 'sort', 'soul', 'sound', 'soundtrack', 'south',\n",
       "       'space', 'speak', 'special', 'spent', 'spirit', 'spoiler', 'stage',\n",
       "       'stand', 'standard', 'star', 'start', 'started', 'state', 'stay',\n",
       "       'steal', 'stick', 'still', 'stop', 'store', 'story', 'storyline',\n",
       "       'straight', 'strange', 'street', 'strong', 'struggle', 'student',\n",
       "       'studio', 'stuff', 'stupid', 'style', 'subject', 'success',\n",
       "       'successful', 'suddenly', 'superb', 'supporting', 'supposed',\n",
       "       'sure', 'surprise', 'surprised', 'suspense', 'sweet', 'take',\n",
       "       'taken', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking',\n",
       "       'taste', 'team', 'teen', 'teenager', 'television', 'tell',\n",
       "       'telling', 'ten', 'tension', 'term', 'terrible', 'thanks',\n",
       "       'theater', 'theme', 'thing', 'think', 'thinking', 'third',\n",
       "       'though', 'thought', 'three', 'thriller', 'throughout', 'throw',\n",
       "       'time', 'title', 'today', 'together', 'told', 'tom', 'tone',\n",
       "       'took', 'top', 'total', 'totally', 'touch', 'towards', 'town',\n",
       "       'track', 'train', 'trash', 'tried', 'trip', 'trouble', 'true',\n",
       "       'truly', 'truth', 'try', 'trying', 'turn', 'turned', 'twist',\n",
       "       'two', 'type', 'typical', 'understand', 'unfortunately', 'unique',\n",
       "       'unless', 'unlike', 'upon', 'us', 'use', 'used', 'using', 'usual',\n",
       "       'usually', 'value', 'vampire', 'van', 'various', 'version',\n",
       "       'victim', 'video', 'view', 'viewer', 'viewing', 'villain',\n",
       "       'violence', 'violent', 'visual', 'voice', 'wait', 'waiting',\n",
       "       'walk', 'want', 'wanted', 'war', 'waste', 'wasted', 'watch',\n",
       "       'watched', 'watching', 'water', 'way', 'weak', 'week', 'weird',\n",
       "       'well', 'went', 'western', 'whatever', 'whether', 'white', 'whole',\n",
       "       'whose', 'wife', 'william', 'win', 'wish', 'within', 'without',\n",
       "       'woman', 'wonder', 'wonderful', 'wood', 'word', 'work', 'worked',\n",
       "       'working', 'world', 'worse', 'worst', 'worth', 'would', 'write',\n",
       "       'writer', 'writing', 'written', 'wrong', 'yeah', 'year', 'yes',\n",
       "       'yet', 'york', 'young', 'younger', 'zombie'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.189091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  absolutely    accent  across       act  acted    acting  \\\n",
       "0          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.182468   \n",
       "1          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "2          0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "3          0.0   0.0    0.000000  0.137926     0.0  0.000000    0.0  0.000000   \n",
       "4          0.0   0.0    0.321531  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "...        ...   ...         ...       ...     ...       ...    ...       ...   \n",
       "29995      0.0   0.0    0.000000  0.000000     0.0  0.135608    0.0  0.088057   \n",
       "29996      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "29997      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.000000   \n",
       "29998      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.050570   \n",
       "29999      0.0   0.0    0.000000  0.000000     0.0  0.000000    0.0  0.097604   \n",
       "\n",
       "       action  actor  ...  written     wrong  yeah      year  yes  yet  york  \\\n",
       "0         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "1         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "2         0.0    0.0  ...  0.00000  0.000000   0.0  0.055835  0.0  0.0   0.0   \n",
       "3         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "4         0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "...       ...    ...  ...      ...       ...   ...       ...  ...  ...   ...   \n",
       "29995     0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29996     0.0    0.0  ...  0.00000  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29997     0.0    0.0  ...  0.00000  0.000000   0.0  0.074143  0.0  0.0   0.0   \n",
       "29998     0.0    0.0  ...  0.07508  0.000000   0.0  0.000000  0.0  0.0   0.0   \n",
       "29999     0.0    0.0  ...  0.00000  0.189091   0.0  0.102630  0.0  0.0   0.0   \n",
       "\n",
       "       young  younger  zombie  \n",
       "0        0.0      0.0     0.0  \n",
       "1        0.0      0.0     0.0  \n",
       "2        0.0      0.0     0.0  \n",
       "3        0.0      0.0     0.0  \n",
       "4        0.0      0.0     0.0  \n",
       "...      ...      ...     ...  \n",
       "29995    0.0      0.0     0.0  \n",
       "29996    0.0      0.0     0.0  \n",
       "29997    0.0      0.0     0.0  \n",
       "29998    0.0      0.0     0.0  \n",
       "29999    0.0      0.0     0.0  \n",
       "\n",
       "[30000 rows x 1000 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_train.A,columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.93"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mnb = MultinomialNB()\n",
    "tfidf_mnb.fit(tfidf_train.A,y_train)\n",
    "predict_tfidf = tfidf_mnb.predict(tfidf_test.A)\n",
    "accuracy_tfidf = accuracy_score(y_test,predict_tfidf)*100\n",
    "accuracy_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central, theme, movie, seems, confusion, rela...\n",
       "16298    [excellent, example, cowboy, noir, called, une...\n",
       "28505    [ending, made, heart, jump, throat, proceeded,...\n",
       "6689     [chosen, one, appreciate, quality, story, char...\n",
       "26893    [really, funny, film, especially, second, thir...\n",
       "                               ...                        \n",
       "29415    [film, came, gift, offering, blue, unlike, rev...\n",
       "11359    [first, started, watching, movie, looking, kin...\n",
       "575      [big, mark, music, neil, young, glowing, prais...\n",
       "17398    [watching, lady, ermine, wondering, betty, gra...\n",
       "4189     [crappy, miserably, acted, movie, based, subli...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ngrams \n",
    "from nltk.util import ngrams\n",
    "def splitting_dataframe(data):\n",
    "    tokens = data.split()\n",
    "    return tokens\n",
    "\n",
    "data = clean_text_test.apply(splitting_dataframe)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_list(data , ngram_range):\n",
    "    ngram = ngrams(data , ngram_range) # Zip file \n",
    "    ngram_list1 = []\n",
    "    for ngram1 in ngram:  # opening Zip file \n",
    "        ngram_list1.append(\" \".join(ngram1))\n",
    "\n",
    "    return ngram_list1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central, theme, movie, seems, confusion, rela...\n",
       "16298    [excellent, example, cowboy, noir, called, une...\n",
       "28505    [ending, made, heart, jump, throat, proceeded,...\n",
       "6689     [chosen, one, appreciate, quality, story, char...\n",
       "26893    [really, funny, film, especially, second, thir...\n",
       "                               ...                        \n",
       "29415    [film, came, gift, offering, blue, unlike, rev...\n",
       "11359    [first, started, watching, movie, looking, kin...\n",
       "575      [big, mark, music, neil, young, glowing, prais...\n",
       "17398    [watching, lady, ermine, wondering, betty, gra...\n",
       "4189     [crappy, miserably, acted, movie, based, subli...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams = data.apply(lambda x :ngram_list(x,1))\n",
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central theme, theme movie, movie seems, seem...\n",
       "16298    [excellent example, example cowboy, cowboy noi...\n",
       "28505    [ending made, made heart, heart jump, jump thr...\n",
       "6689     [chosen one, one appreciate, appreciate qualit...\n",
       "26893    [really funny, funny film, film especially, es...\n",
       "                               ...                        \n",
       "29415    [film came, came gift, gift offering, offering...\n",
       "11359    [first started, started watching, watching mov...\n",
       "575      [big mark, mark music, music neil, neil young,...\n",
       "17398    [watching lady, lady ermine, ermine wondering,...\n",
       "4189     [crappy miserably, miserably acted, acted movi...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = data.apply(lambda x :ngram_list(x,2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823    [central theme movie, theme movie seems, movie...\n",
       "16298    [excellent example cowboy, example cowboy noir...\n",
       "28505    [ending made heart, made heart jump, heart jum...\n",
       "6689     [chosen one appreciate, one appreciate quality...\n",
       "26893    [really funny film, funny film especially, fil...\n",
       "                               ...                        \n",
       "29415    [film came gift, came gift offering, gift offe...\n",
       "11359    [first started watching, started watching movi...\n",
       "575      [big mark music, mark music neil, music neil y...\n",
       "17398    [watching lady ermine, lady ermine wondering, ...\n",
       "4189     [crappy miserably acted, miserably acted movie...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = data.apply(lambda x :ngram_list(x,3))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec \n",
    "> In Previous Unit Vectorizer and TF-IDF we were not understanding meaning of words everything was based on frequency of the word.\n",
    "> Suppose we have example of KING,MAN,HORSE what comes to our mind \n",
    "* For KING : Power , Rich , Male , Palace \n",
    "* For MAN : No Power , Low Rich , Male , Home \n",
    "* For HORSE : Power ,Male , Tail \n",
    "* We as a human being undefstand the meaning of words by properties of that word.\n",
    "* Same concept will be used for training the model using the properties of that word.\n",
    "* In Count Vectorizer and TF-IDF our output was in saprse matrix.\n",
    "> Sparse matrix : maximum values are zero \n",
    "*  Word2Vec uses dense vector.\n",
    "* In Dense vector there will be no value of Zero.\n",
    "* If it is alloted zero then it will take values close to zero but not zero.\n",
    "* Highest weight alloted is 1.98\n",
    "* In Word2Vec we will see how weights are assigned randomly to vectors/data.\n",
    "> Word2Vec\n",
    "* It is a simple neural network.\n",
    "* It has a single hidden layer.\n",
    "* Vector performance is great.\n",
    "* We dont know what properties it is using for calculating vectors.\n",
    "> Assumptions of Word2Vec\n",
    "* Words with similar context will have similar representation of vectors.\n",
    "> Vectors : Which has magnitude and direction.\n",
    "> Similar context : For KING similar context >> Prince , Princess ,Queen , Ruler , Sultan \n",
    "* In Above case representation of KING and Prince has similar context so representation vectors of KING and Prince will be similar.\n",
    "> Cosine similarity : A * B / |A| * |B|\n",
    "* To check the similarity between vectors we use Cosine similarity.\n",
    "* Range - 1 to +1.\n",
    "* If the value is close to -1 then it means that vectors are not similar at all.\n",
    "* If the value is close to +1 then it means that vectors are similar.\n",
    "* cos 0 = 1 , cos 180 = -1 , as the angle is increasing value is decreasing.\n",
    "* Cosine similarity shows similarity between cosine values.\n",
    "> If the representation of two vectors are similar it means that the angle of vectors is low and so the values of similarity is high.\n",
    "> If two word vectors are closer than there cosine similarity will increase and there will be similarity between vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec invented by Tomas Mikolov in 2013 at google and later he invented Fast Text at Facebook.\n",
    "> Fast Text supports multiple languages.\n",
    "> Word2Vec has a lot of community support as it was invented before Fast Text.\n",
    "> We can use word2vec in 2 ways:\n",
    "1. Pretrained model >> Already built and just download and use it.\n",
    "* It is trained on 3 billion words (From Google news dataset)\n",
    "2. Customized model >> We are building Word2Vec on our own data.\n",
    "* \n",
    "> Architectures of Word2Vec \n",
    "* We have 2 architectures\n",
    "1. CBow \n",
    "* It will have multiple inputs and a single hidden layer and we wil get single output.\n",
    "* CBow >> Continuous Bag of words\n",
    "* Example : \"Rajesh is working in MNC, he is a hardworking guy.\"\n",
    "* ANd we will assume that 'working' is the target word.\n",
    "* And other words will be contextual words.\n",
    "* And so our input layer will be contextual words.\n",
    "* We cannot give words directly as the input layer , we need to convert them in to numeric format.\n",
    "* So our binary representation of 'Rajesh' will be [1,0,0,0,0,0,0,0,0,0]\n",
    "* ANd our binary representation of 'is' will be [0,1,0,0,0,0,0,0,0,0]\n",
    "* Same will work for other contextual words.\n",
    "* Weights will be assigned for each contextual word.\n",
    "* \n",
    "* Inside the hidden layer linear function is present.\n",
    "* After preprocessing in the hidden layer output of the hidden layer will be given to the hidden layer.\n",
    "* In output layer we have Softmax activation function and the output will be 'working'.\n",
    "* It will check the error ands see the the output predicted is correct or not.\n",
    "* If the predicted output is not correct then it will be perform Backward propagation.\n",
    "* It will update the weights and it will perform forward propagation.\n",
    "* After getting right output a weight matrix will be created and that will be the vector representation of the target word.\n",
    "* \n",
    "2. Skipgram \n",
    "* It will have single input layer and single hidden layer and multiple outputs.\n",
    "* Example : \"Rajesh is working in MNC,he is a hardworking guy.\"\n",
    "* And we will assume that \"working\" is the target word.\n",
    "* In skipgram our target word will be our input layer.\n",
    "* And our binary representation will be [0,0,1,0,0,0,0,0,0,0]\n",
    "* Weight will be allocated for the single input layer.\n",
    "* After this input wll be fed to the hidden layer we have linear activation function in the hidden layer.\n",
    "* Output of the hidden layer will be given to the output layer in which we have softmax activation function.\n",
    "* Output from the Output layer will be contextual words.\n",
    "* If correct contextual words are not predicted correctly then it will perform backward propagation and new weight will be assigned to the input layer.\n",
    "* After that we will perform forward propagation , this process will go on till we get correct contextual words as output.\n",
    "* After getting correct contextual words as output it will create a new weight matrix.\n",
    "* Vector representation of any contextual words will be \n",
    "> Weight matrix * Probability of that contextual word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
