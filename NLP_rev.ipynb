{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (NAtural Language processing)\n",
    "* It is a branch of Artificial Intelligence.\n",
    "* Whenever text data will come NLP will come into picture.\n",
    "* The human capability of understanding a language will be given to model by NLP.\n",
    "# SUbSections \n",
    "1. NLU (Natural language understanding)\n",
    "2. NLG (Natural language generation) >> we generate new text \n",
    "* Example > If we have watched a movie and if somebody ask us about the movie.If we tell about the script of the movie then this is called as NLU.If we are telling about the movie in our own language then that it is called NLG.\n",
    "* If we are taking as it is from the original data then it is called as NLU and if we are generating new text from the original data then it is called as NLG.\n",
    "# Basic terms of any language\n",
    "1. Phonemes >> Smallest unit of any language ,characters , speech , sound\n",
    "2. Morphemes(words) & lexemes(run - running , swim - swimming)\n",
    "3. Syntax >> Phrases , Sentences \n",
    "4. Context \n",
    "* Meaning \n",
    "* Combination of Syntaxes \n",
    "> NLP applications\n",
    "1. Sentiment analysis >> It is a classification problem (Text classification)\n",
    "* Tweets >> Positive tweets , Negative tweets , Neutral tweets \n",
    "* Movie reviews >> Positive reviews , Negative reviews , Neutral reviews\n",
    "* In sentiment analysis we analyze the sentiment of the text data whethr it is positive , negative or neutral.\n",
    "* Sentiment analysis is quite important as we are dealing with product based or service based Industry interactions between customers is crucial and we need to understand the review of the product or service which we are providing.We should be able to understand the entiment of the text data which will be helpful for the industry to make necessary changes to satisfy the Customers.\n",
    "* Surveys , google forms , audio files\n",
    "2. Document classification \n",
    "* In this type we will get documents to classify.\n",
    "* In case of 2 documents it will be binary classification and in case of multiple documents it will be Multiclass classification.\n",
    "* Adhar card , PAN card , Driving license , Voter ID \n",
    "3. Text summarization\n",
    "* If we get 50 page data and we want to summarize it in a single page.\n",
    "> Extractive text summarization\n",
    "* In extractive text summarization we extract important lines/sentences and will return the summary of the original data.\n",
    "> Abstractive text summarization \n",
    "* Creating new summary (NLG)\n",
    "4. Topic modelling / Topic Identification \n",
    "* Suppose we have 100 documents of data and we ahve 20 documents related to sports , 30 documents related to geography and 50 documents realted to ethics and we are not aware of that.if we build a model on Topic modelling and this will find out the hidden patterns between the text from the 100 documents.\n",
    "* This model will return output as 20 documents related to 0 Class , 30 documents to 1 class and 50 documents related to 2 class.\n",
    "* And then we will ned to manually classify the headings of the documents.\n",
    "* Model will classify the documents based on similar words or sentences in different clusters.\n",
    "5. Chatbot \n",
    "* Automatic response generation\n",
    "* \n",
    "# NLP Pipeline\n",
    "1. Data Extraction \n",
    "2. EDA \n",
    "3. Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Modelling \n",
    "6. Evaluation \n",
    "7. Deployment \n",
    "* \n",
    "1. Data Extraction \n",
    "* We can get data in different data formats >> JSON , Text , CSV , Images(OCR , Pytessaract , Amazon textract , Google vision)\n",
    "> Data Types \n",
    "* Public data : easily available \n",
    "* Private data : belongs to some organiozation \n",
    "> If Data is not available \n",
    "* WE take data from Public options >> Webscrapping \n",
    "> If data is available \n",
    "* Then we will download the data \n",
    "* In this way we get the data.\n",
    "> What is noise in text data?\n",
    "* \"We >>> loved $@ /\\ # the product will ___ defientlri recommend\"\n",
    "* Noise is available in above text.\n",
    "> Quality of data\n",
    "> If there is noise in the text data then what would we do?\n",
    "* We will process on extreme data processing.\n",
    "> If there is minimal noise then what would we do?\n",
    "* Minimal data preprocessing\n",
    "* \n",
    "* After this we will get clean data.\n",
    "> Quantity of data?\n",
    "* In case of POC suppose we have 1GB of data then that is a good thing.\n",
    "* \n",
    "> Problems regarding data?\n",
    "* Quantity of data \n",
    "* Quality of data \n",
    "* Exact data / Specific data is not available for our usecase.\n",
    "* Donot have continous flow of data.\n",
    "* In Industry we will not get data in one time we will get data in cycles.\n",
    "* Montly cycle , quarterly cycle , yearly cycle \n",
    "* \n",
    "2. EDA \n",
    "> Ngram \n",
    "> Wordcloud \n",
    "> Keyphrase Extraction \n",
    "* \n",
    "> Ngram\n",
    "* Unigram \n",
    "* Bigram \n",
    "* Trigram \n",
    "* Quadragram \n",
    "> Suppose we have a example of \"Rajesh is hardworking guy\"\n",
    "* Unigram : [Rajesh , is , hardworking , guy]\n",
    "* Bigram : [Rajesh is , is hardworking , hardworking guy]\n",
    "* Trigram : [Rajesh is hardworking, is hardworking guy]\n",
    "> Why Ngram?\n",
    "* It is a part of EDA.\n",
    "* To get insights from the data(Understanding words)\n",
    "* Suppose we are considering positive reviews we will get >> Positive words by Ngram \n",
    "* Also when we are considering negative reviews we will >> Negative words by Ngram \n",
    "* \n",
    "* To get domain specific stopwords.\n",
    "* \n",
    "> Word cloud \n",
    "* If a frequency of any word is higher than the word font will be higher in that word cloud.\n",
    "* If a frequency of any word is lower than the word font will be lower in that word cloud.\n",
    "* \n",
    "> Key phrase extraction \n",
    "* To extract important keyphrase or keywords.\n",
    "* RAKE , YAKE algorithm used for key phrase extraction.\n",
    "* \"We are learning NLP\" and we apply key phrase extraction.\n",
    "* \"learning NLP\" will be important keyword.\n",
    "* \n",
    "4. Preprocessing \n",
    "> Tokenization \n",
    "* Sentence tokenization \n",
    "* Word tokenization \n",
    "* Suppose we get text \"We are learning NLP.NLP is a huge domain.\"\n",
    "* Sentence tokenization : [We are learning NLP. , NLP is a huge domain.]\n",
    "* Word tokenization : [We, are , learning , NLP , . , NLP , is , a , huge , domain , .]\n",
    "> How does sentence tokenization works or how does it knows it is a sentence?\n",
    "* It will check Syntax.\n",
    "* It will check for punctuaution marks : ! , . , ?\n",
    "* It will check for Consumptions : and , but \n",
    "* \n",
    "> Normalization \n",
    "* Suppose we have words \"G R E A T\" and another word \"great\" these 2 words have the same meaning but our model doesnot understamd them. Model will allocate number for \"G R E A T\" and another number for \"great\" which is not good.\n",
    "* So i NLP we will convert this into single case depends upon us we keep it in lowercase or Uppercase. In Industry standard parctise is to keep it in lowecase.\n",
    "* \n",
    "> Remove punctuation marks\n",
    "* We have string library for this purpose and we import punctuation.\n",
    "* In this punctuation string we have punctuation symbols.\n",
    "* And we can use this to remove punctuation.\n",
    "* \n",
    "> Remove stopwords \n",
    "* Language specific stopwords : Words in text which contributes grammatically but doenot have any impact on the sentence. >> is , has , we , him \n",
    "* Domain Specific stopwords : Words in Domain text which contributes grammatically but doenot have any impact on the sentence. >> doctor , tablet , capsule , treatment.\n",
    "* \"Rajesh is suffering from cancer.Right now Doctor Pravin is treating him.We have given him XYZ tablet.\"\n",
    "* \n",
    "> Lemmatization(lemma) & stemming(stem) : Words pruining \n",
    "* Lemma and stem are greek words.\n",
    "* lemma >> meaningful root word >> has dictionary in backend(Word net dictionary) :\n",
    "* Example >> Running >> it will check the word in word net dictionary whether it is meaningful or not >> Is there any other root word available in word net dictionary >> Output = Run\n",
    "* \n",
    "* Stem >> Root word >> aggresive pruiner : example >> running = run , swimming = swim.\n",
    "* Believe = Beli as it is a aggresive pruiner but it is wrong.\n",
    "* \n",
    "* We should use both lemma and stem and compare the results.\n",
    "* \n",
    "> Contraction mapping : Text expanding \n",
    "* didn't >> did not , doesn't >> does not , haven't >> have not \n",
    "* Supose we have review : \"I didn't like the movie\" , \"I liked the movie\" and we remove the stopwords then we will get as \"like movie\" and \"liked movie\".\n",
    "* In first case we are getting the wrong interpreatation which is not good so we are contract mapping.\n",
    "* We can remove stopwords from stopwords list.\n",
    "* \n",
    "> Handling accented characters \n",
    "* We have unidecode library to handle accented characters.\n",
    "* Accented characters >> s : $ , a : @ \n",
    "* We will convert $ to s and @ to a\n",
    "* \n",
    "> Auto correction\n",
    "* Correct speeling of words \n",
    "* We have autocorrect library and text blob library.\n",
    "* \n",
    "5. Feature Engineering \n",
    "* We convert text to numeric format/Vectors in Feature Engineering of NLP.\n",
    "* Converting text to numeric format/Vectors in Feature Engineering is called as Word Embeddings.\n",
    "> Word Embedding\n",
    "* Frequency based word embedding : It focuses on word frequency and returns in numeric format.\n",
    "* Count Vectorizer \n",
    "* TFIDF \n",
    "* \n",
    "* Prediction based word embedding : It uses algorithm in backend and converts text to numeric format.\n",
    "* Word to weight \n",
    "* Fast Text \n",
    "* Doc2ve \n",
    "* \n",
    "6. Modelling \n",
    "* We have data in Numeric format.\n",
    "* We can use all classification algorithms.\n",
    "* Logistic Regression , Naive Bayes , Random Forest , SVM , Decision Tree , AdaBoost , RNN , LSTM.\n",
    "* \n",
    "7. Evaluation \n",
    "* Confusion Matrix , Recall , Precision , accuracy score , F1-score \n",
    "* If we have low accuracy then we will jump on preprocessing or go for feature engineering.\n",
    "* Suppose we have used frequency based first then we will use prediction based for better result.\n",
    "* \n",
    "8. Deployment \n",
    "* If we get good accuracy then we will go for Deployment.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Natural Language Processing\n",
    "* It is a branch of Artificial Intelligence.\n",
    "* Whenever text data will come NLP will come into picture.\n",
    "* The Human capability of understanding language will be given to the model by NLP.\n",
    "# Subsections \n",
    "1. NLU (Natural Language understanding)\n",
    "2. NLG (Natural language generation) >> We generate new text \n",
    "* Example > If we have watched a movie and if somebody ask us about the movie. If we tell about the script of the movie as it is then it is called as NLU.If we tell about the movie in our own language then that is called as NLG.\n",
    "* If we are taking as it is from the original data then that is called as NLU and if we are generating new text from the original data then that is called as NLG.\n",
    "# Basic terms of any language \n",
    "1. Phonemes >> smallest unit of any language , characters , speech , sound \n",
    "2. Morphemes(Words) & lexemes( run : running , swim : swimming)\n",
    "3. Synatx >> Phrases ,sentences \n",
    "4. Context \n",
    "* Meaning\n",
    "* Combination of Syntaxes\n",
    "> NLP applications \n",
    "1. Sentiment analysis >> It is a classification problem (text classification)\n",
    "* Tweets >> Positive tweets , Negative tweets , Neutral tweets\n",
    "* Movie reviews >> Positive reviews , Negative reviews , Neutral reviews\n",
    "* In sentiment analysis we analyze the sentiment of the text data whether it is positive or negative or neutral.\n",
    "* Sentiment analysis is quite important as we are dealing with product based and service based Industry , Interactions between customers is crucial and we need to understand the review of the product or service which we are providing.We should be able to understand the sentiment of the text data which will be helpful for the industry to make necessary changes to satify the customers.\n",
    "* Surveys , google forms , audio files\n",
    "2. Document classification\n",
    "* In this type we will get documents to classify.\n",
    "* In case of 2 documents it will be binary classification nad in case of multiple documents we will get Multi class classification.\n",
    "* Adhar card , PAN card , Driving License , Voter ID \n",
    "3. Text Summarization\n",
    "* If we get 50 page data and we want to summarize it into a single page.\n",
    "> Extractive text summarization \n",
    "* In Extractive text summarization we extract important lines/sentences and wil return the summary of the original data.\n",
    "> Abstractive text summarization \n",
    "* Creating new summary (NLG)\n",
    "4. Topic Modelling / Topic Identification \n",
    "* Suppose we have 100 documents of data and we have 20 documents related to sports , 30 documents related to geography , 50 documents related to ethics and we are not aware of that.If we build a model on Topic modelling and this will find out the hidden patterns between the text from the 100 documents.\n",
    "* This model will return output as 20 documnets realted to 0 class , 30 documents related to 1 class , 50 documents related to 2 class.\n",
    "* And then we will need to manually classify the headings of the documents.\n",
    "* Model will classify the documents based on similar words or sentences in different clusters.\n",
    "5. Chatbot \n",
    "* Automatic response generation\n",
    "* \n",
    "# NLP pipeline \n",
    "1. Data Extraction \n",
    "2. EDA \n",
    "3. Preprocessing \n",
    "4. Feature Engineering \n",
    "5. Modelling \n",
    "6. Evaluation \n",
    "7. Deployment \n",
    "* \n",
    "1. Data Extraction \n",
    "* We can get data in different formats >> JSON , text , CSV , Images(OCR , Pytessarct , Amazon Textract , Google vision)\n",
    "> Data Types \n",
    "* Public data >> Easily avaialable \n",
    "* private data >> belongs to some orginization \n",
    "> If data is not available \n",
    "* We take data from Public options >> Webscrapping \n",
    "> If data is available \n",
    "* We will download the data.\n",
    "* In this way we get the data.\n",
    "> What is noise in text data?\n",
    "* \"We >>> loved $@ /\\ # the product will ___ defientlri recommend\"\n",
    "* Noise is available in above example.\n",
    "> Quality of the data \n",
    "> If there is noise in text data then what we would do?\n",
    "* We will process on extreme data preprocessing.\n",
    "> If there is minimal noise then what we would do?\n",
    "* Minimal data preprocessing\n",
    "* \n",
    "* After these we will get clean data.\n",
    "> Qunatity of the data?\n",
    "* In case of POC suppose we have 1Gb of data then that is a good thing.\n",
    "* \n",
    "> Problems regarding the data?\n",
    "* Quantity of the data\n",
    "* Quality of the data \n",
    "* Exact data / specific data is not available for our use case.\n",
    "* Donot have continous flow of data.\n",
    "* In Industry we will not get data in one time we will get data in cycles.\n",
    "* Monthly cycle , quarterly cycle , yearly cycle \n",
    "* \n",
    "2. EDA \n",
    "> Ngram \n",
    "> Word Cloud \n",
    "> Keyphrase extraction \n",
    "* \n",
    "> Ngram \n",
    "* Unigram \n",
    "* Bigram \n",
    "* Trigram\n",
    "* Quadragram\n",
    "> Suppose we have a example \"Rajesh is a hardworking guy.\"\n",
    "* Unigram = [Rajesh , is , a , hardworking , guy]\n",
    "* Bigram = [Rajesh is , is a , a hardworking ,hardworking guy]\n",
    "* Trigram = [Rajesh is a , is a hardworking , a hardworking guy]\n",
    "> Why Ngram?\n",
    "* It is a part of EDA.\n",
    "* To get insights from the data(Understanding words)\n",
    "* Suppose we are considering positive reviews we will get >> positive words by Ngram \n",
    "* Also when we are considering negative reviews we will get >> negative words by Ngram\n",
    "* \n",
    "* To get domain specific stopwords.\n",
    "* \n",
    "> Word Cloud \n",
    "* If a frequency of any word is higher than the word font will be higher in that word cloud.\n",
    "* If a frequency of any word is lower than the word font will be lower in that word cloud.\n",
    "* \n",
    "> Key phrase extraction \n",
    "* To extract important keyphrases or Keywords.\n",
    "* RAKE , YAKE algorithm used for key phrase extraction.\n",
    "* \"We are learning NLP\" and we apply key phrase extraction.\n",
    "* \"learning NLP\" will be important keyword.\n",
    "* \n",
    "4. Preprocessing\n",
    "> Tokenization :\n",
    "* Sentence tokenization\n",
    "* Word tokenization\n",
    "* Suppose we get text \"We are learning NLP.NLP is a huge domain.\"\n",
    "* Sentence tokenization : [We are learning NLP. , NLP is a huge domain.]\n",
    "* Word tokenization : [We , are , learning , NLP, . , NLP , is , a , huge , domain , .]\n",
    "> How does Sentence tokenization works or how does it know ot is a sentence?\n",
    "* It will check Syntax.\n",
    "* It will check for punctuation marks >> ! , . , ?\n",
    "* It will check for consumtions >> and , but \n",
    "* \n",
    "> Normalization\n",
    "* Suppose we have words \"G R E A T\" and another word \"great\" these 2 words have the same meaning but our model doesnot understand them.Model will allocate number for \"G R E A T\" and another number for \"great\" which is not good.\n",
    "* So in NLP we will convert this in single case depends upon us we keep it in lowecase or uppercase. In Industry standard practise is in lowercase.\n",
    "* \n",
    "> remove punctuation / symbols \n",
    "* we have string library we import punctuation \n",
    "* In this punctuation string we will have multiple symbols.\n",
    "* And we can use this to remove punctuation.\n",
    "* \n",
    "> remove stopwords\n",
    "* Language specific stopwords : Words in text which contributes grammatically buty doenot have any impact on the sentence. >> is , has , we , him \n",
    "* Domain specific stopwords : Words in domain text which contributes grammatically buty doenot have any impact on the sentence. >> doctor , tablet , capsule , treatment\n",
    "* \"Rajesh is suffering from cancer.Right now Doctor Pravin is treating him.We have given him XYZ tablet.\"\n",
    "* \n",
    "> Lemmatization(lemma) & stemming(stem) : words pruining \n",
    "* Lemma and stem are greak words.\n",
    "* lemma >> meaningful root word >> has dictionary in backend(word net dictionary) \n",
    "* example >> Running >> It will check the word in wordnet dictionary whether it is meaningful or not >> Is there any other root word available in word net dictionary >> Output = Run \n",
    "* \n",
    "* Stem >> Root word >> aggresive prunner >> example : running >> run , swimming >> swim\n",
    "* Believe >> Beli which is wrong as it is aggresive prunner \n",
    "* \n",
    "* We should use both lemma and stem and compare the results.\n",
    "* \n",
    "> Contraction mapping : text expanding\n",
    "* didn't >> did not , doesn't >> does not , haven't >> have not \n",
    "* Suppose we have review : \"I didn't like the movie\" , \"I liked the movie\" and we romve the stopwords then we will get as \"like movie\" and \"liked movie\".\n",
    "* In first case we are getting the wrong interpretation of the original sentence.which is wrong so we are using contraction mapping.\n",
    "* We can remove stopwords from stopwords list.\n",
    "* \n",
    "> Handling accented characters: \n",
    "* We have unidecode library to handle accented characters.\n",
    "* Accented characters : s >> $ , a >> @ \n",
    "* We will convert $ to s and @ to a\n",
    "* \n",
    "> Autocorrection \n",
    "* Correct spellings of word \n",
    "* We have autocorrect library and text blob library.\n",
    "* \n",
    "5. Feature Engineering \n",
    "* We convert text to numeric format/ Vectors in feature engineering of NLP.\n",
    "* Converting text to numeric format/ Vectors in feature engineering is called as Word embedding.\n",
    "> Word embedding \n",
    "* Frequency based word embedding : It focuses on word frequency and returns numeric format.\n",
    "* Count vectorizer \n",
    "* TFIDF \n",
    "* \n",
    "* Prediction based word embedding : It uses algorithm in backend and convert text to numeric format.\n",
    "* Word to weight\n",
    "* fast text \n",
    "* Doc2Ve\n",
    "* \n",
    "6. Modelling \n",
    "* We now have data in numeric format.\n",
    "* We can use all classification algorithms.\n",
    "* Logistic regression , SVM ,Random Forest , AdaBoost , Decision Tree , Naive Bayes , LSTM , RNN.\n",
    "* \n",
    "7. Evaluation \n",
    "* Confusion matrix , accuracy score , precision score , recall score , F1 score\n",
    "* If we have low accuracy we will jump on preprocessing or go for feature engineering.\n",
    "* Suppose we have frequent based first then we will go for prediction based for better accuracy.\n",
    "* \n",
    "8. Deployment \n",
    "* If we get good accuracy then we will go for Deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
