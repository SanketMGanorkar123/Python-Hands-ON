{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Unsupervised Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the name suggests unsupervised learning is a machine learning technique in which models are not supervised using training data set.Instead models itself find hidden patterns and insights from the given data.It can be compared to learning which takes place in human brain while learning new things.\n",
    "* It can be defined as a type of machine learning in which models are trained on Unlabeled data and are allowed to act on that data without supervision.\n",
    "* Unsupervised learning cannot be directly applied to regression or classification problems becuase unlike supervised learning we have the input data but no corresponding output data.The goal of the unsupervised learning is to find the uderlying structure of the dataset , group that data according to similarities and represent that data in compressed format.\n",
    "* In summary, the unsupervised learning is a valuable technique for tasks such as clustering,dimensionality reduction and anomaly detection.It mirrors the way humans learn from unstructured information , finding hidden patterns and insights and representation within the data set.\n",
    "> key points \n",
    "* Unsupervised learning allows the model to discover patterns and relationships in the Unlabeled data.\n",
    "* Clustering algorithms groups the similar data points together based on their inherent characteristics.\n",
    "* Feature extraction captures essential information from the data,enabling the model to make meaningful distinctions.\n",
    "* Label association assigns categories to the clusters based on extracted patterns and characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Explain the steps of the k-Means Clustering Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means Clustering is an Unsupervised learning algorithm , which groups the Unlabelled dataset into different clusters , here k defines the number of pre defined clusters that need to be created in process ,as if K=2 there will be 2 clusters and for K = 3 there will be 3 clusters and so on.\n",
    "* It is an iterative algorithm that divides the unlabeled dataset into k different clusters in such a way that each dataset belong only one group that has similar properties.\n",
    "* It allows us to cluster the data into different groups and a convenient way to discover the categories of groups in the unlablled dataset on its own without the need for any training.\n",
    "* It is a centroid based clustering algorithm , where each cluster is associated with a centroid.The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
    "* The K means clustering algorithm mainly performs two tasks \n",
    "1. Determines the best value for K center points or Centroids by an iterative process.\n",
    "2. Assigns each data point to its closest K centre. Those data points which are near to the particular K-Centre,create a cluster.\n",
    "\n",
    "* Hence each cluster has data points with some commonalities and it is away from other clusters.\n",
    "> Step 1 \n",
    "* Determine the value of K \n",
    "> Step 2\n",
    "* Centroid Initialization \n",
    "> Step 3 \n",
    "* Measure the distance \n",
    "> Step 4 \n",
    "* Assign to the nearest cluster \n",
    "> Step 5 \n",
    "* New Centroid Initialization \n",
    "> Step 6 \n",
    "* Not Convergence \n",
    "* Not Maximum number of iterations \n",
    "* Again we will go for Step 3\n",
    "> Step 7\n",
    "* Measure the Variance \n",
    "* Get the lowest sum of variance \n",
    "> Step 8\n",
    "* Repeat until get the lowest sum of variance \n",
    "* Repeat process from Step 2\n",
    "> We get Clustering result \n",
    "\n",
    "> Step 1 > Initialization \n",
    "* Choose the number of clusters(K) that you want to identify in the dataset.\n",
    "* Randomly intialize the centroid of the K clusters.\n",
    "* A Centroid is the centre point of a Cluster.\n",
    "\n",
    "> Step 2 > Assign data points to clusters \n",
    "* For each data point in the dataset ,calculate the distance from each centroid.\n",
    "* Assign the data point to the cluster whose centroid is the closest( Using distance metrics like Eucledian Distance)\n",
    "\n",
    "> Step 3 > Update Centroids \n",
    "* Recalculate the centroids of the clusters based on the data points assigned to each cluster.\n",
    "* The new centroid is the mean of all the data points in the cluster.\n",
    "> Repeat \n",
    "* Repeat steps 2 and 3 until convergence is achieved.Convergence is achieved when the assignments of data points to clusters no longer changes significantly or when a predetermined number of iterations is reached.\n",
    "* Its important to note that K-means is sensitive to the initial choice of centroids so the algorithm may converge to different solutions based on different initializations.To mitigate this multiple runs with different initialization may be performed and the best result can be chosen.\n",
    "\n",
    "> Step 4 \n",
    "* The algorithm converges and each data point is assigned to specific cluster.\n",
    "* The Final Clusters and their centroids repersent the grouping of data points based on similarity.\n",
    "\n",
    "> Step 5 Evaluate and interpret \n",
    "* Evaluate the quality of the clustering using metrics such as the sum of squared distance between the data points and their assigned centroids(Inertia)\n",
    "* Interpret the results and analyze the characteristics of each cluster to gain insight into the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is K in K-means algorithm and what is its significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means Clustering is an Unsupervised learning algorithm that groups an unlabeled set of data points into different clusters. The parameter K defines the predefined clusters to be created. For instance if K = 2, then we will have two clusters and for K = 3 we will have 3 clusters and so on. The algorithm is iterative , dividing the dataset into K clusters so that each data point belong to the group with similar properties.\n",
    "* This Centroid Based clustering algorithm associates each cluster with a Centroid , aimimg to minimize the sum of distances between data points and their corresponding clusters.The optimal Centroid is computed through iterations until convergence. The choice of K is Crucial as it determines the granularity of the clusters and significantly impact the results.\n",
    "* In the K-means clustering algorithm the letter k represents the number of clusters the algorithm aims to identify in the data set.It is a flat clustering algorithm ,presuming the known number of clusters. The algorithm assigns data points to Clusters ,minimizing the sum of squared distance between data points and centroids. \n",
    "* Selecting an appropriate value for K is essential for obtaining meaningful and interpretable results.Technique like the elbow method or Silhouette analysis aid in determining the optimal K ,Striking a balance between capturing meaningful patterns and avoiding overfitting or Oversimplification in the Cluster Structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What is the difference between the Manhattan Distance and Euclidean Distance in Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manhattan distance and Euclidean distance are both metrics used in Clustering to measure the similarity or dissimilarity between the data points.\n",
    ">> Eucledian distance \n",
    "* Also known as straight line distance or L2 norm.\n",
    "* It calculates the distance between two points in a Eucledian space.\n",
    "* For two points (X1 , Y1) and (X2 , Y2) the eucledian distance is the square root of the sum of squared differences in each dimension.\n",
    "* Eucledian distance is sensitive to both large and small differences in all dimensions.\n",
    "* Formula = sqrt[(X2 - X1)^2 + (Y2 - Y1)^2]\n",
    "\n",
    ">> Manhattan distance\n",
    "* Also known as City block distance or L1 norm.\n",
    "* It calculates the distance between two points by summing the absolute diffrences between their coordinates.\n",
    "* For two points (X1 , Y1) and (X2 , Y2) the manhattan distance is the sum of the absolute differences in X and Y dimensions.\n",
    "* Manhattan distance is less sensitive to outliers and emphazies differences along each axis independently.\n",
    "* Formula = |X2 - X1| + |Y2 - Y1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What are some Stopping Criteria for k-Means Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These criteria helps decide when the algorithm has reached a satisfactory or optimal solution.\n",
    "* Here are some common stopping criteria for K means Clustering \n",
    "1. Centroid stability \n",
    "* Centroids of newly formed clusters donot change , which means that data points will remain in the same cluster.\n",
    "3. Points in cluster \n",
    "* Points remain in the same cluster.\n",
    "2. Maximum Iterations\n",
    "* Set a predefined maximum number of iterations.If the algorithm doesnot converge by reaching this limit stop the process.This is a common safeguard to prevent the algorithm from running indefinitely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What is the main difference between k-Means and k-Nearest Neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Objective \n",
    "> K-nearest neighbor (KNN) \n",
    "* KNN is a Supervised learning algorithm used for classification and regression.It makes predictions for new unseen data points based on the majority class(For Classification) or the average of nearby points (For regression) in the feature space.\n",
    "> K Means Clustering \n",
    "* Kmeans is an Unsupervised learning algorithm used for Clustering. Its goal is to partition the dataset into k clusters , with each cluster having data points that are more similar to each other than to those in other clusters.\n",
    "\n",
    "2. Nature \n",
    "> KNN \n",
    "* Its a lazy learning algorithm ,meaning it doesnt build a model during the training phase.Instead it memorizes the entire data set and uses it for predictions when needed.A lazy learner doenot have a training phase.\n",
    "> Kmeans\n",
    "* Its an eager learning algorithm that partitions the data into clusters during the training phase.The algorithm assigns data points to each cluster based on their features.An eager learner has a model fitting that means a training step.\n",
    "\n",
    "3. Usage \n",
    "> KNN \n",
    "* Typically used for classification and regression tasks.It requires labelled training data to make predictions.\n",
    "> Kmeans\n",
    "* Used for clustering Unlabelled data.It groups data points based on their similarities without using any pre existing labels.\n",
    "\n",
    "4. Supervision \n",
    "> KNN \n",
    "* Supervised learning algorithm as it requires labelled training data.\n",
    "> Kmeans \n",
    "* Unsupervised learning algorithm as it doesnt rely on labellled data for clustering.\n",
    "> summary \n",
    "* KNN is a Supervised learning algorithm used for prediction tasks while Kmeans clustering is an Unsupervised learning algorithm used for grouping similar data points.\n",
    "\n",
    "5. Application \n",
    "> KNN > Classification and regression tasks \n",
    "> K means > Customer Segmemtation ,Image compression ,anomaly detection ,and other clustering tasks\n",
    "\n",
    "6. Limitations \n",
    "> KNN \n",
    "* Sensitivity to the choice of K and distance metric \n",
    "> Kmeans\n",
    "* Sensitivity to initial placement of cluster centres and assumptions of isotropic and equally sized data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What is WCSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WSS is also called as \"Within sum of squares\" is another solution under the Kmeans algorithm which helps to decide the value of K(Number of Clusters).The values to taken the plot the WSS plot will be the variance from each observation in the cluster to its Centroid ,Summing up to obtain a value.\n",
    "* Within Cluster sum of Sqaures (WCSS) which measures the sqaured average distance of all the points within the cluster to the Cluster Centroid .To Calculate WCSS , you first find the Eucledian distance between the given point and the centroid to which it is assigned. You the iterate the process for all the points within the cluster and then sum the values for the Cluster and divide by the number of points.Finally you calculate the average across all clusters.This will give you the average WCSS.\n",
    "* Essentially WCSS measures the variability of the observations within each cluster.\n",
    "* Compactness or Cohesion \n",
    "* Lower WCSS values indicate that the data points within each cluster are closer to the centroid ,implying more compact and cohesive Clusters.\n",
    "* The goal of the Kmeans algorithm is to find cluster assignments and centroids that minimize the overall WCSS across all clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is Elbow Method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Elbow method is a technique used to determine the optimal numbers of clusters(K) in a kmeans clustering algorithm.It is named \"Elbow\" because the method looks for a point on the graph where the reduction in the within cluster sum of Squares(WCSS) begins to slow down ,resembling the bending of the elbow.\n",
    "1. Run Kmeans for different values of K\n",
    "* Execute the k-means clustering algorithm for various values of K(Number of Clusters) starting from a small value and gradually increasing it.\n",
    "2. Calculate WCSS for each K \n",
    "* For each value of K calculate the WCSS which is the sum of squared distances between data points and their assigned cluster centroids.\n",
    "3. Plot the Elbow Curve \n",
    "* Plot a graph where the x - axis represents the number of clusters(K) and the y - axis represents the corresponding WCSS values\n",
    "4. Identify the Elbow point \n",
    "* Look for the elbow or bend in the curve.This is the point where further increasing the number of Clusters doesnot significantly reduce the WCSS .\n",
    "5. Select the optimal K \n",
    "* The optimal number of Clusters (K) is typically chosen at the point where the reduction in WCSS begins to slow down.\n",
    "* This is often the point at the elbow of the curve.\n",
    "* The rationale behind the elbow method is that as the number of clusters increases the WCSS tends to decrease because each data point is closer to its clusters Centroid.However after a certain point adding more clusters doesnot lead to significant reduction in WCSS and the curve begins to flatten out.\n",
    "* By selecting the number of clusters at the elbow point , practitioners aim to strike a balance between capturing the underlying structure in the data (Represented by lower WCSS) and avoiding overfitting or creating too many clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What is a centroid point in K means Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In K-means clustering a centroid is a representative point that serves as the center of the cluster.The K-means algorithm aims to partition the dataset into K clusters and each cluster is characterized by its centroid.These Centroids are crucial in defining cluster and assigning data points to the cluster that has the closest centroid.\n",
    "> Heres how the k-means clustering algorithm works with Centroids\n",
    "1. Initialization\n",
    "* Choose k initial centroids randomly from the dataset.These Centroids act as the starting point of the Cluster.\n",
    "2. Assignment \n",
    "* For each data point in the dataset calculate the distance to each centroid\n",
    "* Assign the data point to the cluster whose centroid is closest.(Typically using Euclidean distance)\n",
    "3. Update Centroids\n",
    "* Recalculate the Centroid of each cluster based on the mean(Average) of the data points assigned to that cluster.\n",
    "4. Repetition\n",
    "* Repeat the assignment and centroid update steps iteratively until the Convergence is reached.Convergence occurs when the Centroids no longer change significantly or when a predefined number of iterations is reached.\n",
    "* The centroid of the cluster is the point that minimizes the sum of sqaured distances from itself to all the data points within that cluster.\n",
    "* The K-means algorithm iteratively refines the Centroid and cluster assignments to minimize the (WCSS) Within cluster sum of sqaures and create well defined clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Is Feature Scaling required for the K means Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yes feature scaling is often recommended for the k-means clustering algorithm.Feature scaling is a preprocessing step that standarizes the range of independent variables or features in the dataset.The Goal is to ensure that all the features contribute equally to the similarity or dissimilarity measures used by the k-means clustering algorithm.\n",
    "* The main reason for applying feature scaling in k-means clustering is that the algorithm is sensitive to the scale of the features.Since k-means relies on distance measures(Euclidean distance) to assign data points to the clusters,features with larger scales may dominate the clustering process.Without proper scaling k-means clustering might give more weight to the features with larger numerical ranges leading to biased results.\n",
    "* Common methods of feature scaling are\n",
    "1. MinMax Scaling (Normalization)\n",
    "* Scales the value to a specific range often between 0 and 1\n",
    "* Formula > (X - Xmin) / (Xmax - Xmin)\n",
    "2. Standardization (Z score Normalization)\n",
    "* Scales the value to have a mean of 0 and a standard deviation of 1 \n",
    "* Formula > (X - Xmean) / std\n",
    "\n",
    "* Applying feature scaling ensures that all features have a similar influence on the clustering process and it helps the algorithm converge more efficiently.It also improves the interpretability of the results making it easier to compare the importance of different features in contributing to the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. What is Normalization? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalization in the context of data preprocessing refers to the process of scaling and transforming the features of data set so that they may fall within a specific range.The Goal is to ensure that all features contribute equally to the analysis ,preventing features with larger scales from dominating the compuations.Normalization is particularly important or useful for algorithms that are distance based algorithms.\n",
    "1. MinMax Scaling (Normalization)\n",
    "* Scales the value to a specific range often between 0 and 1\n",
    "* Formula : (X - Xmin) / (Xmax - Xmin) \n",
    "> When to use it ?\n",
    "1. Use Minmax Scaling when you want to scale features to a specific range typically between 0 and 1.\n",
    "2. Useful when you have a clear understanding of the minimum and maximum possible values for the features.\n",
    "3. The algorithms on which the data are being trained donot make presumptions about the data distribution such as Artificial Neural Network.\n",
    "\n",
    "* Here Xmax and Xmin are the maximum values and the minimum values of the features.\n",
    "> Why should we use Feature scaling ?\n",
    "1. Gradient descent based algorithms\n",
    "* Machine Learning algorithms like linear regression , logistic regression , neural network , PCA (Principal Component Analysis) etc that uses gradient descent as an optimization technique require data to be scaled.\n",
    "* The presence of a feature value X in the formula will affect the step size of the gradient descent.The difference in the ranges of features will cause different step sizes for each feature .to ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all features, we scale the data before feeding it to the model.\n",
    "* Having Features on a similar scale can help the gradient descent converge more quickly towards the minima.\n",
    "2. Distance based algorithm \n",
    "* Distance algorithms like KNN , K-means clustering and SVM (Support vector Machine) are most affected by the range of features.This is because they are distance based algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. What is Standardization? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standardization is a specific type of normalization that involves transforming the features of a dataset th have a mean of 0 and a standard deviation of 1.Z score normalization is a common method of Standardization.Note in that case the values are not restricted to a particular range.\n",
    "* Z score normalization(Standardization)\n",
    "* Formula > X - Xmean / std\n",
    "> When to use it ?\n",
    "1. Use Z score normalization(Standardization) when you want to ensure that features have comparable means and standard deviations.\n",
    "2. Patricularly useful when working with algorithms that assume a Gaussian distribution or when you need to compare features on a Standardization scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. How would you Pre-Process the data for k-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means input data requirements \n",
    "1. Numerical varaiables only \n",
    "* K-means uses distance based measurements to determine the similarity between the data points.If you have Categorical data use k-modes clustering , if data is mixed use k-prototype clustering.\n",
    "2. Data has no noises or Outliers\n",
    "* k-means is very sensitive to outliers and noisy data.\n",
    "3. Data has Symmetric distribution of variables (It isnt skewed)\n",
    "* Real data always has outliers and noise and its difficult to get rid of it . Transformation data to normal distribution helps to reduce the impact of these issues.In this way its much easier for the algorithm to identify clusters.\n",
    "4. Variables on the same scale \n",
    "* Have the same mean and variance usually in the range -1.0 to +1.0 (STandardized data) or 0 to 1 (Normalized data)\n",
    "* For the machine learning algorithm to consider all attributes as equal they must all have the same scale.\n",
    "5. There is no collinearity ( A high level of corelation between two varaiables) \n",
    "* Corelated varaibles are not useful for machine learning segmentation algorithms because they represent the same characteristic of a segment.So corelated variables are nothing but Noise.\n",
    "6. Few numbers of dimension \n",
    "* As the number of dimensions (Variables) increases a distance based similarity measures converge to a constannt value between any given examples.The more variables the more difficult to find strict differences between instances.\n",
    "> Stages of data preprocessing Kmeans Clustering \n",
    "1. Data cleaning \n",
    "Removing duplicates, \n",
    "removing irrlevant obsercvvations and errors, \n",
    "removing unnecessary columns ,\n",
    "handling inconsistent data ,\n",
    "handling outliers and noise\n",
    "2. Handling missing data \n",
    "3. Data Integration \n",
    "4. Data Transformation \n",
    "* Handling skewness \n",
    "* Data Scaling \n",
    "* Feature construction \n",
    "5. Data Reduction \n",
    "* Removing dependent (Highly corelated) varaibles\n",
    "* Feature selection \n",
    "* PCA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Which metrics can you use to find the accuracy of the K means Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evalauting the performance of the K-means algorithm involves using various metrics.Here are some commonly used metrics to assess the accuracy of the K-means algorithm \n",
    "1. Inertia or Within cluster sum of sqaures(WCSS)\n",
    "* Inertia measures the sum of squared distances between each data point and its assigned Centroid.A lower inertia indicates a better clustering algorithm .You can use the inertia_attribute in scikit learn K-means module to access this value.\n",
    "* from sklearn.cluster import KMeans\n",
    "* Assuming data as your input data \n",
    ">kmeans = KMeans(n_clusters = your_desired_number_of_clusters)\n",
    ">kmeans.fit(data)\n",
    ">inertia = kmeans.inertia_\n",
    ">print(\"Inertia:\", inertia)\n",
    "\n",
    "2. Silhouette Score \n",
    "* The Silhouette score in the K-means clustering algorithm is between -1 and 1.This score represents how well the data point has been clustered and scores above 0 are seen as good , while negative points your K-means algorithm has puth that data point in the wrong cluster.\n",
    "\n",
    "> How is Silhouette score Calculated?\n",
    "* Conceptually, the Silhouette score utilizes some distance parameter to measure how far a point is from its cluster compared to the Centroid of a different cluster.If this value is negative, this data point is considered closer to the centroid of the another cluster than the one assigned.\n",
    "> from sklearn.metrics import silhouette_score \n",
    "> silhouette_avg = silhouette_score(data,kmeans.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. What are the challenges associated with K means Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means Clustering is a popular algorithm,but it comes with its sets of challenges.Here are some of the main challenges associated with K-means clustering.\n",
    "1. Sensitivity to initial centroid selection\n",
    "* K- means is sensitive to the innitial placement of Centroids.Different initialization may lead to different final clusters.\n",
    "2. Dependence on the number of CLusters(K) \n",
    "* The algorithm requires the number of clusters(K) to be specified in advance.Choosing an inappropriate K can lead to suboptimal results.\n",
    "3. Assumption of spherical clusters\n",
    "* K-means assume that clusters are spherical and equally sized which might not be suitable for complex shaped clusters or clusters with varying sizes.\n",
    "4. Impact of outliers \n",
    "* Oultiers can significantly affect the centroids position , leading to the creation of clusters that donot accurately represent the underlying data distribution.\n",
    "5. Balancing Cluster sizes \n",
    "* K-means tends to produce cluster of approximately equal size.In scenarios where the clusters have vastly different sizes,the algorithm may not perform well.\n",
    "6. Sensitive to Feature Scaling \n",
    "* features with different scales can dispropertionately influence the distance calculations,potentially leading to biased cluster assignments.\n",
    "7. Lack of Robustness to noise \n",
    "* K-means is sensitive to noisy data and can be influenced by outliers , which may result in Suboptimal Cluster assignments.\n",
    "8. Hard Assignment of data point \n",
    "* K-means assigns each data point to a single cluster , even if the point is on the border between two clusters.This can lead to misiterpretation of data points near cluster boundaries.\n",
    "9. Non Convex Cluster boundaries \n",
    "* K-means assumes convex-shaped clusters.It may struggle to identify non convex or irregularly shaped clusters.\n",
    "10. Computational Complexity \n",
    "* The algorithm has a time complexity that depends on the number of iterations and the number of data points.For large datasets it can be computationally expensive.\n",
    "\n",
    "> IMP \n",
    "* Despite these challenges K-means can still be effective in certain situations ,especially when the assumption align with the characteristics of the data.Its important to be aware of these challenges and consider alternative clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Explain some cases where K means clustering fails to give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means clustering may fail to provide accurate and meaningful results in certain situations due to its assumptions and limitations .Here are some cases where k-means clustering may not give good results.\n",
    "1. Non GLobular clusters \n",
    "* K-means assume that cluster are spherical and eqaully sized.Whne the true clusters have non globular shapes or significantly different sizes , K-means may struggle to accurately capture the underlying patterns.\n",
    "2. Uneqaul cluster densities \n",
    "* If the cluster in the data have varying densities ,K-means may fail to represent the true structure. It tends to create clusters with similar sizes whcih may not reflect the actual distribution of the data.\n",
    "3. Variable cluster sizes \n",
    "* K-means tends to produce clusters of roughly equal sizes.In cases where the cluster have highly variable sizes,the algorithm may not perform very well and smaller clusters may be overshadowed by larger ones.\n",
    "4. Outliers \n",
    "* Outliers can significantly impact the position of Centroids,leading to inaccuarte cluster assignments.K-means is sensitive to outliers an their presence can distort the results.\n",
    "5. Sensitive to Initial Centroid positions \n",
    "* K-means is sensitive to the initial placement of Centroids. Different intializations may lead to different final clusters and the algorithm might converge to a local minimum instead of the global minmum.\n",
    "6. Irregular cluster shapes \n",
    "* When clusters have complex and non-convex shapes , K-means may fail to accuartely represent the structure.The algorithm assumes convex clusters and may struggle with clusters that have irregular boundaries.\n",
    "7. High Dimensional data \n",
    "* In high dimensional spaces where the number of features is much larger than the number of data points,the Eucledian distance used by K-Means may become less meaningful and the curse of dimensionality can affect the Clustering results.\n",
    "8. Categorical data and mixed data types \n",
    "* K-means is designed for numerical data and works based on distance measures.When dealing with Categorical or mixed data types the concept of distance becomes less straightforward and K-means may not provise meaningful results.\n",
    "9. Data withb noise \n",
    "* K-means is sensitive to noise and outliers. Noisy data can lead to Suboptimal cluster assignments,especially if the noise is not distinguished from the actual patterns \n",
    "* In these cases alternative clustering algorithms that can handle more complex structures and variations may be more suitable.\n",
    "* Its essential to assess the characteristics of the data and choose a clustering method that align with its properties and distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. What are the advantages and disadvantages of the K means Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advantages of Kmeans clustering algorithm\n",
    "1. Simplicity and Speed \n",
    "* Kmeans is computationally efficient and easy to implement.It is particularly well suited for large datasets and situations where simplicity is a priority.\n",
    "2. Scalability\n",
    "* Kmeans can handle a large number of data points and features making it scalable to large datasets.\n",
    "3. Applicability\n",
    "* It is versatile and can be applied to various types of data,making it suitable for a wide range of clustering tasks.\n",
    "4. Interpretability\n",
    "* The results of Kmeans clustering are relatively easy to understand especially when dealing with well separated datasets and spherical clusters.\n",
    "5. Ease of parameterization\n",
    "* requires specifying only the number of clusters (K) as a parameter making it straightforward for users to apply Kmeans clustering.\n",
    "\n",
    "> Disadvantages \n",
    "1. Assumption of Spherical clusters \n",
    "* K-means assume that clusters are spherical ,equally sized and have similar densities which may not hold true for all datasets.\n",
    "2. Sensitivity to initial Centroid Positions\n",
    "* The algorithms final results can be sensitive to the initial placement of centroids, potentially leading to different outcomes with different initializations \n",
    "3. Dependence on the number of clusters (K)\n",
    "* The User needs to specify the number of clusters (K) in advance,and choosing an inappropriate value can result in suboptimal clustering \n",
    "4. Difficulty with non convex shapes \n",
    "* K-means may struggle with identifying clusters with complex , non convex shapes.It tends to form convex shaped clusters.\n",
    "5. Handling Outliers \n",
    "* K-means is sensitive to outliers and their presence can significantly impact the final cluster assignments and centroid positions.\n",
    "6. Hard assignment of data points \n",
    "* It assigns each data point to single cluster which might be problematic for datasets with points that could belong to multiple clusters.\n",
    "7. Influence of feature scaling \n",
    "* The algorithm is sensitive to the scale of features and features with different scales can dispropertionality affect the clustering results.\n",
    "8. Limited to numerical data \n",
    "* K-means is designed for numerical data and its distance metric may not be suitable for categorical or mixed data types without appropriate preprocessing.\n",
    "9. Doesnot handle varying cluster sizes well \n",
    "* K-means tends to create cluster of similar sizes and it may not handle clusters with highly varaiable sizes appropriately.\n",
    "10. May converge to local minimum \n",
    "* Depending on the initial centroid positions , K-means may converge to a local minimum rather than global minimum affecting the quality of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. What are the applications of the K-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image Compression\n",
    "2. Customer Segmentation\n",
    "3. Anomaly Detection\n",
    "4. Document Clustering \n",
    "5. Genetic Clustering\n",
    "6. Network Security\n",
    "7. Recommendation Systems\n",
    "8. Stock Market Analysisq\n",
    "9. Speech Recognition\n",
    "10. Geographical data analysis\n",
    "11. Medical Imaging\n",
    "12. Quality Control\n",
    "13. Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
