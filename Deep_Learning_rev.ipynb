{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Objective of Deep Learning \n",
    "* To bulid an AI system capable enough to predict and to take decision on behalf of Human Being.\n",
    "\n",
    "> Disadvantage of Machine Learning\n",
    "* We are able to build a model on;y on structured data in machine learning.\n",
    "\n",
    "> Deep Learning is applicable to Unstructured Data.\n",
    "> Types of Unstructured Data\n",
    "1. Text > Recurrent Neural Network\n",
    "2. Image \n",
    "3. Audio\n",
    "4. Video \n",
    "\n",
    "> Text \n",
    "* Example if we have 200 to 300 Sentences and suppose we have 10,000 words each word will act as Individual Features and because of which our dimensionality of datasets gets increased (In Naive Bayes and SVM we can handle features upto 2000 and beyond that it will not work coorectly)\n",
    "\n",
    "> Image \n",
    "* In case of image (Image is created by multiple pixels)\n",
    "* Resolution > Count of pixels >> Examples >> HD , Ultra HD\n",
    "* Example of my Laptop 14 inch laptop >> Resolutions >> 1920 * 1080 >> 20,73,500 are the total number of pixels.\n",
    "* In Image each pixel acts as an Feature.\n",
    "* In Such Cases Machine Learning model will not work good for such amount of features and because of which we use Deep Learning.\n",
    "\n",
    "> Performance graph between ML and DL \n",
    "* DL can handle more complex data then ML and thats why DL came in use.\n",
    "* ML is suitable for simple data and DL can handle more complex data.\n",
    "* Complex data is that in the data we have curse of dimensionality(High Dimensional Data) and number of records is also high (Number of Rows)\n",
    "\n",
    "> Use of Unstructured Data\n",
    "1. Text data (NLP >> RNN ,LSTM(Big Brother of RNN)) >> In case of chatgpt we give text data input and the output we get from it is also in text format in its own language >> Language Translation\n",
    "2. Image data (CNN,ANN) >> AI image >> Lenskart(Choossing Frames options)\n",
    "3. Audio Data (Audio Analytics) >> Voice Cloning >> Audio data of customers can be used by industry to see if the customer is happy or not while talking to customer Service.\n",
    "4. Video Analytics >> Example >> We know our universe is continously expanding so to calculate the distance of the objects(Planets) NASA uses video analytics to calculate distance between the objects, >> In terms of Crash Test for safety ratings previously multiple crash tests were done to calculate the safety of the car by use of video analytics we can reduce the number of crashes and instead get notible insights from the crash test video taken one time.\n",
    "\n",
    "> CPU (Central Processing Unit)\n",
    "* General purpose processor for a wide range of tasks.\n",
    "* Few powerful cores optimized for sequential processing.\n",
    "* Suited for complex decision making and control flow.\n",
    "\n",
    "> GPU (Graphics Processing Unit)\n",
    "* Originally designed for graphics rendering and parallel processing.\n",
    "* Many small Specialized cores for parallel tasks.\n",
    "* Suited for parallelizable tasks like graphics,machine learning and simulations.\n",
    "\n",
    "> Cardinality \n",
    "* If we have Categorical data in target column we need to do label encoding.\n",
    "* Label encoding is done when we have order in our Categorical data.In Case of label encoding if we have more values, then the model can give higher weightage for maximum values(If we have more than 3 to 4 Categorical data)\n",
    "* In One hot encoding we will get more features anf there will be curse of Dimensionality which is not Good.\n",
    "* Having many categorical values in our features is called as Cardinality.\n",
    "* In such cases we use only one technique by deciding the weight of each category and the error associated with that category in that feature , in this way we select only category which has high presence and low error.\n",
    "* Cardinality affects vertically in the dataset (In terms of rows)\n",
    "\n",
    "> Curse of Dimensionality\n",
    "* Higher number of Indepndent features causes Curse of Dimensionality.\n",
    "* It affects data Vertically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Objective Of Deep learning \n",
    "* To build an AI system capable enough to predict and make decision on behalf of Human being.\n",
    "\n",
    "> Disadvantage of MAchine Learning\n",
    "* We are able to build Efficient model on UNstructured data.\n",
    "\n",
    "> Deep learning is mostly applicable for Unstructured data\n",
    "> TYpes of Unstructured data\n",
    "1. Text > Recurrent Neural Network\n",
    "2. Images\n",
    "3. Audio \n",
    "4. Video\n",
    "\n",
    "> Text \n",
    "* Example we have 200 to 300 sentences and suppose we have 10,000 words in them each word will act as independent feature and because of which our dimensionality of dataset gets increased(In Naive Bayes and SVM we can handle features upto 2000 beyond that it will not work perfectly)\n",
    "\n",
    "> Image \n",
    "* In Case of Image (Image is created by multiple pixels)\n",
    "* Resolution > Count of pixels >> Examples >> HD,Ultra HD\n",
    "* 14 inch Laptop >> Resolution >> 1920 * 1080 >> 20,73,500 Pixels on my Laptop.\n",
    "* In Image Data each pixel act as independent feature.\n",
    "* In such cases Machine Learning model will not work perfectly for such amount of features and because of which we use Deep Learning.\n",
    "\n",
    "> Performance graph between ML and DL \n",
    "* DL can handle more complex data then ML , and so DL came in Use.\n",
    "* ML is suitable for simple data and DL is suitable for complex data.\n",
    "* Complex data is that data which have curse of dimensionality(No of columns is geater than no of rows) and also the number of records is also high(Numbr of rows is also High)\n",
    "\n",
    "> Use of Unstructured data \n",
    "1. Text data (NLP > RNN , LSTM(Big Brother of RNN)) >> In case of chatgpt we give text data input and the output we get from it is also in text format in its own language >> Language Translation \n",
    "2. Image Data (CNN , ANN) >> AI image ,>> Lenskart (CHoosing Frames option)\n",
    "3. Audio Data (Audio analytics) >> Voice cloning , Audio data of customers can be used by Industry to see if the customer is happy or not while talking to Customer Service.\n",
    "4. Video Analytics >> We know our universe is continuously expanding so to the calculate the distance of the objects captured NASA uses Video analytics to calculate distance between objects >> In terms of crash test for safety ratings previously multiple crash test were done to calculate the safety of the car use by use of video analytics we can reduce the number of crashes and instead study the safety of the car by a single video.\n",
    "* In Video analytics we convert video into image.\n",
    "\n",
    "> CPU (Central Processing Unit)\n",
    "* General purpose processor for a wide range of tasks.\n",
    "* Few powerful cores optimized for sequential processing.\n",
    "* Suited for complex decision making and control flow.\n",
    "\n",
    "> GPU (Graphics Processing Unit)\n",
    "* Originally designed for graphics rendering and parallel processing.\n",
    "* Many small, Specialized cores for parallel tasks.\n",
    "* Suited for parallelizable tasks like graphics , machine learning  and simulations.\n",
    "\n",
    "> Cardinality \n",
    "* If we have categorical data in target column we need to do label encoding.\n",
    "* Label encoding is done when we have order in our Categorical data . In case of label encoding if we have more values than the model wil give higher weightage for maximum values( we have 3 to 4 categorical data)\n",
    "* In one hot encoding we will get more features and there will be curse of Dimensionality which is not Good.\n",
    "* Having many categorical values in our features is called as Cardinality.\n",
    "* In Such Cases we use only one technique by deciding the weight of the each category and the error associated with that category in the total feature , in this way we select only category which has high presence and low error.\n",
    "* Cardinality affects vertically in the dataset ( In terms of rows)\n",
    "\n",
    "> Curse of Dimensionality\n",
    "* Higher number of Independent features causes Curse of Dimensionality\n",
    "* It affects the data Horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perceptron > Before deep learning we used this ALgorithm.\n",
    "* It is a 2 stage algorithm\n",
    "* It is a part of Machine Learning\n",
    "* Weight,Bias >> External parameters\n",
    "* Suppose we have X1,X2,X3 and X4 as input parameters and we have W1,W2,W3,W4 as weights for each parameter.\n",
    "* And we get Yp as Output Parameter.\n",
    "\n",
    "> Stage 1 (Summation Function) (Z)\n",
    "* As it is a 2 stage algorithm in first stage we will perform Summation (Z)\n",
    "* Z = Summation (Wi * Xi + b) (b = Bias)\n",
    "\n",
    "> Stage 2 (Activation Function) Sigmoid \n",
    "* Sigmoid(Z) = 1 / (1 + e^-Z)\n",
    "* Y predicted will be the output of Sigmoid Function\n",
    "* This whole process is known as perceptron and Perceptron itself act as a Neuron in Neural Network.\n",
    "\n",
    "* Perceptron was used for creating Neural Network and that perceptron is called as Neuron.\n",
    "* Neural Network is made up of n number of different Algorithms.\n",
    "* Hence Neural Network is called as Architecture and not a Algorithm.(ANN , CNN)\n",
    "\n",
    "> What is Sigmoid ?\n",
    "* It is used to add Non Linearity\n",
    "* Sigmoid converts continous values into probablity distribution(Range 0 to 1)\n",
    "* It will always gives probablistic value.\n",
    "* It will give probablity value of any one class.\n",
    "\n",
    "> Structure of Neural Network?\n",
    "* Network >> When there is a connection between Source and link.\n",
    "* When there is a connection between neuron1 and neuron2 then there is neural network.\n",
    "* When Neuron1 and Neuron2 are connected then there is neural network.For connection between the neurons some weight is assigned to this connection.\n",
    "* Bias in this term is different from Machine Learning.\n",
    "* We will apply Bias to the Head of the Neurons.\n",
    "* Input Layer > It holds the input and feed them to hidden layer.\n",
    "* There is only one input layer \n",
    "* Hidden layer > Done all the processing / Learning\n",
    "* Hidden layer can be one or more than one.\n",
    "* If there is only one hidden layer then it is called as Shalow Neural Network.\n",
    "* If there is more than one hidden layer then it is called as Deep Neural Network.\n",
    "* We have our learning only in our Hidden layer.\n",
    "* Output layer > It shows Output\n",
    "* Output layer is always one layer.\n",
    "\n",
    "> How data flow in Neural Network?\n",
    "* Number of Neurons in Input Layer = Numbers of input features\n",
    "* Number of Neurons in Output Layer = Decided by the activation function and number of event.\n",
    "\n",
    "> Case 1 Lets say we are dealing with binary classification and in our output layer we have signoid function as activation function?\n",
    "* How many neurons will be there in the output layer?\n",
    "> 1\n",
    "\n",
    "> Case 2  Lets say we are dealing with Multiclass classification and in out output layer we have softmax function as activation function (Softmax activation is used for Multiclass classification and it is the modified version of Sigmoid activation) and number of events is 5.\n",
    "* How many neurons will be there in the output layer?\n",
    "> 5\n",
    "\n",
    "> Case 3 Lets say we are dealing with regression problems and in our output layer we have relu as a activation function (Relu activation is used for regression)\n",
    "* How many neurons will be there in the output layer?\n",
    "> 1\n",
    "\n",
    "* We have 7 to 8 activation functions \n",
    "* Number of Neurons in the output will depend on the activation function.\n",
    "\n",
    "> Number of Neurons in the Hidden Layer?\n",
    "* Not fixed \n",
    "\n",
    "> General Thumb rule (Manually building ANN)\n",
    "* If we have 15 neurons as input layer and we are getting 3 Neurons in the output layer so naturally the neurons in the hidden layer will be in the range(3 to 15)\n",
    "* If we increase the number of neurons (Neuron itself is a Algorithm) our complexity of the architecture(Neural Network) will increase and our computational cost and time will increase and more resources will be used.\n",
    "\n",
    "> Shape of the Layers \n",
    "* If we have 3 Neurons in 1 Container as our input layer then we say that the shape of the input layer is (1,3).\n",
    "* Likewise we have shape of the hidden layer 1 as (1,4) and shape of the hidden layer 2 as (1,2) and shape of the output layer is (1,1).\n",
    "\n",
    "> How to control shape of the layers is Neural Network?\n",
    "* Lets Consider W1 (Weight matrix) and we start from 3 neurons and we end with 4 neurons.\n",
    "* We will get 12 connections and we will get weight for each connection that is we will get 12 weights.\n",
    "* So the shape of W1 will be (4,3)\n",
    "* Z = Summation (Wi * Xi + b)\n",
    "* Z = Summation (Wi^T * Xi + b) >> T = Transpose\n",
    "* Weight decide the shape of the Layers \n",
    "\n",
    "> Importance of BIAS ?\n",
    "* Bias is a 1 Dimensional array that consists value of Bias.\n",
    "> Z = Summation (Wi * Xi + b)\n",
    "* Even if we have 0 as the value of Independent variable while calculating Z we should not get the ouput of that variable as Zero we add Bias . Or we can also say that to add importance to that feature we add Bias.\n",
    "\n",
    "> How Neural Network Learns?\n",
    "* We folow a chain rule for selecting optimal weight and bias which are external parameters.\n",
    "* Same steps we followed in Gradient Descent in Linear regression for finding optimal m and c values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perceptron > before deep learning we used this algorithm.\n",
    "* It is a 2 stage algorithm\n",
    "* It is a part of Machine Learning.\n",
    "* Weight,bias >> External parameters\n",
    "* Suppose we have X1,X2,X3,X4 as input parameters and we have W1,W2,W3,W4 as weights for each parameters.\n",
    "* And we get Yp as Output Parameter.\n",
    "\n",
    "> Stage 1 (Summation Fucntion) Z\n",
    "* As it is a 2 stage algorithm in first stage we will perform summation (Z)\n",
    "* Z = Summation(Wi * Xi + b)  (b = bias)\n",
    "\n",
    "> Stage 2 (Activation Fucntion) Sigmoid\n",
    "* Z = 1 / (1 + e^-Z)\n",
    "* Y predicted will be the output of the Sigmoid function.\n",
    "* This whole process is known as Perceptron and Perceptron itself acts as Neuron in Neural Network.\n",
    "\n",
    "* Perceptron was used for creating Neural Network and that perceptron is called as Neuron.\n",
    "* Neural Network is made up of n numbers of different Algorithms.\n",
    "* Hence Neural Network is called as an Architecture and not a Algorithm.\n",
    "\n",
    "> What is Sigmoid?\n",
    "* It is used to add Non linearity.\n",
    "* Sigmoid converts continous values into probablity distribution. (Range 0 to 1)\n",
    "* It always gives probablistic values.\n",
    "* It always gives probablity value of any one class.\n",
    "\n",
    "> Structure of Neural Network\n",
    "* Network >> When there is connection between Source and link.\n",
    "* When there is a connection between Neuron1 and Neuron2 then there is Neural Network.\n",
    "* When Neuron1 and Neuron2 are connected then there is Neural Network.For connection between the Neurons some weight is assigned to this connection.\n",
    "* Bias in this term is different from Machine Learning.\n",
    "* We will apply Bias to the Head of Neurons.\n",
    "* Input Layer > It hold the input and feed them to hidden layer.\n",
    "* There is only one input layer.\n",
    "* Hidden layer >> Done all the processing / Learning\n",
    "* Hidden layer can be more than one.\n",
    "* If there is only one hidden layer then it is called as Shalow Neural Network.\n",
    "* If there is more than one hidden layer then it is called as Deep Neural Network.\n",
    "* We have our Learning only in our hidden layer.\n",
    "* Output layer >> It shows Output\n",
    "* Output layer is always one layer.\n",
    "\n",
    "> How data flow in Neural Networks?\n",
    "* Number of neuron in Input Layer = Number of input features\n",
    "* Number of neurons in Output Layer = Decided by the activation function and number of event.\n",
    "\n",
    "> Case 1 Lets say we are dealing with binary classification and in our output layer we have Sigmoid activation function.\n",
    "* How many neurons will be there in the output layer?\n",
    "> 1\n",
    "\n",
    "> Case 2 Lets say we are dealing with Multiclass classification and in our output layer we have softmax activation function (SoftMax activation is used for Multiclass classification and is the modified version of Sigmoid activation) and number of event is 5\n",
    "* How many neurons will be there in the output layer?\n",
    "> 5\n",
    "\n",
    "> Case 3 Lets say we that we are dealing with regression problems and in our output layer we have Relu as a activation function (Relu activation is used for regression)\n",
    "* How many neurons will be there in the output layer?\n",
    "> 1 (Continous Values)\n",
    "\n",
    "* We have 7 to 8 activation functions.\n",
    "* Number of neurons in the output layer will depend on the activation function.\n",
    "\n",
    "> Number of Neurons in the hidden layer?\n",
    "* Not Fixed\n",
    "\n",
    "> General Thumb Rule (Manually building ANN)\n",
    "* If we have 15 neurons as input layer and we are getting 3 neurons in the output layer so naturally the neurons in the hidden layer will be in the range(3 to 15)\n",
    "* If we increase the number of neurons (Neuron is itself a Algorithm) our complexity of the architecture (Neural Network) will increase and our Computation cost and time will increase and more resources will be used.\n",
    "\n",
    "> Shape of the Layers \n",
    "* If we have 3 Neurons in 1 Container as our input layer then we say the shape of the Input Layer is = (1,3).\n",
    "* Likewise we have shape of the hidden layer 1 as (1,4) and shape of the Hidden Layer 2 as (1,2) and shape of the output layer is (1,1).\n",
    "\n",
    "> How to control shape of the Layers in Neural Network?\n",
    "* Lets consider W1 (weight Matrix) and we start from 3 Neurons and we end with 4 Neurons.\n",
    "* We will get 12 connections and we will get weight for each connection that is we will get 12 weights.\n",
    "* So the shape of W1 will be (4,3)\n",
    "* Z = Summation (Wi * Xi + b)\n",
    "* Z = Summation (Wi^T * Xi + b) >> T = Transpose\n",
    "* weight decide the shape of the layers.\n",
    "\n",
    "> Importance of Bias \n",
    "* Bias is a 1 dimensional array that consists values of Bias.\n",
    "> Z = Summation(Wi * Xi + b) \n",
    "* Even if we have 0 as the value of Independent variable while calculating Z we should not get the output of that variable as zero we add Bias.Or we can also say that to add importance to that feature we add Bias.\n",
    "\n",
    "> How Neural Network learns?\n",
    "* We follow a chain rule for selecting optimal weight and Bias which are external parameters.\n",
    "* Same steps we follow in Gradient Descent in Linear regression for finding optimal m and c values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparison of Linear regression and ANN\n",
    "* Formula for Y in Linear regression is Y = MX + C and formula for Z in ANN is Z = Summation (Wi * Xi + b)\n",
    "* We find the best values of M and C in Linear regression and in ANN we find the best values of W and b.\n",
    "* We use Gradient Descent algorithm for both Linear regression and ANN.\n",
    "* In Linear regression Mnew = Mold - alpha * (Daeba[L] / (Daeba[M]))\n",
    "                       Cnew = Cold - alpha * (Daeba[L] / (Daeba[C]))\n",
    "* In ANN               Wnew = Wold - alpha * (Daeba[L] / (Daeba[W]))\n",
    "                       b[new] = b[old] - alpha * (Daeba[L] / (Daeba[b]))\n",
    "* \n",
    "* In above formula for ANN we saw (Daeba[L] / (Daeba[W])) these are gradient.\n",
    "* Gradient tells us the change in loss with respect to weight.\n",
    "* In ANN we calculate multiple loss  to find the minimum loss amongst all losses.\n",
    "* By finding the minimum loss we will find the best values of W and b.\n",
    "* By finding the best values of W and b we will find the best summation function.\n",
    "* Then we will select the activation function.\n",
    "* Finally we get the Y predicted.\n",
    "* The above process can also be called as Learning.\n",
    "* \n",
    "* When we follow the process from input layer to hidden layers and then to output layer this process is called as Forward propagation.\n",
    "* When we do the above process again and again to find the calculate the best values of W and b and this process os called as Backward propagation.\n",
    "* \n",
    "> Case 1 \n",
    "* Lets say we have 1500 data points.\n",
    "* If we are applying forward and backward propagation on these 1500 data points then we will perform 1 Epoch.\n",
    "* We can control the number of Epoch we control the complexity of the Architecture.\n",
    "* \n",
    "> Case 2 \n",
    "* Suppose we divide the 1500 data points in 3 batches of 500.\n",
    "* And we apply Forward and Backward propagation on each batch.\n",
    "* Each forward and backward propagation on a batch is called as Iteration.\n",
    "* And combination of 3 Iterations creates 1 Epoch.\n",
    "* We are performing the above process in 3 batches Simultaneously.\n",
    "* \n",
    "> In case 1 our resource consumption will be high but in Case2 we are dividing the data and then performing Iterations this process in Case 2 is called as Parallel and distribution learning.\n",
    "* In parallel and distribution learning we are working simultaneously on 3 batches(Case 2) so it is called as parallel learning and we also divide the data in 3 Batches that is why it is called as Distribution learning and combining these options is called as Parallel and distribution learning.\n",
    "* In parallel and distribution learning our Burden of calculation will be reduced in short our time complexity will be reduced and also the consumption of resources will be reduced.\n",
    "> Example of Parallel and Distribution Learning is picking a sand of bag by one hand and picking the same sand bag by both hands. Our efforts will be reduced when we pick the sand bag by both hands.Efforts in this case can be related by the resources used while building the architecture.\n",
    "* \n",
    "> Simple example of Neural Network works\n",
    "* Suppose when we consider about trying to do savings , we will consider different types of expenses like shopping expenses , Food expenses , Travel expenses ,Entertainment expenses and so on.Now i will try to give weightage on each type of expense while spending. Like my weightage for Entertainment expenses will be high as compared to food expenses.Then i will deicide about my weightage of each type of expense at every month ending for the next month, this is called as Learning. Weightage is important while considering learning.\n",
    "* \n",
    "> Types of functions used in Neural Network\n",
    "* \n",
    "1. Optimization function (Same as Gradient Descent algorithm)\n",
    "* To minmize or optimize the error.\n",
    "* It is used to find the best values for Weight and bias.\n",
    "* It helps to achieve Convergence (To obtain Global minima)\n",
    "* \n",
    "2. Activation function \n",
    "* Governing the way of Neuron Behave.\n",
    "* It controls the output of any Neuron.\n",
    "3. Loss function\n",
    "* It gives the number which represents the goodness of Model.\n",
    "* It helps to update the Neural Network parameters.\n",
    "* \n",
    "> 1.Optimization function \n",
    "* Optimization of Loss function.\n",
    "* We can either minimize the value of the loss function or we can maximize the value of gain function.\n",
    "* Example of Loss function >> Error \n",
    "* Example of gain function >> Accuracy \n",
    "* We only use the loss function in Neural Network.\n",
    "* \n",
    "> Types of Optimization functions \n",
    "* \n",
    "* In first type of optimization function we dynamically update Weight and bias.\n",
    "* Dynamically update >> Continously update \n",
    "* \n",
    "* Inn Second type of optimization function we dynamically update Learning Rate.\n",
    "* \n",
    "* In Third type of optimization function we dynamically update Weight , bias and learning rate.\n",
    "* \n",
    "> 1. Gradient descent Algorithm (In ANN we call it as Batch Gradient Descent (Batch >> complete data) algorithm)(BGDA)\n",
    "* In BGDA epoch get perform on whole data no matter how big our dataset.\n",
    "> Lets say we have 1000 data points and we apply BGDA on that data and we achieve Convergence in 1 minute and memory consumption will be 1GB.\n",
    "* For 1,00,000 data points we will require 100 minutes and 100GB consumption which is not Good.\n",
    "* \n",
    "> Drawbacks of BGDA \n",
    "* We cannot use BGDA in case of Big data because Epoch get performed on whole data.\n",
    "* \n",
    "> How BGDA works in Backend?\n",
    "* Suppose we have 1000 data points in dataset having 1 independent and 1 dependent variable.\n",
    "* And we perform BGDA on above datset.\n",
    "* We will get Y predicted.\n",
    "* Now we have Y predicted and Y actual so we can calculate Loss(Error)\n",
    "* So now we can calculate Wnew and b[new]\n",
    "> Wnew = Wold - alpha * (Daeba[L] / Daeba[W])\n",
    "> b[new] = b[old] + alpha * (Daeba[L] / Daeba[b])\n",
    "* \n",
    "> Advantages of BGDA \n",
    "* We get smooth learning curve(Parabola in GDA) because we will perform Epoch on whole data.\n",
    "* \n",
    "> In case of Gradient Descent our learning step reduces as we reach towards our gobal minima.\n",
    "* Because we have learned much from our previous learning step and we will not move aggressively towards our global minima as we did in our previous learning steps.\n",
    "* \n",
    "2. Stochastic Gradient Descent algorithm(SDGA)\n",
    "* Stochastic > Random or Randomness \n",
    "* Suppose we have 1000 data points and we will select one data point randomly from the data set.\n",
    "* We calculate Y predicted and loss using Forward propagation(FP1) and backward propagation(BP1).\n",
    "* We can also call it as a Iteration.\n",
    "* \n",
    "* In Stochastic Gradient Descent algorithm we consider a single data point at a time randomly.\n",
    "* In SDGA 1 Epoch = N number of Iterations (N = Number of data points)\n",
    "* In SDGA we learn from single data point at a time hence learning curve becomes very noisy.\n",
    "* \n",
    "> Example \n",
    "* Suppose we have [2,4,6,8,10,12,14,16,18,20] as data points.\n",
    "* 1st Iteration > 6    >  Yp1 ,L1    > W` , b`\n",
    "* 2nd Iteration > 12   >  Yp2 ,L2    > W`` , b``\n",
    "* 3rd Iteration > 18   >  Yp3 ,L3    > W```, b```\n",
    "* We get our learning curve noisy that we will have multiple local minima.\n",
    "* In SDGA we can get stuck at Local minima instead of Global minima.\n",
    "* In SDGA sice we are learning from single data point at a time we dont have sufficient amount of learning to overcome local minima as a result after some time SDGA will consider local minima as Global minima and which is wrong to consider.\n",
    "> Advantage of SDGA \n",
    "* Compare to BDGA ,SDGA requires less resources to achieve Convergence.\n",
    "> Disadvantage of SDGA \n",
    "* It is very prone to Local Minima because in SDGA we learn from single data point at a time hence we dont have sufficient amount of learning to overcome local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparison of Linear regression and ANN \n",
    "* Formula for Y in Linear regression is Y = MX + C and formula for Z in ANN is Z = Summation(Wi * Xi + b)\n",
    "* In linear regression we find the best values of M and C and in ANN we find the best values of W and b.\n",
    "* We use Gradient Descent algorithm for both linear regression and ANN.\n",
    "* In linear regression > Mnew = Mold - alpha * (Daeba[L] / Daeba[M])\n",
    "                       > Cnew = Cold - alpha * (Daeba[L] / Daeba[C])\n",
    "* In ANN               > Wnew = Wold - alpha * (Daeba[L] / Daeba[W])\n",
    "                       > b[new] = b[old] - alpha * (Daeba[L] / Daeba[b])\n",
    "* In Above formula for ANN we saw (Daeba[L] / Daeba[W]) these are Gradient.\n",
    "* In above Gradient it tells us change in loss with respect to weight.\n",
    "* In ANN we calculate multiple losses and amongst all losses we select the minimum one.\n",
    "* By finding the minumum loss we will get the best values of W and b.\n",
    "* By finding the best values of W and b we will get the best Summation function.\n",
    "* Then we will select the activation function.\n",
    "* Finally we will get the Y predicted.\n",
    "* The above process can also be called as Learning.\n",
    "* \n",
    "> Start of Lec \n",
    "* When we follow the process through the input layer then to hidden layer then to output layer this process is called as Forward Propogation.\n",
    "* When we go from hidden layer 2 to hidden layer 1 to calculate the best values of M and b then this process is called as Backward Propogation.\n",
    "* \n",
    "> Forward Propogation \n",
    "* Work of the Forward Propogation is to calculate the Y predicted and Loss.\n",
    "* \n",
    "> Backward Propogation \n",
    "* Work of the Backward Propogation is to update the values of W and b.\n",
    "* \n",
    "> Epoch / Iteration\n",
    "> Case 1 \n",
    "* Lets say we have 1500 data points\n",
    "* If we are applying forward propagation and backward propagation on these 1500 data points then we will perform 1 Epoch.\n",
    "* We can control the number of Epoch to control the complexity of the Architecture.\n",
    "* \n",
    "> Case 2\n",
    "* Suppose we divide the 1500 data points in 3 batches of 500.\n",
    "* And we apply forward propagation and backward propagation on each batch simultaneously.\n",
    "* Each Forward Propagation and Backward Propagation on a batch is called as Iteration.\n",
    "* And combination of 3 Iterations creates 1 Epoch.\n",
    "* We are performing the above process in 3 batches simultaneously.\n",
    "* \n",
    "> In Case 1 our resource consumption will be high but in Case 2 we are dividing the data and then performing iterations this process in Case 2 is called as Parallel and Distribution Learning.\n",
    "* In Parallel and Distribution Learning we are working simultaneously on 3 batches(Case 2) so it is called as Parallel learning and also we are dividing the data in 3 batches so it is also called as Distributed Learning.Combination of Parallel Learning and Distribution learning is called as Parallel and Distribution Learning.\n",
    "* In Parallel and Distribution Learning our burden of calculation will be reduced in short our time complexity will be reduced and also the consumption of resources will be reduced.\n",
    "> Example of Parallel and Distribution Learning is picking a sand of bag by one hand and picking the same sand bag by both hands.Our efforts will be reduced when we the pick the same sand bag by both hands.Efforts in this case can be related by the resources used while building the Architecture.\n",
    "* \n",
    "> Simple Example of How Neural Network works?\n",
    "* Suppose when we consider about trying to do savings , we will consider different types of expenses like shopping expenses,Food expenses ,Travel expenses,Entertainment expenses and so on.Now i will try to give weightage on each type of expenses while spending.Like my weightage for Entertainment expenses will be high as compared to food expenses.Then i will decide about my weightage of each type of expenses at every month ending for the next month, this is called as Learning.Weightage is important while considering Learning.\n",
    "* \n",
    "> Types of Functions used in Neural Network?\n",
    "* \n",
    "1. Optimization function (same as Gradient descent algorithm)\n",
    "* To minimize or optimize the error .\n",
    "* It is used to find the best values for weight and bias.\n",
    "* It helps achieve Convergence (To obtain Global Minima)\n",
    "* \n",
    "2. Activation function \n",
    "* Governing the way of Neuron Behave.\n",
    "* It controls the output of any Neuron.\n",
    "* \n",
    "3. Loss function\n",
    "* It gives a number which represents the goodness of BFL.\n",
    "* It helps to update the Neural Network parameters.\n",
    "* \n",
    "> 1. Optimization function \n",
    "* Optimization of loss function.\n",
    "* We can either minimze the loss function or we can maximize the value of the gain function.\n",
    "* Example of loss function >> Error \n",
    "* Example of gain function >> Accuracy\n",
    "* We only use the loss function in Neural Network.\n",
    "* \n",
    "> Types of Optimization functions\n",
    "* \n",
    "* In first type of Optimization function we dynamically update the weight and bias.\n",
    "* Dynamically update > Continuously update\n",
    "* \n",
    "* In second type of Optimization function we dynamically update the learning rate.\n",
    "* \n",
    "* In third type of Optimization function we dynamically update the learning rate,weight and bias.\n",
    "* \n",
    "> 1. Gradient descent algorithm (In ANN we call it as Batch Gradient Descent (Batch > Complete data) algorithm)(BGDA)\n",
    "* In BGDA epoch get perform on whole data no matter how big our dataset is.\n",
    "> Lets say we have 1000 data points and we apply BGDA on that data and we achieve Convergence in 1 minute and memory consumption will be 1GB.\n",
    "* For 1,00,000 data points we will require 100 minutes and 100GB of memory consumption which is not Good.\n",
    "* \n",
    "> Drawbacks of BGDA \n",
    "* We cannot use BGDA in case of Big data because Epoch get performed on whole dataset.\n",
    "* \n",
    "> How BGDA works in Backend?\n",
    "* Suppose we ahve 1000 data points in the dataset having 1 independent and 1 dependent variable.\n",
    "* And we perform BGDA on above dataset.\n",
    "* We will get Y predicted.\n",
    "* Now we have Y predicted and Y actual so we can calculate loss(Error)\n",
    "* So now we can calculate Wnew and bnew\n",
    "> Wnew = Wold - alpha * (Daeba[L] / Daeba[w])\n",
    "> bnew = bold - alpha * (Daeba[L] / Daeba[b])\n",
    "* \n",
    "> Advantage of BGDA\n",
    "* We get smooth learning curve(Parabola in GDA) in BGDA because we perform Epoch on whole dataset.\n",
    "* \n",
    "> In case of Gradient descent our learning step reduces as we reach towards our GLobal minima.\n",
    "* Because we have learned much from our previous learning step and we will not move aggressively towards the GLobal minima as we did in our previous learning steps.\n",
    "* \n",
    "> 2. Stochastic Gradient Descent algorithm(SDGA)\n",
    "* Stochastic > Random or Randomness \n",
    "* Suppose we have 1000 data points and we will select 1 data point from the data set.\n",
    "* We calculate the Y predicted and loss using Forward propagation(FP1) and backward propagation(BP1).\n",
    "* We can also call it as Iteration.\n",
    "* \n",
    "* In Stochastic Gradient Descent algorithm we consider single data point at a time randomly.\n",
    "* In SGDA 1 Epoch >> N number of Iteration (N = Number of data points)\n",
    "* In SGDA we learn from single data point at a time so our learning curve becomes very noisy.\n",
    "> Example \n",
    "* Suppose we have [2,4,6,8,10,12,14,16,18,20] as data points.\n",
    "* 1st Iteration > 6   > Yp1 , L1    > W` , b`\n",
    "* 2nd Iteration > 12  > Yp2 , L2    > W`` , b``\n",
    "* 3rd Iteration > 18  > Yp3 , L3    > W`` , b``\n",
    "* We will get our learning curve noisy that is will have multiple local minima.\n",
    "* In SDGA we can get stuck at Local Minima instead of Global minima.\n",
    "* In SDGA since we are learning from single data point at a time we dont have sufficient amount of learning to overcome the local minima point as a result after some time SDGA consider Local minima as a Global minima and that is wrong.\n",
    "> ADvantages of SDGA \n",
    "* Compare to BGDA ,SDGA requires less resources to achieve convergence.\n",
    "> Disadvantages of SDGA \n",
    "* It is very prone to local minima because in SDGA we learn from single data point at a time hence we donot have sufficient amount of Learning to overcome Local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Mini Batch Stochastic Gradient Descent Algorithm (MBSGDA)\n",
    "* We are creating mini batch randomly and applying Gradient Descent Algorithm on it.\n",
    "* MBSGDA is a combination of BGDA and SGDA.\n",
    "* In BGDA we are getting smoothness of the learning curve but resources and time were used more. In SGDA we were doing step by step learning our resources and time were used less but we get noise in our data.\n",
    "* Suppose we have a Batch(Complete data) and we are creating mini batches MB1,MB2,MB3 and MB4 by random sampling.\n",
    "* We will apply Forward propagation and backward propagation on each mini batch simultaneously.\n",
    "* In Forward propagation we will get the Y predicted and we will calculate the Loss.\n",
    "* For MB1 we will get Ypred1 and Loss L1 by Forward propagation and we will apply backward propagation after that to get the updated valyues of Weight and bias.\n",
    "* Same will apply for all mini batches.\n",
    "* We will calculate Loss of all mini batches and minmum of all mini batches will be selected(Min Loss).\n",
    "* We will get the best values of Weight and bias.\n",
    "* We will calculate the best Summation(Z) function.\n",
    "* Then we will apply the Activation function.\n",
    "* And after that we will get final Ypred.\n",
    "* We are performing Parallel and Distribution learning im MBSGDA.\n",
    "* Still we will get noise in our learning curve after performing above operations.\n",
    "* But our noise will be less compared to the SGDA (Noice >> No Smoothness in the curve)\n",
    "* In MBSGDA we will learn from each mini batch at a time hence we now have sufficient amount of learning to overcome the local minima point,We ensure that we will reach the Global minima.\n",
    "* \n",
    "> Advantages of MBSGDA\n",
    "* Compared to BGDA, MBSGDA requires less resources and time to achieve Convergence is less.\n",
    "* Since we are learning from mini batches we have sufficient amount of learning to overcome the local minima.\n",
    "* Hence we will reach the Global minima.\n",
    "* \n",
    "> Disadvantages of MBSGDA \n",
    "* Noise is still present in the learning curve.\n",
    "* \n",
    "> Why we dont want noise in our learning curve?\n",
    "* Example \n",
    "* Suppose we have 3 floor building and we want to hoist the flag on the top the building for Independence Day.\n",
    "* And we are given Straight Ladder to climb on the building.\n",
    "* We will require less time but more effort.\n",
    "* And we also have 2nd building with 3 floors but with Zig Zag Ladder.\n",
    "* We will require more time and less effort to climb on the building as we have Zig Zag Ladder and we will require more time to climb on the building.\n",
    "* Now we also have 3rd building with 3 floors but with Spiral Ladder.\n",
    "* Because of the spiral ladder we will require less time and less effort to climb on the building.\n",
    "* \n",
    "* Same applies for Noise, if we remove noise we will require less time and less resources to achieve the Convergence.\n",
    "* By reducing the noise we can remove the disadvantages of MBSGDA.\n",
    "* \n",
    "> 4. Mini Batch Stochastic Gradient Descent Algorithm with Momentum.\n",
    "* Momentum >> Smoothning of the Learning Curve (Removing noise)\n",
    "> Working \n",
    "* Formula \n",
    "> Wnew = Wold - alpha * (Daeba[L] / Daeba[W])\n",
    "> bnew = bold - alpha * (Daeba[L] / Daeba[b])\n",
    "* We will apply Momentum to our Gradients (Daeba[L] / Daeba[W]) and (Daeba[L] / Daeba[b])\n",
    "* Gradient >> Potential energy \n",
    "* At the starting point of the learning curve we will have maximum Gradients (Change in Loss with respect to weight) as to thr Gradients which are closer towards the Global minima.\n",
    "* Updated equations \n",
    "> Wnew = Wold - alpha * Vdw\n",
    "> bnew = bold - alpha * Vdb\n",
    "* Vdw , Vdb are the velocity components \n",
    "> Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "> Vdbt = Beta * Vdb t-1 + (1 - Beta) (Daeba[L] / Daeba[b])\n",
    "* Where t = no of hidden layers\n",
    "* Beta >> Smoothening parameter\n",
    "* Vdw , Vdb are the velocity components\n",
    "> Case 1 Beta = 0\n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = (Daeba[L] / Daeba[W])    >>  No Smoothning\n",
    "> Case 2 Beta = 1\n",
    "* Vdwt = Beta * Vdwt t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = Vdw t-1                  >>  Full Smoothning\n",
    "> Case 3 Beta = 0.98\n",
    "* Vdwt = Beta * Vdwt t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* In above formula (Beta * Vdw t-1) represents magnitude and (1 - Beta) (Daeba[W] / Daeba[W]) represents direction\n",
    "* \n",
    "> Above Revision \n",
    "1. By Batch Gradient Descent algorithm we were getting a smooth learning curve with no noise but the problem we are facing that the Gradient descent algorithm took more time and more resources which is Bad.So people working on it thought we should create another algorithm.\n",
    "2. Then came the Stochastic Gradient Descent algorithm here we are selecting an random data point and performing the forward propagation and backward propagation on a single data point.In stochastic gradient descent we had reduced the reduced the time complexity and also the reduced the usage of resources , but the problem we are facing is that we are facing as we had no previous learning experience we are not able to overcome the local minima and so our local minima will be considered as the global minima.\n",
    "3. To overcome this problem we came with Mini batch Stochastic Gradient Descent algorithm in this algorithm we are creating mini batches of data points and then performing the forward propagation and backward propagation on each mini batch of data points simultaneously. In Mini batch Stochastic Gradient Descent algorithm we use parallel and Distribution learning. In this algorithm we had enough data points to have the learning experience to overcome the local minima and so our algorithm was good enough then SDGA.\n",
    "4. But we were still facing the noise issue in our learning curve , noise present in our learning curve says that we care using more resouces and more time is being spent.To solve this issue came Mini batch Stochastic Gradient Descent algorithm with momentum.Momentum is smoothning of the learning curve.We are applying momentum on our Gradients.We are replacing gradients by velocity componenets.Why we are applying momentum only on Gradients? Lets consider gradients as the potential energy.When we use equations (Wnew = Wold - alpha * (Daeba[L] / Daeba[W])) we get noise in our learning curve.To avoid this noise we need to control the gradients and so we will replace the gradients with velocity componenets.So the updated equations are \n",
    "> Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "> Vdbt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[b])\n",
    "* Where t = no of hidden layer \n",
    "* Beta >> Smoothning parameter ( Range 0 to 1)\n",
    "* Vdw ,Vdb = Velocity components \n",
    "> Case 1 Beta = 0 \n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = (Daeba[L] / Daeba[W])     >> No Smoothning \n",
    "> Case 2 Beta = 1 \n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = Vdw t-1                   >> Full Smoothning \n",
    "> Case 3 Beta = 0.98\n",
    "* Vdwt = Beta * Vdwt t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* In above formula (Beta * Vdw t-1) represents Magnitude and (1 - Beta) (Daeba[L] / Daeba[W]) represents direction.\n",
    "* With the help of the Beta we can control the Gradients and by that we control the Noise in the learning curve.\n",
    "* \n",
    "> Best optimization function for weight and bias?\n",
    "* Mini Batch Stochastic Gradient Descent Algorithm with Momentum.(MBSGDA with momentum)\n",
    "* \n",
    "> Why we nned to update the Learning rate dynamically in Neural Network?\n",
    "* Before studying Learning rate we nned to understand the Learning Step.\n",
    "* When we start a point on learning curve whatever baby steps we take to reach towards our global minima are called as learning steps.\n",
    "* The rate through which our learning steps vary are called as learning rate.\n",
    "> Difference between Machine learning and Deep learning?\n",
    "* In Machine learning before building a model we make changes in oue data by the host. We create a dataset which is suitable for our model.We check distribution of data , Multicollinearity reduction,Feature Selection techniques that is we remove our anomaly.\n",
    "* In Machine learning there is less complexity of the data that is why we keep the learning rate high it doensot affect the model much, but this doesnot apply to the deep learning.\n",
    "* Because in Deep learning we neither apply the feature selection techniques , nor feature engineering because the Deep learning(ANN) Architecture itself selects the features whiich have high weightage or have importance.\n",
    "* As we are not making changes in the data the complexity of the data is high so we need to dynamically update the learning rate which is mandatory .\n",
    "* \n",
    "> Case 1 If learning rate is too small then what will happen?\n",
    "* If we are having small learning rate then our learning steps would be too small to reach global minima and this phenomenon is called as Vanishing Gradient.\n",
    "* Training time increases exponentially.\n",
    "* We face Vanishing Gradient issues.\n",
    "* We never achieve Convergence.\n",
    "* \n",
    "> Case 2 If learning rate is too high then what will happen?\n",
    "* If we are having high learning rate then our learning steps would be too high it can overpass the global minima and this phenomenon is called as Exploding Gradient.\n",
    "* We face overshooting issues.\n",
    "* Due to Overshooting issues we face Exploding Gradient issues.\n",
    "> To avoid the vanishing gradient problem and Exploding Gradient issues we need to dynamically update the learning rate.\n",
    "> One more reason to dynamically update the learning rate is while considering the values to start with the learning rate may cause vanishing gradient issues or Exploding Gradient issues.While dynamically updating the learning rate we need not to worry about vanishing gradient issues or Exploding Gradient issues.\n",
    "* To dynamically update the learning rate we have 2 optimizations functions\n",
    "1. Ada Grad (Adaptive Gradient)\n",
    "* It learns from the gradient to update the learning rate.\n",
    "2. Ada Delta (RMS props) >> Root mean squared propagation\n",
    "* \n",
    "> 1. Ada Grad (Adaptive Gradient)\n",
    "* Wnew = Wold - alpha(new) (Daeba[L] / Daeba[W])\n",
    "* bnew = bold - alpha(new) (Daeba[L] / Daeba[b])\n",
    "* alpha(new) = alpha(old) / sqrt[eta + E]    >> E = Epsilon\n",
    "* We use Epsilon to avoid zero division square error\n",
    "* Where eta = Summation (Daeba[L] / Daeba[W])^2\n",
    "* We follow the weight gradient to update the learning rate.\n",
    "* This is called as Ada Grad (Adaptive)\n",
    "* If we increase the number of hidden layers our values of eta will also increase.\n",
    "* The values of ets is inversely proportional to the new learning rate.\n",
    "* If our value of eta is increasing then our value of new learning rate will decrease and we will face the Vanishing Gradient issue.\n",
    "> Draw back of Ada Grad (Adaptive Gradient)\n",
    "* We cannot use Ada Grad (Adaptive Gradient) for Deep Neural Network(As multiple hidden layers are present)\n",
    "* To Overcome this problem we will see Ada Delta(RMS prop) >> Root mean squared propagation \n",
    "> 2. Ada Delta (RMS prop) >> Root mean squared propagation\n",
    "* We now replace eta by Sdwt\n",
    "* Now we have \n",
    "* Wnew = Wold - alpha(new) (Daeba[L] / Daeba[W])\n",
    "* bnew = bold - alpha(new) (Daeba[L] / Daeba[b])\n",
    "* alpha(new) = alpha(old) / sqrt[Sdwt + E]\n",
    "* Where Sdwt = velocity component \n",
    "* Vdwt = Beta * Vdw t-1  + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Sdwt = Beta * Sdw t-1  + (1 - Beta) (Daeba[L] / Daeba[W])^2\n",
    "* Where Beta = Smoothening parameter\n",
    "* Sdwt doenot allow eta value to increase so our alpha value would not be reduced.\n",
    "* Sdwt caps the value of eta in a certain range it will decrease but it will not increase.\n",
    "* \n",
    "> Best optimization function for learning rate \n",
    "* Ada Delta (RMS prop) >> Root mean squared propogation \n",
    "* \n",
    "> Best optimization function for Weight and Bias?\n",
    "* Mini Batch Stochastic Gradient Descent Algorithm with momentum.\n",
    "* \n",
    "> Best optimization function for Learning rate?\n",
    "* Ada Delta (RMS prop) >>> Root mean squared propogation \n",
    "* Applying two optimization function seperately on algorithm can be time consuming and more resource intensive.\n",
    "* To overcome this issue we combined the Mini Batch Stochastic Gradient Descent Algorithm with momentum and Ada Delta(RMS prop) >> Root Mean squared propogation and that optimization function is called as ADAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Mini Batch Stochastic Gradient descent algorithm (MBSGDA)\n",
    "* We are creating mini batches randomly and applying Batvh Gradient descent algorithm on it.\n",
    "* MBSGDA is a combination of BGDA and SGDA.\n",
    "* In BGDA we get Smooth learning curve but our resources and time would be used much amd in SGDA we were saving our resources and time but we were not getting smooth curve.\n",
    "* Suppose we have a Batch(Complete data) and we are creating mini batches MB1, MB2,MB3 and MB4 by random sampling.\n",
    "* We will apply forward propagation and backward propagation on each mini batch simultaneously.\n",
    "* In forward propagation we get Ypred and we will calculate the Loss.\n",
    "* For MB1 we will get Ypred1 and L1 by forward propagation and we will perform backward propagation after that.\n",
    "* In Backward propagation we will update the values of Weight and bias.\n",
    "* Same we will apply for all mini batches.\n",
    "* We will calculate the losses for all mini batches and minimum of all losses will be selected.\n",
    "* We will get the best values of Weight and bias.\n",
    "* We will calculate the best Summation function.\n",
    "* Then we will apply the activation function.\n",
    "* And after that we will get the final Ypred.\n",
    "* We are performing parallel and distribution learning in MBSGDA.\n",
    "* Still we will get noise in our learning curve after performing above operations.\n",
    "* But our noise will be less as compared to SDGA (Noise >> Smoothness of Curve)\n",
    "* In MBSGDA we learn from mini batch at a time hence we now have sufficient amount of learning to overcome local minima point , we ensure that we will reach to global minima.\n",
    "* \n",
    "> Advantages of MBSGDA\n",
    "* Compared to BGDA , MBSGDA requires less resources and time to achieve convergence is less.\n",
    "* Since we are learning from mini batch at a time we have sufficient amount of learning to overcome local minima point.\n",
    "* Hence we will reach to global minima.\n",
    "* \n",
    "> Disadvantages of MBSGDA\n",
    "* Noise is still present in the learning curve.\n",
    "* \n",
    "> Why we dont want noise in our learning curve?\n",
    "* If we remove noise will require less time and less resources to achieve convergence.\n",
    "* By reducing the noise we can overcome the disadvantages of MBSGDA.\n",
    "* \n",
    "> 4. Mini batch Stochastic Gradient Descent algorithm with momentum(MBSGDA with momentum)\n",
    "* Momentum >> Smoothening of the learning curve\n",
    "> Working \n",
    "* Formula \n",
    "> Wnew = Wold - alpha * (Daeba[L] / Daeba[W])\n",
    "> bnew = bold - alpha * (Daeba[L] / Daeba[b])\n",
    "* We will apply momentum to our gradients (Daeba[L] / Daeba[W]) and (Daeba[L] / Daeba[b])\n",
    "* Gradient >> Potential energy \n",
    "* At the starting of the learning curve the starting point will have maximum gradient (Changes in loss with respect to weight) then the point which is closer towards the global minima.\n",
    "* Updated equations\n",
    "> Wnew = Wold - alpha * Vdw\n",
    "> bnew = bokd - alpha * Vdb\n",
    "* Vdw , Vdb = Velocity components\n",
    "> Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "> Vdbt = Beta * Vdw t-1 + (1 - Beta) (Daeba[W] / Daeba[b])\n",
    "* where t = no of hidden layers \n",
    "* Beta >> Smoothning parameters (Range 0 to 1)\n",
    "* Vdw , Vdb = Velocity components\n",
    "> Case 1 Beta = 0 \n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = (Daeba[L] / Daeba[W])   >> No smoothning \n",
    "> Case 2 Beta = 1\n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = Beta * Vdw t -1         >> Full Smoothning \n",
    "> Case 3 Beta = 0.98\n",
    "* Vdwt = Beta * Vdw t -1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* In above formula (Beta * Vdw t -1) represents magnitude and (1 - Beta) (Daeba[L] / Daeba[W]) represents direction.\n",
    "* \n",
    "> Above revision \n",
    "1. By Batch Gradient descent algorithm we were getting a smooth learning curve but the problem we were facing was that we were getting more time complexity and more use of resources which is Bad.To solve this issue we came up with another algorithm.\n",
    "2. Then came the Stochastic Gradient Descent algorithm where we were selecting a random data point and performing the forward propagation and backward propagation on a single data point.In Stochastic Gradient Descent algorithm we reduced our time complexity and use of the resources, but the problem we were facing that as we have no previous learning experience we were not able to overcome the local minima and so our local minima will be consirded as the global minima which is not Good as we will not get the minimum loss.\n",
    "3. To Overcome this issue we came up with another algorithm Mini Batch Stochastic Gradient Descent algorithm in this algorithm we created mini batches from the original data set and then perform the forward propagation and backward propagation on each mini batch simultaneously.In Mini Batch Stochastic Gradient Descent algorithm we performed parallel and distribution learning.In this algorithm we have enough learning experience to overcome the local minima and so we can achieve the convergence(Global Minima point) and so we can say that is better than SDGA.\n",
    "4. But we were still facing the issue of noise which is not good. Noise present in the learning curve says that we still face the issue of time complexity and still we were using more resources.To solve this issue then came the Mini Batch Stochastic Gradient Descent algorithm with momentum.Momentum is the smoothning of the learning curve that is we are reducing the noise in the learning curve.We are applying momentum on our Gradients by replacing the gradients with velocity components.Why we are applying momentum on our Gradients? Lets consider gradients as our potential energy.When we use equation (Wnew = Wold - alpha * (Daeba[L] / Daeba[W])) we get noise in our learning curve.To avoid the noise in the learning curve we replace the gradients with velocity components.So the updated equations are\n",
    "> Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "> Vdbt = Beta * Vdb t-1 + (1 - Beta) (Daeba[L] / Daeba[b])\n",
    "* Where t = no of hidden layer \n",
    "* Beta = Smoothning parameter (Range 0 to 1)\n",
    "* Vdw , Vdb = Velocity components\n",
    "> Case1 Beta = 0 \n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = (Daeba[L] / Daeba[W])    >>  No smoothning\n",
    "> Case2 Beta = 1\n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Vdwt = Beta * Vdw t-1           >>  Full smoothning\n",
    "> Case3 Beta = 0.98\n",
    "* Vdwt = 0.98 * (Vdw t-1) + 0.2 * (Daeba[L] / Daeba[W])\n",
    "* \n",
    "* In Above formula Beta * (Vdw t-1) represents the magnitude and (1 - Beta) (Daeba[L] / Daeba[W]) represents the direction.\n",
    "* With the help of Beta we can control the gradients and by that we can control the noise in the learning curve.\n",
    "* \n",
    "> Best optimization function for Weight and bias?\n",
    "* Mini Batch Stochastic Gradient descent algorithm with momentum (MBSGDA with momentum)\n",
    "* \n",
    "> Why we need to update the learning rate dynamically in Neural Network?\n",
    "* Before studying about the learning rate we need to understand the learning step\n",
    "* When we start a point on the learning curve whatever baby steps we take to reach towards the global minima aare called as learning steps.\n",
    "* The rate through which our learning steps vary are called as learning rate.\n",
    "* \n",
    "> Differences between Machine Learning and Deep Learning?\n",
    "* In Machine Learning before building the model we make changes in the data.We create a dataset which is suitable for our model.We check distribution of the data , Multicolinearity reduction , Feature selection techniques, Feature Engineering techniques that is we remove our anomaly.\n",
    "* In Machine Learning there is less complexity in the data and so even if we apply high learning rate it doenot affect our model.But this is not in the case of Deep Learning.\n",
    "* As in Deep Learning we donot apply Feature Selection techniques, Feature Engineering techniques on the data because the Deep Learning Architecture itsekf selects the features of higher weightage and higher importance.\n",
    "* As we are not making changes in the data the complexity of the data is higher.So we need to dynamically update the learning rate which is mandatory in Deep Learning.\n",
    "* \n",
    "> Case1 If Learning Rate is too small then what would happen?\n",
    "* If we are having small learning rate then our learning steps would be too small and we would not be able to reach Global minima and this phenomenon is called as Vanishing gradient.\n",
    "* Training time increases exponentially.\n",
    "* We face vanishing gradient issues.\n",
    "* We never achieve Convergence(Global minima point)\n",
    "* \n",
    "> Case2 If learning rate is too high then what would happen?\n",
    "* If we are having high learning rate then are learning steps would be too big it can overpass the global minima and this phenomenon is called as Exploding gradient.\n",
    "* We face overshooting issues.\n",
    "* Due to overshooting we face Exploding gradient issues.\n",
    "* \n",
    "> To avoid the Vanishing gradient issue and Exploding gradient issues we need to dynamically update the learning rate.\n",
    "> One more reason to dynamically update the learning rate is while selecting the initial learning rate we may not know that the learning rate we have choosen may cause Vanishing Gradient issues or Exploding gradient issues.While dynamically updating the learning rate we need no to worry about the Vanishing gradient issue and Exploding gradient issue.\n",
    "* To Dynamically update the learning rate we have 2 optimization techniques.\n",
    "1. Ada Grad(Adaptive Gradient)\n",
    "* It learns from the gradient to update the learning rate.\n",
    "2. Ada Delta (RMS prop) >> Root mean squared propogation \n",
    "* \n",
    "> 1. Ada Grad(Adaptive Gradient)\n",
    "* Wnew = Wold - alpha(new) (Daeba[L] /Daeba[W])\n",
    "* bnew = bold - alpha(new) (Daeba[L] /Daeba[b])\n",
    "* alpha(new) = alpha(old) / sqrt[eta + E]    E = Epsilon\n",
    "* We use Epsilon to avoid the zero division square error\n",
    "* Where eta = Summation(Daeba[L] / Daeba[W])^2\n",
    "* We follow the weight gradient to update the learning rate.\n",
    "* This is called as Ada Grad(Adaptive Gradient)\n",
    "* If we increase the number of hidden layers our value of eta will also increase.\n",
    "* The value of eta is inversely proportional to the new learning rate.\n",
    "* If our value of eta is increasing then our learning rate will decrease and we will face the issue of Vanishing gradient issues.\n",
    "* \n",
    "> Draw backs of Ada Grad(Adaptive Gradient)?\n",
    "* We cannot use Ada Grad(Adaptive Gradient) in case of Deep neural networks.\n",
    "* To overcome this problem we will see Ada Delta(RMS prop) >> Root Mean Squared propogation.\n",
    "* \n",
    "> 2. Ada Delta(RMS prop) >> Root Mean Squared propogation\n",
    "* We now replace eta by Sdwt \n",
    "* Now we have \n",
    "* Wnew = Wold - alpha(new) (Daeba[L] / Daeba[W])\n",
    "* bnew = bold - alpha(new) (Daeba[L] / Daeba[b])\n",
    "* alpha(new) = alpha(old) - sqrt(Sdwt + E) \n",
    "* Where Sdwt is the velocity component\n",
    "* Vdwt = Beta * Vdw t-1 + (1 - Beta) (Daeba[L] / Daeba[W])\n",
    "* Sdwt = Beta * Sdw t-1 + (1 - Beta) (Daeba[L] / Daeba[b])^2\n",
    "* Where Beta is the smoothning parameter.\n",
    "* Sdwt doesnot allow the eta value to increase so our alpha value would not be reduced.\n",
    "* Sdwt caps the value of eta in the certain range it will decrease but it will not increase.\n",
    "* \n",
    "> Best Optimization function for learning rate ?\n",
    "* Ada Dela(RMS prop)    >> Root mean squared propogation\n",
    "* \n",
    "> Best Optimization function for Weight and bias?\n",
    "* Min Batch Stochastic Gradient descent algorithm with momentum(MBSGDA with momentum)\n",
    "* \n",
    "* Applying two optimization functions seperately on algorithm can be time consuming and more resource intensive.\n",
    "* To overcome this we combined the Mini Batch Stochastic Gradient descent algorithm with momentum and Ada Delta (RMS prop) >> Root mean squared propogation and that optimization function is called as ADAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ADAM (Adaptive moment estimation)\n",
    "* Best optimization function for weight and bias?\n",
    "* Mini Batch Stochastic gradient descent algorithm with momentum(MBSGDA with momentum)\n",
    "* Best optimization function for learning rate?\n",
    "* Ada Delta (RMS prop) Root Mean Squared propogation\n",
    "> ADAM = MBSGDA with momentum + Ada Delta(RMS prop) Root Mean Squared propogation\n",
    "* ADAM update weight , bias and learning rate simultaneously.\n",
    "> MBSGDA with momentum \n",
    "* Wnew = Wold - alpha * Vdw\n",
    "* bnew = bold - alpha * Vdb\n",
    "> Ada Delta(RMS prop) Root Mean Squared propogation\n",
    "* Wnew = Wold - alpha(new) * (Daeba[L] / Daeba[W])\n",
    "* bnew = bold - alpha(new) * (Daeba[L] / Daeba[b])\n",
    "* alpha(new) = alpha(old) / sqrt[Sdwt + E]   E > Epsilon\n",
    "> ADAM \n",
    "* Wnew = Wold - (alpha(old) / sqrt[Sdwt + E]) * Vdw\n",
    "* bnew = bold - (alpha(old) / sqrt[Sdwt + E]) * Vdb\n",
    "> Best optimization function for weight,bias and learning rate?\n",
    "* ADAM\n",
    "1. MBSGDA with momentum\n",
    "2. Ada Delta (RMS prop) >> Root mean squared propogation \n",
    "3. ADAM      >> 90% we use in Industry \n",
    "> Activation function \n",
    "* It controls the output of any neuron in Neuron Network.\n",
    "1. Sigmoid Function     >>  Binary classification\n",
    "2. Tanh                 >>  Binary classification\n",
    "3. Softmax              >>  Multi-class classification\n",
    "4. Relu (Rectified linear unit)\n",
    "* Base activation function for regression\n",
    "5. Leaky Relu\n",
    "6. P - Relu \n",
    "7. ELu \n",
    "8. Swish \n",
    "* Leaky Relu , P - Relu , Elu , Swish are the variant of Relu.\n",
    "* \n",
    "> 1. Sigmoid Function \n",
    "* Z = summation (Wi * Xi + b)\n",
    "* Sigmoid(Z) = 1 / (1 + e^-Z)\n",
    "* Sigmoid always gives probablity value of any class.\n",
    "* Range 0 to 1\n",
    "> Advantages of Sigmoid activation function if we use it in output layer?\n",
    "* Sigmoid actiation function gives uniform result (Range is 0 to 1)\n",
    "* On the basis of probablity value we can take decisions confidently.\n",
    "> Why we avoid using Sigmoid function in output hidden layer?\n",
    "1. Sigmoid is non zero centric function.\n",
    "* Zero centricity > mean(f(x)) = 0   then f(x) > Zero centric function\n",
    "* If mean(f(x)) is not equal to zero then it is Non zero centric function.\n",
    "* Zero centric function converges faster than Non zero centric function.\n",
    "* Sigmoid > range 0 to 1 > mean = 0.5 \n",
    "* As Sigmoid function is a non zero centric function we avoid using Sigmoid function in output hidden layer.\n",
    "2. Derivatives of Sigmoid is very small number \n",
    "* Changes in gradients(Daeba[L] / Daeba[W]) causes changes in loss and changes in loss vary the weight and bias and changes in weight and bias causes changes in Best summation function and changes in best summation function causes changes in activation function and changes in activation function causes changes in the output(Daeba[O2] / Daeba[W]) of the hidden layer.\n",
    "* So we can write as (Daeba[L] / Daeba[W]) = (Daeba[O2] / Daeba[W]) \n",
    "* We can olso write the above eqeuation as (Daeba[L] / Daeba[W]) = (Daeba[O2] / Daeba[Z]) * (Daeba[Z] / Daeba[W])\n",
    "* As O2 = Sigmoid(Z)\n",
    "* The above eqeuation becomes (Daeba[L] / Daeba[W]) = (Daeba[sigmoid(Z)] / Daeba[Z]) * (Daeba[Z] / Daeba[W])\n",
    "* (Daeba[sigmoid(Z)] / Daeba[Z]) is the derivative of the Sigmoid.\n",
    "* If our derivative of Sigmoid is small then our gradient will drop and we will face the issue of Vanishing gradient.\n",
    "* So we will avoid the Sigmoid function.\n",
    "* We can use Sigmoid function in Shallow Neural Network.\n",
    "* When we are manually building our ANN then at time avois using the Sigmoid function in the hidden layers.\n",
    "* But if we want to hypertune our ANN then at time we can use the Sigmoid function.\n",
    "* \n",
    "> 2. Tanh\n",
    "* Range -1 to +1 \n",
    "* As the mean of the Tanh activation function is Zero we consider it as Zero Centric function.\n",
    "* Formula for Tanh = e^Z - e^-Z / e^Z + e^-Z \n",
    "> Advantage of Tanh function \n",
    "* It is a Zero Centric function so it will converge faster than Sigmoid function.\n",
    "> Diadvantage of Tanh function\n",
    "* It is mathematically heavy as compared to Sigmoid function.\n",
    "* Derivative of Tanh function is still a small number.\n",
    "* This development of Tanh function was in the year 1998 to 2000.\n",
    "* In the Year 2000 came Relu activation function and also GPU was launched.\n",
    "* \n",
    "> 3. Relu (Rectified linear unit)\n",
    "* Relu is a non zero centric function.\n",
    "* It works on maximize theory.\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "> Case1  Z = +ve (Z = 55)\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* Relu(Z) = 55\n",
    "> Case2  Z = -ve (Z = -55)\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* Relu(Z) = 0\n",
    "* So is is called as rectified linear unit.\n",
    "> How Relu solved the Vanishing gradient problem?\n",
    "> Case1 > Z = +ve \n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[Max(0,Z)] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[Z] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = 1\n",
    "> Case2 > Z = -ve \n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[Max(0,Z)] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[0] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = 0\n",
    "* Then we will the face issue of Dead Neuron.\n",
    "* It is called as Dead Relu issue.\n",
    "> Our Bias can never be negative but our weight can , in case of features which are inversely proportional to the target column\n",
    "* But as our Z contains summation of all values and our Z will be negative when all the summation values are negative, which is not possible.\n",
    "* As the disadvantages of Relu are Dead neuron will not be faced oftenly(5%) we use Relu extensively.\n",
    "> Advantages of Relu \n",
    "* Mathematically lightweight\n",
    "* With the help of Relu we can avoid the vanishing gradient problem.\n",
    "> Disadvantages of Relu\n",
    "* It is a non zero centric function.\n",
    "* We might face Dead Neuron issue or Dead Relu issue.\n",
    "* \n",
    "> 4. Leaky Relu \n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* Leaky Relu(Z) = Max(0.001Z , Z)\n",
    "> Case1   Z = +ve \n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = Daeba[Max(0.001Z,Z)] / Daeba[Z]\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = Daeba[Z] / Daeba[Z]\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = 1\n",
    "> Case2   Z = -ve \n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = Daeba[Max(0.001Z,Z)] / Daeba[Z]\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = -0.001\n",
    "* When we get our gradient negative we will face the issue of noise in the learning curve.\n",
    "* \n",
    "> 5. P-Relu (Parametric Relu)\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* LeakyRelu(Z) = Max(0.001Z , Z)\n",
    "* P-Relu = Max(CZ , Z)   Where C = Learnable parameter\n",
    "> Case1 When C = 0 \n",
    "* P-Relu = Max(CZ , Z)\n",
    "* P-Relu = Max(0 , Z)   > Relu\n",
    "> Case2 when C = 0.001\n",
    "* P-Relu = Max(CZ , Z)\n",
    "* P-Relu = Max(0.001Z , Z)    >> LeakyRelu\n",
    "* In Deep Learning keep the architecture as simple as possible because the more the complicated architecture, the more the resources will be used.\n",
    "* \n",
    "> 6. Elu (Exponential linear unit)\n",
    "* By the use of Exponential curve we get a smooth learning curve.\n",
    "* Elu is a zero centric function \n",
    "* It is used in too much complex data (NASA,Science research)\n",
    "* Elu(Z) = MAX(alpha(e^Z - 1) , Z)\n",
    "* where alpha is a learnable parameter.\n",
    "* Mathematicallly heavy \n",
    "* \n",
    "> 7.Swish \n",
    "* When the research papers of LSTM were published at that time the use of Swish was first seen.\n",
    "* Swish(Z) = Z * Sigmoid(Z)\n",
    "* Swish(Z) = Z * 1 / (1 + e^-Z)\n",
    "* We cannot use Swish in case of Shallow Neural Network.\n",
    "* We can use Swish when numbers of hidden layers are greater than 40.\n",
    "* \n",
    "> 8. Softmax function \n",
    "* Modified version of Sigmoid function and it is specially designed for Multiclass Classification.\n",
    "* Lets say we are dealing with multiclass classification we have 5 classes.\n",
    "*           Class              Output of Softmax \n",
    "              A              prob(X give Y = A) = Prob(A)\n",
    "              B              prob(X give Y = B) = Prob(B)\n",
    "              C              prob(X give Y = C) = Prob(C)\n",
    "              D              prob(X give Y = D) = Prob(D)\n",
    "              E              prob(X give Y = E) = Prob(E)\n",
    "\n",
    "              prob(A) + prob(B) + prob(C) + prob(D) + prob(E) = 1\n",
    "              Max(prob(A) + prob(B) + prob(C) + prob(D) + prob(E))\n",
    "* Whatever the maximum of all probablity will be selected as output.\n",
    "* S(A) = prob(A) = e^A / e^A + e^B + e^C + e^D + e^E\n",
    "* Where A, B, C, D and E are real numbers.\n",
    "* Same we will apply for all and amongst all probabilities we will select the maximum probablity.\n",
    "* Softmax converts a vector of k real numbers into a probability distribution of k possible outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ADAM (Adaptive moment estimation)\n",
    "* Best optimization function for Weight and bias?\n",
    "* Mini Batch Stochastic gradient descent algorithm with momentum(MBSGDA with momentum)\n",
    "* Best optimization function for Learning Rate?\n",
    "* Ada Delta(RMS prop) >> Root Mean Squared propogation\n",
    "> ADAM = MBSGDA with momentum + Ada Delta(RMS prop) >> Root Mean Squared propogation\n",
    "* ADAM update weight,bias and learning rate simultaneously.\n",
    "> MBSGDA with momentum \n",
    "* Wnew = Wold - alpha * Vdw\n",
    "* bnew = bold - alpha * Vdb\n",
    "> Ada Delta(RMS prop) >> Root Mean Squared propogation\n",
    "* Wnew = Wold - alpha(new) * (Daeba[L] / Daeba[W])\n",
    "* bnew = bold - alpha(new) * (Daeba[L] / Daeba[b])\n",
    "* alpha(new) = alpha(old) / sqrt[Sdwt + E]  E >> Epsilon \n",
    "> ADAM \n",
    "* Wnew = Wold - (alpha(old) / sqrt[Sdwt + E]) * Vdw\n",
    "* bnew = bold - (alpha(old) / sqrt[Sdwt + E]) * Vdb\n",
    "> Best optimization function to update the weight,bias and learning rate?\n",
    "* ADAM \n",
    "1. MBSGDA with momentum \n",
    "2. Ada Delta (RMS prop) >> Root mean squared propogation \n",
    "3. ADAM >> 90% use in Industry \n",
    "> Activation function\n",
    "* It controls the output of any neuron in the Neural Network.\n",
    "> Types of Activation functions\n",
    "1. Sigmoid Function >> Binary classification\n",
    "* Converts the output of neuron in Specific range.\n",
    "2. Tanh             >> Binary classification\n",
    "3. Softmax          >> Multiclass classification >>> Modified version of Sigmoid\n",
    "4. Relu (Rectified Linear unit)\n",
    "* Base activation function for regression.\n",
    "5. Leaky Relu \n",
    "6. P-Relu \n",
    "7. Elu \n",
    "8. Swish \n",
    "> 1. Sigmoid function \n",
    "* Z = summation(Wi * Xi + b)\n",
    "* Sigmoid(Z) = 1 / (1 + e^-Z)\n",
    "* Sigmoid always gives probablity value of any 1 class.\n",
    "* Range 0 to 1\n",
    "> Advantages of Sigmoid activation function if we use it in output layer?\n",
    "* Sigmoid activation function gives uniform result.(Range 0 to 1)\n",
    "* On the basis of probablity value we can take decisions confidently.\n",
    "> Why we avoid to use the Sigmoid function in output hidden layer?\n",
    "1. Sigmoid is a non zero centric function.\n",
    "* When mean(f(x)) = 0 then we say that f(x) is a zero centric function.\n",
    "* if mean(f(x)) is not equal to 0 then we say that f(x) is a non zero centric function.\n",
    "* Zero centric function coverges faster than non zero centric function.\n",
    "* Sigmoid range > 0 to 1 >> mean = 0.5\n",
    "* As Sigmoid function becomes a non zero centric function we avoid the use Sigmoid function in outpit hidden layer.\n",
    "2. Derivatives of Sigmoid is very smaller number\n",
    "* Changes in Gradients(Daeba[L] / Daeba[W]) vary the changes in Loss and changes in loss vary the changes in weight and bias and changes in weight and bias vary the changes in best summation function and changes in Best summation function vary the changes in activation function and changes in activation function vary the changes in the output (Daeba[O2] / Daeba[W]) of the hidden layer.\n",
    "* So we can write the equation (Daeba[L] / Daeba[W]) = (Daeba[O2] / Daeba[W])\n",
    "* We can also write the equation (Daeba[L] / Daeba[W]) = (Daeba[O2] / Daeba[Z]) * (Daeba[Z] / Daeba[W])\n",
    "* As O2 = Sigmoid(Z)\n",
    "* The above equation becomes (Daeba[L] / Daeba[W]) = (Daeba[Sigmoid(Z)] / Daeba[Z]) * (Daeba[Z] / Daeba[W])\n",
    "* Daeba[Sigmoid(Z)] / Daeba[Z] is the derivative of the Sigmoid.\n",
    "* If our derivatives of the sigmoid is small then our gradients will also decrease and this will cause the issue of Vanishing gradient.\n",
    "* So we will avoid the Sigmoid function.\n",
    "* We can use the Sigmoid function in case of Shallow Neural Network.\n",
    "* When we are manually building the ANN then we should avoid the use of the Sigmoid function in the hidden layers.\n",
    "* But if we want to hypertune our ANN then at that time we can use the Sigmoid function.\n",
    "* \n",
    "> 2. Tanh \n",
    "* Range -1 to +1 \n",
    "* As the mean of the Tanh function is zero we consider it as Zero centric function.\n",
    "* Which means that it converges faster than the Sigmoid function.\n",
    "* Formula Tanh(Z) = e^Z - e^-Z / e^Z + e^-Z \n",
    "> Advantage of the Tanh function\n",
    "* As it is a Zero centric function it converges faster than the Sigmoid function\n",
    "> Disadvantage of the Tanh function \n",
    "* It is Mathematically heavy as compared to the Sigmoid function.\n",
    "* Derivative of Tanh is still a small number.\n",
    "* This development of the Tanh function was in the Year 1998 to 2000\n",
    "* In the Year 2000 came Relu activation function also the GPU was launched.\n",
    "* \n",
    "> 3. Relu (Rectified Linear unit)\n",
    "* Relu is a non zero centric function.\n",
    "* It works on Maximize theory.\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "> Case1   Z = +ve (Z = 55)\n",
    "* Relu(Z) = Max (0 , Z)\n",
    "* Relu(Z) = 55\n",
    "> Case2   Z = -ve (Z = -55)\n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* Relu(Z) = 0\n",
    "* So it is called as Rectifier linear unit.\n",
    "> How Relu solved the Vanishing Gradient issue?\n",
    "> Case1    Z = +ve \n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[Max(0,Z)] / Daeba[Z]\n",
    "* Daeba[Relu(z)] / Daeba[Z] = Daeba[Z] / Daeba[Z]\n",
    "* Daeba[Relu(z)] / Daeba[Z] = 1\n",
    "> Case2    Z = -ve \n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[Max(0,Z)] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = Daeba[0] / Daeba[Z]\n",
    "* Daeba[Relu(Z)] / Daeba[Z] = 0 \n",
    "* Then we will face the issue of Dead neuron.\n",
    "* It is also called as Dead Relu issue.\n",
    "> Our bias can be never negative but our weight can in case of features which are inversely propertional to the target column.\n",
    "* But as our Z contains summation of all features , all features cannot be inversely proportional to the target column.\n",
    "* As the disadvantage of Relu is not faced oftenly(5%) we use Relu extensively.\n",
    "> Advantage of Relu\n",
    "* Mathematically lightweight \n",
    "* With the help of Relu we can avoid the vanishing gradient issue.\n",
    "> Disadvantage of Relu\n",
    "* It is a non zero centric function.\n",
    "* We might face Dead neuron issue are Dead relu issue.\n",
    "* \n",
    "> 4. Leaky Relu \n",
    "* Relu(Z) = Max(0 , Z)\n",
    "* Leaky Relu(Z) = Max(0.001Z , Z)\n",
    "> Case1    Z = +ve \n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = Daeba[Max(0.001Z,Z)] / Daeba[Z]\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = 1\n",
    "> Case2    Z = -ve\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = Daeba[Max(0.001Z,Z)] / Daeba[Z]\n",
    "* Daeba[LRelu(Z)] / Daeba[Z] = -0.001\n",
    "* When we get our gradient negative we will face the issue of noise in our learning curve.\n",
    "* \n",
    "> 5. P-Relu (Parametric Relu)\n",
    "* Relu(Z) = Max(0 ,Z)\n",
    "* LeakyRelu(Z) = Max(0.001Z , Z)\n",
    "* P-Relu(Z) = Max(CZ , Z)     > C = Learnable parameter\n",
    "> Case1    when C = 0 \n",
    "* P-Relu(Z) = Max(0 ,Z)      >> Relu\n",
    "> Case2    when C = 0.001\n",
    "* P-Relu(Z) = Max(0.001Z , Z) >> LeakyRelu\n",
    "* In Deep Learning kep the architecture as simple as possible because the more complicated the architecture the more resources will be allocated.\n",
    "* \n",
    "> 6. Elu (Exponential linear unit)\n",
    "* By use of the Exponential function we get a smooth learning curve.\n",
    "* Elu is a zero centric function \n",
    "* it is used in too much complex data(NASA ,SCienctific research)\n",
    "* Elu(Z) = Max(alpha(e^Z - 1) , Z)\n",
    "* Where alpha = learnable parameter\n",
    "* Mathematically heavy \n",
    "* \n",
    "> 7. Swish \n",
    "* When the research papers of LSTM were published at that time Swish was first seen.\n",
    "* Swish(Z) = Z * Sigmoid(Z)\n",
    "* Swish(Z) = Z * 1 / (1 + e^-Z)\n",
    "* We cannot use Swish in case of Shallow Neural Network.\n",
    "* We can use Swish when number of hidden layers are greater than 40.\n",
    "* \n",
    "> 8. Softmax \n",
    "* Modified version of the Sigmoid function and it is specifically used for Multiclass classification.\n",
    "* Lets say we are dealing with a multiclass classification having 5 samples \n",
    "*              Class             Output of Softmax\n",
    "                 A              prob(X given Y = A) = prob(A)\n",
    "                 B              prob(X given Y = B) = prob(B)\n",
    "                 C              prob(X given Y = C) = prob(C)\n",
    "                 D              prob(X given Y = D) = prob(D)\n",
    "                 E              prob(X given Y = E) = prob(E)\n",
    "\n",
    "                 prob(A) + prob(B) + prob(C) + prob(D) + prob(E) = 1\n",
    "                 Max(prob(A) + prob(B) + prob(C) + prob(D) + prob(E))\n",
    "* Whatever the maximum probability of all will be selected \n",
    "* S(A) = prob(A) = e^A  / e^A + e^B + e^c + e^D + e^E\n",
    "* Where A,B,C,D,E are real numbers.\n",
    "* Same will apply for all and amongst all probabilities we will select the maximum probablity.\n",
    "* Softmax converts a vector of k real numbers into a probability distribution of k possible outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Regression loss function \n",
    "*   1. MSE Mean Squared Error\n",
    "* MSE = 1/n summation(Ya - Yp)^2\n",
    "* MSE increases big error values so that optimizer function can easily identify big error terms.\n",
    "> Drawbacks of MSE \n",
    "* Since we are penalizing the big error values we cannot use MSE in case of Outliers.\n",
    "*   2. Mean Absolute Error(MAE)\n",
    "* MAE = 1/n summation |Ya - Yp|\n",
    "* We can use MAE in case of Outliers.\n",
    "*   3. Mean squared logarithmic error (MSLE)\n",
    "* MSE = 1/n summation (Ya - Yp)^2\n",
    "* MAE = 1/n summation |Ya - Yp|\n",
    "* MSLE = 1/n summation (log(Ya +1) - log(Yp + 1))^2\n",
    "* Example > Age Feature in Netflix data \n",
    "* \n",
    "> 2. Binary classification loss function \n",
    "*   1. Binary cross entropy loss function (Log Loss Function)\n",
    "> Condition for use \n",
    "* Classification    >>  Binary classification \n",
    "* Activation function in the output layer should be Sigmoid.\n",
    "* Dependent variable should be encoded.\n",
    "* Log Loss = -1 / n summation [Ya * logYp + (1 - Ya) * log(Yp -1)]\n",
    "*   2. Hinge Loss \n",
    "> Condition for use\n",
    "* Classification    >> Binary classification\n",
    "* Actiavtion function in the output layer should be Tanh.\n",
    "* Dependent variable should be encoded within range -1 to +1\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Tanh range = -1 to +1\n",
    "> Case1 Ya = 1 and Yp = 0.6\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Hinge loss = Max(0 , 1 - (1 * 0.6))\n",
    "* Hinge loss = 0.4\n",
    "> Case2 Ya = 1 and Yp = -0.6\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Hinge loss = Max(0 , 1 - (1 * -0.6))\n",
    "* Hinge loss = 1.6\n",
    "* Tanh is sign specific.\n",
    "* It classifies on the basis of sign if negative then negative class(-1) and if poisitve then poisitive class(+1).\n",
    "* We dont use oftenly Tanh in Industry.\n",
    "* \n",
    "> 3. Multiclass classification of loss function \n",
    "*   1. Muliclass cross entropy loss function (Categorical cross entropy loss function)\n",
    "* Loss = - summation (Ya * log(Yp))\n",
    "> Conditions for use \n",
    "* Classification    >>  Multiclass \n",
    "* Output layer activation function   >>  Softmax\n",
    "* Dependent variable should be one hot encoded.\n",
    "> Drawbacks of Multiclass cross entropy loss function\n",
    "* In case of Multiclass cross entropy loss function dependent variable should be one hot encoded.\n",
    "* It creates a Sparse matrix.\n",
    "* So in this case it makes mathematically complex.\n",
    "*   2. Sparse Multiclass cross entropy loss function\n",
    "> Sparse Matrix\n",
    "* A Sparse Matrix is a special case of a matrix in which the number of zero elements is much highe than the number of non zero elements.As a rule of thumb if 2/3 rd of the total elements of the matrix are zeros then it is called as Sparse Matrix.\n",
    "> Condition for use \n",
    "* CLassification   >>  Multiclass \n",
    "* Output layer activation function >> Softmax\n",
    "* Dependent variable should be one hot encoded but loss will be calculated for 1.\n",
    "* Loss = - summation (Ya * log(Yp))\n",
    "* \n",
    "* \n",
    "> Initializer \n",
    "* In Deep Learning time complexity and resource consumption are very important they should be low as Possible.\n",
    "* Parameter >> Weight,bias and learning rate.\n",
    "* Initializer only works for considering the weightage between neurons.\n",
    "* Bias and learning rate can be initalized randomly.\n",
    "* But we cannot consider this in case of weight.\n",
    "* Initializer tells us what value to intialize with consideration of weight.\n",
    "* Initializer is a type of weight distribution.\n",
    "> Types of initializers\n",
    "1. He initializer\n",
    "* He normal distribution\n",
    "* He uniform distribution\n",
    "2. Glorort initializer\n",
    "* Glorort normal distribution\n",
    "* Glorort uniform distribution\n",
    "> General thumb rule \n",
    "* If we deal with regression problems we should consider He initializer it will work good.\n",
    "* If we deal with classification problems we should consider Glorot initializer it will work good.\n",
    "* This thumb rule applies when we are buliding our model manually.\n",
    "* If we are hypertuning our parameters then we should use all 4 initializers.\n",
    "* Use only Sigmoid and Relu as activation function of the hidden layers and not softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Regression loss function\n",
    "*   1. Mean squared error(MSE)\n",
    "* MSE = 1/n summation (Ya - Yp)^2\n",
    "* MSE increases big error values so that optimizer function can easily identify big error terms.\n",
    "> Drawbacks of MSE \n",
    "* Since we are penalizing the big error values we cannot MSE in case of Outliers.\n",
    "*   2. Mean Absolute Error(MAE)\n",
    "* MAE = 1/n summation |Ya - Yp|\n",
    "* We can use MAE in case of Outliers.\n",
    "*   3. Mean Squared logarithmic error(MSLE)\n",
    "* MSE = 1/n summation (Ya - Yp)^2\n",
    "* MAE = 1/n summation |Ya - Yp|\n",
    "* MSLE = 1/n summation (log(Ya + 1) - log(Yp + 1))^2\n",
    "* Example > Age Feature in Netflix data \n",
    "* \n",
    "> 2. Binary classification loss function\n",
    "*   1. Binary cross entropy loss function\n",
    "> Conditions of use \n",
    "* Classification     >>   Binary \n",
    "* Activation function in output layer should be Sigmoid.\n",
    "* Dependent variable should be encoded.\n",
    "* Log loss = -1/n summation [Ya * log(Yp) + (1 - Ya) * log(1 - Yp)]\n",
    "*   2. Hinge Loss \n",
    "> Conditions for use \n",
    "* classification     >>    Binary \n",
    "* Activation function in output layer should be Tanh.\n",
    "* Dependent variable should be encoded in -1 to +1.\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Tanh range -1 to +1\n",
    "> Case1   Ya=1 , Yp=0.6\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Hinge loss = Max(0 , 1 - (1*0.6))\n",
    "* Hinge loss = 0.4\n",
    "> Case2   Ya=1 , Yp=-0.6\n",
    "* Hinge loss = Max(0 , 1 - YaYp)\n",
    "* Hinge loss = Max(0 , 1 - (1*-0.6))\n",
    "* Hinge loss = 1.6\n",
    "* Tanh is sign specific.\n",
    "* It classifies on the basis of sign if negative then negative class(-1) and if positive then pisitive class(+1).\n",
    "* We dont use Tanh oftenly in Industry.\n",
    "* \n",
    "> 3. Multiclass classification loss function \n",
    "*  1. Multiclass cross entropy loss function (Categorical cross entropy loss function)\n",
    "* Loss = - summation (Ya * log(Yp))\n",
    "> conditions for use \n",
    "* Classification    >>   Binary \n",
    "* Output layer activation function   >>  Softmax\n",
    "* Dependent variable should be one hot encoded.\n",
    "> Drawbacks of Mulitclass cross entropy loss function\n",
    "* In case of Muliclass cross entropy loss function dependent variable should be one hot encoded.\n",
    "* It creates a Sparse matrix.\n",
    "* So in this case it is mathematically complex.\n",
    "*  2. Sparse Multiclass cross entropy loss function\n",
    "> Sparse matrix\n",
    "* A Sparse matrix is a special case of matrix in which the number of zero elements is much higher than the number of non zero elements.As a rule of thumb if 2/3 rd of the total elements of the matrix are zeroes it can be called as Sparse Matrix.\n",
    "> Conditions for use \n",
    "* Classification      >>>   Multiclass\n",
    "* Output layer activation function    >>  Softmax\n",
    "* Dependent variable should be one hot encoded but loss will be calculated for 1.\n",
    "* Loss = - summation (Ya * log(Yp))\n",
    "* \n",
    "> Initializer \n",
    "* In deep learning time complexity and resource consumption are very important both should be low as possible.\n",
    "* Parameter >> weight , bias and learning rate\n",
    "* Initializer works only for considering the weightage between Neurons.\n",
    "* Bias and learning rate can be initialized randomly.\n",
    "* But we cannot consider this in case of weight.\n",
    "* Iniatilizer tells us what value to consider to iniatilize with in consideration of the weight.\n",
    "* Initializer is a type of weight distribution.\n",
    "> Types of iniatilizer?\n",
    "1. He initializer\n",
    "* He normal distribution\n",
    "* He uniform distribution\n",
    "2. Gorot initializer\n",
    "* Gorot normal distribution\n",
    "* Gorot uniform distribution\n",
    "> General thumb rule \n",
    "* If we deal with regression problems He iniatilizer will work good.\n",
    "* If we deal with Classification problems Gorot iniatilizer will work good.\n",
    "* This thumb rule applies when we are building our model manually.\n",
    "* If we are hypertuning our parameters then we should use all four iniatizers.\n",
    "* Use only Sigmoid and Relu in activation function of the hidden layers and not Softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
