{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction technique \n",
    "* In Dimensionality reduction technique we reduce the number of features to reduce the complexity of the model.\n",
    "1. Feature selection\n",
    "  1. Filter method (Before model training) \n",
    "  2. Wrapper method (During model training)\n",
    "  3. Embedded method (After model training)\n",
    "2. Feature Extraction\n",
    "  1. Linear discriminant method \n",
    "  2. Principle component analysis(PCA)\n",
    "* 90% we use PCA \n",
    "> Feature selection\n",
    "* In feature selection we select the subset of original dataset i.e we are selecting the best features and then we are training the model on it.But in this case we are losing the data which may have some significance in predicting Y.\n",
    "* In Feature selection we select the best features by their variance the more the variance the more they contribute in predicting Y.\n",
    "* But when we are selecting the best feature between the two features if the variance of the both features are same then we may face problem while selecting the best feature between the two features in such cases we may use PCA.\n",
    "> Principle component analysis(PCA)\n",
    "* It is one of the most popular feature extraction techniques.\n",
    "* It is a supervised machine learning algorithm which is used for dimensionality reduction techniques.\n",
    "* In PCA we convert high dimensional data to low dimensional data.\n",
    "* When we had 2 feature in the data and we had to select the best feature among them but if they had same variance then we had to face problems while selecting the best feature.\n",
    "* And by selecting the best feature we may also avoid the another feature which was also contributing in predicting Y , this was happening in Feature selection.\n",
    "* In PCA we combine these 2 features and create a single feature which is the combination of the two features.\n",
    "* So in this way we are performing dimensionality reduction techniques and also in some instances we may increase the accuracy of the model.\n",
    "* Suppose we have features X1,X2,X3 and X4 as independent variables and Y as Target variable.\n",
    "* In PCA we wil get the number of columns(PC1,PC2,PC3,PC4)(PC = Principal Component) equal to number of features(X1,X2,X3,X4).\n",
    "* PC1 wil be the extraction of X1,X2,X3,X4.\n",
    "* PC2 wil be the extraction of X1,X2,X3,X4.\n",
    "* Same for PC3 and PC4.\n",
    "* That is we are having essence of X1,X2,X3,X4 in each principal component.\n",
    "* As PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 and suppose we are training our model on PC1,PC2,PC3,PC4 and we are getting the accuracy of the model as 96%.\n",
    "* As PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 , by using only PC1,PC2 we can get the accuracy of the model close to 96%.\n",
    "* In this way by selecting only PC1,PC2 we are getting the accuracy of the model close to 96% , which we were getting by combination of PC1,PC2,PC3,PC4 . So by selecting only PC1,PC2 we are reducing the number of features , so we are converting high dimensional data to low dimensional data.\n",
    "> Difference between covariance and coefficient of correlation(R)\n",
    "* R has a range of -1 to +1 , Covariance range is -infinity to +infinity.\n",
    "* Covariance shows directional relationship between 2 variables and R shows strength of relationship between 2 variables.\n",
    "* Formula for Covariance is = summation((Xi - Xmean) (Yi - Ymean)) / N\n",
    "* Formula for R = summation((Xi - Xmean)^2 (Yi - Ymean)^2) / sqrt((Xi - Xmean)^2 (Yi - Ymean)^2)\n",
    "* Formula for R = Covariance / Product of Standard Deviation\n",
    "> Mean \n",
    "* Mean tells us the central tendency of the data.\n",
    "> Variance \n",
    "* Formula for Variance = summation((Xi - Xmean)^2) / N\n",
    "* Variance tells us the spread of the data.\n",
    "* \n",
    "> Working of PCA \n",
    "* Previously when we had 2 features in the dataset with same variance we were not able to select the best feature among the two.\n",
    "* In PCA we are shifting the new X axis towards the data points and creating the difference between the variance.\n",
    "* While shifting the new X axis we will see for where we get the maximum variance after that the perpendicular line will be the Y axis.\n",
    "* The new X axis will PC1 and the new Y axis will PC2.\n",
    "> Difference between Variance and CoVariance \n",
    "* Variance will never give the direction , it will show the spread of the data.\n",
    "* Covariance will show the directional relationship between 2 variables.\n",
    "* From further on we will not use variance and the reason is that Covariance gives direction and spread of the data points and variance doesnot.\n",
    "> Revise on Eigen values and Eigen Vectors these both come under linear transformation.\n",
    "* In PCA Eigen values represent the amount of variance captured by each principal component.\n",
    "* Larger Eigen values indicate more significant components.\n",
    "* Eigen vectors represent the direction of maximum variance captured by each principal component.\n",
    "* In short Eigen values guide the importance of componenets and eigen vectors define their directions for dimensionality reduction in PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction techniques\n",
    "* In Dimensionality reduction techniques we reduce the number of features to reduce the complexity of the data.\n",
    "1. Feature selection\n",
    "  1. Filter method (Before model training)\n",
    "  2. Wrapper method (During model training)\n",
    "  3. Embedded method (After model training)\n",
    "2. Feature Extraction \n",
    "  1. Linear discriminant method \n",
    "  2. Principal component analysis(PCA)\n",
    "* 90% we use PCA \n",
    "> Feature Selection\n",
    "* In Feature Selection we select the subset of the original dataset i.e we are selecting the best features and then we will train our model on the selected features.\n",
    "* In Feature Selection we select the best features based on their variance the more the variance the mire it is contributing in predcting Y.\n",
    "* But when we are selecting the best features if we have 2 features and they have same variance then we may find problem while selecting the best feature while working in Feature Selection so we use PCA in such cases.\n",
    "> Principle component analysis(PCA)\n",
    "* It is one of the most popular Feature Extraction techniques.\n",
    "* It is a supervised machine learning algorithm which is used for dimensionality reduction techniques.\n",
    "* In PCA we convert high dimensional data to low dimensional data.\n",
    "* When we had 2 features having same variance in the dataset but when we had to select the best feature we had to face difficulty.\n",
    "* And by selecting the best feature we may also avoid the another feature which was contributing in predicting Y.\n",
    "* In this form we are loosing our data which is not a ideal way while building the model.\n",
    "* In PCA we combine both features and create a single feature which is combination of both features.\n",
    "* As we are reducing the number of features we are performing dimensionality reduction techniques and also we can increase the accuracy of the model.\n",
    "* Suppose we have X1,X2,X3,X4 as independent variables and Y as Target variable.\n",
    "* In PCA we will number of columns(PC1,PC2,PC3,PC4)(PC = Principal Component) equal to number of features(X1,X2,X3,X4).\n",
    "* PC1 will be the extraction of X1,X2,X3,X4 features.\n",
    "* PC2 will be the extraction of X1,X2,X3,X4 features.\n",
    "* Same for PC3 and PC4.\n",
    "* That is we are having essence of X1,X2,X3,X4 features in each Principal Component.\n",
    "* PC1,PC2,PC3,PC4 are the extraction of X1,X2,X3,X4 features.Suppose we train our model on PC1,PC2,PC3,PC4 and we get the accuracy 96%. Using Only PC1 and PC2 we may get the accuracy close to 96% as each principal component is the extraction of X1,X2,X3,X4 features.\n",
    "* In this way by selecting the PC1 and PC2 we are getting accuracy equivalent to combination of PC1,PC2,PC3 and PC4 and also we are reducing the number of features and so we are converting high dimensional data to low dimensional data.\n",
    "> Difference between Covariance and Coefficient of Corelation\n",
    "* R has a range between -1 to +1 , Covariance has a range between -infinity to +infinity.\n",
    "* Covariance shows directional relationship between 2 variables , and R shows strength of relationship between 2 variables.\n",
    "* Formula for Covariance = summation((Xi - Xmean) (Yi - Ymean)) / N\n",
    "* Formula for R = summation((Xi - Xmean)^2 (Yi - Ymean)^2) / sqrt((Xi - Xmean^2) (Yi - Ymean^2))\n",
    "* Formula for R = Covariance / Product of standard deviation\n",
    "> Mean \n",
    "* Mean tells us the Central tendency of any kind of data.\n",
    "> Variance \n",
    "* Formula for variance = summation((Xi - Xmean)^2) / N\n",
    "* Variance tells us the spread of the data\n",
    "> Working of PCA \n",
    "* Previously when we had 2 features in the dataset with same variance we were not able to select the best feature among two.\n",
    "* In PCA we are shifting the new X axis towards the data points and creating the difference between the variance.\n",
    "* While shifting the new X axis we will see where we get the maximum variance after that the perpendicular line drawn will be the Y axis.\n",
    "* The New X axis will be PC1 and Y axis will be PC2.\n",
    "> Difference between Variance and Covariance\n",
    "* Variance will never give the direction , it will only show the spread of the data.\n",
    "* Covariance gives directional relationship between 2 data points.\n",
    "* From further on we will not use Variance and the reason is that Covariance gives direction and spread of the data points and Variance doesnot.\n",
    "> Revision on Eigen values and Eigen Vectors these both come under Linear Transformations\n",
    "* In PCA Eigen values represent the amount of variance captured by the Principal Component.\n",
    "* Larger Eigen values indicate more significant components.\n",
    "* Eigen Vectors represent the direction of maximum variance captured for each principal component.\n",
    "* In Short Eigen values guide the importance of components and Eigen Vectors represent define their directions for dimensionality reduction in PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
