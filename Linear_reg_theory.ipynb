{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear regression \n",
    "* It is a predictive model used to find linear relationships between dependent and one or more independent variables.\n",
    "* Which says that it finds outhow the value of dependent variable changes according to the value of independent variable.\n",
    "* It is a parameteric algorithm >> assumptions on distribution of data.\n",
    "* \"Our aim in linear regression is to find the linear relationship between dependent and one or more independent variables.\"\n",
    "* In linear regression we find the best fit line that can accurately predict the value of dependent variable.\n",
    "\n",
    "> Types of linear regression\n",
    "1. Simple linear regression \n",
    "* If a single independent variable is used to predict the value of dependent variable then such type of linear regression is called Simple linear regression.\n",
    "* i.e. we have only one independent variable\n",
    "* y = mx + c\n",
    "m >> slope - linear regression coefficients (scale factor to each input value)\n",
    "c >> intercept of line (gives an additional degree of freedom)\n",
    "x >> Independent variable (Predictor Variable)\n",
    "y >> dependent variable\n",
    "\n",
    "2. Multiple linear regression\n",
    "* If we use more than one independent variable to predict the value of dependent variable then such type of linear regression is called Multiple linear regression.\n",
    "* we have one or more independent variable\n",
    "* Y = m1x1 + m2x2 + c\n",
    "* Y = m1x1 +m2x2 + ....+mnxn + c\n",
    "m1,m2,m3 >> slope \n",
    "x1,x2,x3 >> Independent variables\n",
    "Y >> dependent variables\n",
    "slope(m) >> (Y2 - Y1) / (X2 - X1)\n",
    "\n",
    "> Residuals or error or coefficient of regression(e):\n",
    "* Distance between actual value and predicted value is called as residual.\n",
    "* If the observed points are far away from the regression line ,then the residual will be high and so cost function will be also high.\n",
    "* If the observed points are close to the regression line ,then the residual will be low and so cost function will be also low.\n",
    "* e = Yactual - Ypredicted\n",
    "* Yactual - Actual value of Y >> Original value .. data point\n",
    "* Ypredicted - Predicted value of Y >> value according to the regression line\n",
    "\n",
    "> Linear regression line \n",
    "* A linear line showing linear relationship between dependent variable and independent variables is called as linear regression line.\n",
    "\n",
    "* Positive linear relationship : (R=1)\n",
    "* If we increase our value in independent variable and accordingly the value of dependent variable increases,then such relationship will be called as positive linear relationship.\n",
    "* Negative linear relationship : (R=-1)\n",
    "* If we decrease our value in independent variable and accordingly the value of dependent variable decreases,then such relationship will be called as negative linear relationship.\n",
    "\n",
    "> Advantages of linear regression \n",
    "1. Simple to implement and easier to interpret the output coefficients\n",
    "2. when you know the there is linear relationship between dependent variable and independent variables this algorithm is best to use and it is less complex as compared to other algorithms.\n",
    "3. Linear regression is prone to overfitting but it can be avoided using dimensionality reduction techniques,regularization (L1 and L2) techniques and cross validation.\n",
    "\n",
    "> Disadvantages of linear regression\n",
    "1. If the independent variables are corelated it may affect the performance \n",
    "2. It is only efficient for linear data (High Corr between x and y) \n",
    "3. Sometimes a lot of feature enginnering is required.\n",
    "4. Scaling is required >> predictors have a mean of 0 \n",
    "5. It is often quite prone to noise and overfitting.\n",
    "6. It is sensitive to missing values\n",
    "7. It is sensitive to outliers \n",
    "\n",
    "> Applications of Linear regression \n",
    "1. Forecasting the data \n",
    "2. Analyzing the time series \n",
    "3. Prize prediction \n",
    "4. Salary prediction \n",
    "\n",
    "> Assumptions of Linear regression \n",
    "1. Linearity >> linear relationship between the independent variables and dependent variables\n",
    "2. NoMultiCoLinearity >> All the independent variables should be independent to each other.\n",
    "3. Normality of residual >>\n",
    "4. Homoscedasticity >> variance of residual is same or constant at every level of x or for any value of x\n",
    "\n",
    "1. Linearity >> There should be a linear relationship between the independent variables and dependent variables.\n",
    "* There should be a linear relationship between the independent variables and dependent variables \n",
    "* A linear relationship tells us how 1 unit difference in independent variable affects the dependent variable.\n",
    "* An additive relationship suggests that effect of independent variable on dependent variable is independent of other independent variable.\n",
    "* In linear regression a straight line will be passed through maximum data points as good as possible.\n",
    "* As the line will be passed through maximum data points our coefficient of regression will be minimum.\n",
    "* The line shows linear relationship between independent variable and dependent variable.\n",
    "\n",
    "> How to check linearity?\n",
    "* Coefficient of corelation \n",
    "* scatterplot\n",
    "* Corelation matrix\n",
    "\n",
    "> How to handle linearity if violated?\n",
    "* Apply nonlinear transformation to the independent variable and/or dependent variable\n",
    "* Log transformation\n",
    "* sqrt transformation\n",
    "* reciprocal transformation\n",
    "\n",
    "> What if these linearity get violated?\n",
    "* if linearity gets violated then we will see that the regression line is not passing through maximum data points whcih says that doenot fulfill the requirements of linearity.\n",
    "* Due to which our coefficient of regression (error) will be maximum\n",
    "* It shows non linear relationship between the independent variable and the dependent variable.\n",
    "* Which will result in bad prediction of result so we cannot use Linear regression algorithm\n",
    "\n",
    "> No multicolinearity\n",
    "* All the independent variables are independent to each other is called as Nomulticolinearity.\n",
    "* There should be no linear relationship between the independent variables .\n",
    "* Multicolinearity happens when the independent variables are linearly correlated with each other.\n",
    "* This means that one independent variable can be predicted from another independent variable.\n",
    "* Multicolinearity can be a problem in a regression model because we would not be able to distinguish the individual effects of independent variable on dependent variable.\n",
    "* Y = M1X1 + M2X2 + C\n",
    "* Coefficient M1 is the increase in Y for a unit increase in X1 while keeping X2 constant.\n",
    "* But Since X1 and X2 are corelated changes in X1 will be reflected in X2 .\n",
    "* Multicolinearity doesnot affect the accuracy of the model but we will not be able to distinguish the effects of independent variable on dependent variable.\n",
    "* Mulitcolinearity means high corelation between two or more independent variables\n",
    "* Due to multicolinearity it may be difficult to distinguish the effects of independent variable on dependent variable.\n",
    "* Or we can say it is difficult to determine which independent variable is affecting the dependent variable and which is not .\n",
    "* So the model assumes either little or no multicolinearity between the features or the independent variables.\n",
    "* y = (m1 * x1) + (m2 * x2) + (m3 * x3) + ....... +(mn * xn) + C\n",
    "* If 2 independent variables are strongly correlated with each other then the X1 and X2 values become same or almost same ,regression model will not be able to determine M1 and M2 that means our regression model is unstable \n",
    "* Multcolinaerity is measured by VIF(Variance inflation factor)\n",
    "* VIF starts at 1 and has no upper limit \n",
    "* if VIF =1 no corelation between the independent variables\n",
    "* VIF exceeding 5 or 10 indicates high multcolinaerity between the independent variables\n",
    "\n",
    "> What if these nomulticolinearity gets violated?\n",
    "* If multicolinaerity available it may be difficult to find the true relationship between the independent variables and dependent variable.or we can say that it is difficult to determine which independent variables is affecting dependent variable and which are not.\n",
    "\n",
    "> How to detect multicolinaerity?\n",
    "* VIF(Variance inflation factors)\n",
    "* corelation matrix / corelation plot\n",
    "* scatter plots \n",
    "\n",
    "* The VIF score of independent variable represents how well the variable is explained by other independent variables.\n",
    "* VIF = 1 / (1 - R^2)\n",
    "* VIF = 1 >> No corealtion \n",
    "* VIF = 1 to 5 >> Moderate corelation\n",
    "* VIF > 10  >> High corelation\n",
    "\n",
    "> If any Multicolinaerity present in your model then how it can affect your model?\n",
    "* It will reduce the statistical major or power of your model .Efficiency of prediction will reduce .\n",
    "\n",
    "> How to handle this ?\n",
    "* dropping variables \n",
    "* combining variables\n",
    "* If there is high corelation in independent variables then \n",
    "1. Remove or drop the highly correlated variables(99% of the time we donot remove data as data is important)\n",
    "2. Linearly combine these 2 highly correlated variables\n",
    "3. Principle component analysis(PCA) technique .It is also know as dimensionality reduction technique\n",
    "4. Lasso Regression\n",
    "\n",
    "> Normal Distribution of the error (normality of residual)\n",
    "* if the data points are near the regression line the better the normal distribution of data .\n",
    "* It shows Linear strong relationship between dependent and independent variables.\n",
    "* Normal distribution >> How much our data points are closer or near to the mean of most frequent occurrance data points.\n",
    "* Residuals >> the diffrence between actual and predicted values \n",
    "* residuals >> (Yactual - Ypred)\n",
    "* A normal distribution has some important properties\n",
    "1. The mean,median,mode all represent the centre of the distribution\n",
    "2. The distribution is a bell shaped curve \n",
    "3. 68% of the data falls within 1 standard deviation of the mean ,=95% of the data falls within 2std  of the mean \n",
    "and =99.7% of the data falls within 3 std of the mean.\n",
    "\n",
    "> How to check normality ?\n",
    "* graphs for normality test :\n",
    " 1. Distribution curve,histogram (sns.kdeplot,sns.histplot)\n",
    " 2. QQ or quantile - quantile plot\n",
    "\n",
    "* statiostical tests for normality (Hypothesis testing)\n",
    "1. Shapiro test\n",
    "2. kstest\n",
    "3. normal test\n",
    "\n",
    "> how to handle normality?\n",
    "1. Check and remove outliers\n",
    "2. Apply a nonlinear transformation to the independent and or dependent variables\n",
    "a. log transformation\n",
    "b. Square root transformation\n",
    "c. reciprocal transformation\n",
    "\n",
    "> What if these normality gets violated?\n",
    "* if the data points is far away from the regression line then it is Bad normal distribution.\n",
    "* It shows Non linear realtionship between dependent and independent variables\n",
    "* Residual error should be normally distributed\n",
    "1. KDE plot\n",
    "2. QQ plot\n",
    "3. Skewness residual (skew = 0 >> data is norally distributed)\n",
    "4. Hypothesis testing\n",
    "  * shapiro\n",
    "  * kstest\n",
    "  * normal test\n",
    "\n",
    "> skewness \n",
    "* skewness can be used to test for normality.\n",
    "* if skewness is not close to zero then your data set is not normally distributed\n",
    "* Skewness is a numerical indicator of how far a data sample data deviates from the normal distribution\n",
    "* The data is visually represented as a bell shaped curve with equal mean(average) and mode(highest value in the data set)\n",
    "* It is a normal distribution\n",
    "* Positive skewness >> tail of the distribution is longer towards the right hand side \n",
    "                    >> if the data distribution mean is greater than mode\n",
    "* Negative skewness >> tail of the distribution is longer towards the left hand side\n",
    "                    >> if the data distribution mean is smaller than the mode \n",
    "* Symmetrical data  >> -0.5 to +0.5 symmetrical distribution\n",
    "                    >> -1 to -0.5 >> negatively skewed distribution\n",
    "                    skew < -1  >> highly negatively skewed distribution\n",
    "                    >> 0.5 to 1 >> positively skewed data \n",
    "                    >> skew > +1 highly positive skewed data\n",
    "\n",
    "> Hypothesis testing \n",
    "* Any data science project starts with exploring the data \n",
    "* when we perform an analysis on a sample through exploratory data analysis and inferential statistics we get information about the sample.\n",
    "* \"Hypothesis testing\" is done to confirm our observations about the population using sample data,within desired error level.\n",
    "* Through hypothesis testing we can determine whether we have enough statistical evidence to conclude if the hypothesis testing about the population is true or not \n",
    "* The Hypothesis is defined as the proposed explaination based on sufficient evidence or assumptions\n",
    "* it is just a guess based on some known facts but has not yet been proven \n",
    "* A good hypothesis is testable which results in either true or false\n",
    "* In this example a scietist just claims that UV rays are harmful to the eyes but we assume that may cause blindness\n",
    "* however it may or not be possible . hence these types of assumptions are called as hypothesis .\n",
    "* The null hypothesis represented as Ho is the initial claim that is based on the prevailing belief about that population.\n",
    "* The alternate hypothesis represented as H1 is the challenge to the null hypothesis.\n",
    "* It is the claim which we would like to prove as True\n",
    "\n",
    "\n",
    "* Null hypothesis >> data is normally distributed\n",
    "* Alternate hypothesis >> data is not normally distributed \n",
    "* p_val range >> 0 to 1\n",
    "* 0.05 is significant value in hypothesis testing \n",
    "* p_val >= 0.05   >> we are accepting null hypothesis\n",
    "* p_val < 0.05    >> we are accepting alternate hypothesis\n",
    "* Shapiro >> Shapiro test is a statistical test used to check whether the consirded data is normally distributed or not.\n",
    "* In Shapiro test ,the null hypothesis is states that the population is normally distributed\n",
    "* If the P_val is greater than 0.05 then the null hypothesis is accepted\n",
    "\n",
    "> homoscedasticity\n",
    "* Residuals have constant variance at every level of x .This is known as homoscedasticity.\n",
    "* when this is not the case, the residuals are said to suffer from heteroscedasticity.\n",
    "* In the Homoscedasticity all the data points disperse within the range from starting of the regression line at the end.\n",
    "* So due to that error value in between that.\n",
    "* It shows linear relationship between independent and dependent variable .\n",
    "* the error terms must have constant variance for all the values of independent variables is known as homoscedasticity.\n",
    "* The presence of non consistent variance in the error results in heteroscedasticity\n",
    "* In the heterscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "* So due to that error value from at one point to another point gradually increase\n",
    "* It shows non linear relationship between independent and dependent variable\n",
    "* generally this heterscedasticity occurs bcoz of non constant variance aries in the presence of outliers or extreme leverage values .Look like these values get too much weight thereby disproportionately influences the models performance \n",
    "* When heteroscedasticity is present in the regression analysis the results of the analysis become hard to trust\n",
    "* In the Homoscedasticity all the data points disperse within the range from starting of the regression line to the end.\n",
    "* In the heteroscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "\n",
    "> How to check homoscedasticity ?\n",
    "* Scatter plot between fitted value and residual plot\n",
    "\n",
    "> How to Handle homoscedasticity ?\n",
    "* Transform the independent variable(Y)\n",
    "* Log transformation of the dependent variable\n",
    "* Redifine the dependent variable\n",
    "* Use weighted regression\n",
    "\n",
    "> what if these homoscedasticity get violated ?\n",
    "* The presence of non constant variance in the error terms results in heteroscedasticity\n",
    "* If we get violated homoscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "* So due to that error value ,from a one point to another point gradually increases.\n",
    "* It shows non linear relationship between the independent variable and the dependent variable\n",
    "* Generally this heteroscedasticity occurs due to non constant variance aries in the presence outliers or extreme levereage values.Look like these values get too much weight therby affecting the models performance \n",
    "\n",
    "> No autocorelation \n",
    "* There should be no corealtion between the residual error terms. Absence of this phenomenenon ,absence of this phenomenon is known as autocorelation.\n",
    "\n",
    "> Coefficient of corelation (corr) or Pearson correlation coefficient\n",
    "* Corelation coefficients are used to measure how strong a relationship is between two variables .\n",
    "* A correlation coefficient tells us the strength and direction of the relationship between the two variables.\n",
    "* Corelation coefficients shows how two variables related \n",
    "* Linear relationship between independent variables and dependent variable.\n",
    "* corr = covariance / prod of std\n",
    "* Corelation coefficients = covariance(x,y) / std(x) * std(y)\n",
    "* Corelation coefficients always lies between -1 to +1\n",
    "* where -1 represents X and Y are negatively correlated and +1 represents X and Y are positive correlated.\n",
    "* Where r is corelation coefficient\n",
    "* Best values of R >> -1 to +1\n",
    "* Positive Corr >> R is +ve\n",
    "* Negative Corr >> R is -ve\n",
    "* Good predictors >> R >= 0.7 OR R <= -0.7\n",
    "1 > R  > 0.7 >> very strong +ve Corelation coefficient\n",
    "-0.7 > R > -1 >> very strong -ve Corelation coefficient\n",
    "0.3 > R > -0.3 weak corr\n",
    "R = 1 >> indicates a strong +ve Corelation coefficient\n",
    "R = -1 >> indicates a strong -ve corelation coefficient \n",
    "R = 0 >> indicates no linear relationship\n",
    "\n",
    "> advantages of coefficient of corelation \n",
    "1. Its easy to workout and its easy to interpret\n",
    "2. Coefficient of corelation is an index of the strength of linear association between 2 variables\n",
    "\n",
    "> disadvantages of coefficient of corelation \n",
    "* Disadvantages of the corelation coefficient are that it only measures linear relationship between X and Y and for any relationship to exist ,any change in X has to have a constant proportional change in Y .\n",
    "* If the relationship is not linear then the result is not accurate \n",
    "* In addition to this the corelation is meaningless if it is about categorical data ,such as hair color or gender\n",
    "\n",
    "> feature selection technique \n",
    "* One of the technique is coefficient of corelation (R)\n",
    "* This is to select best feature \n",
    "independent variable x1,x2,x3\n",
    "dependent variable y \n",
    "* If x2 is highly correlated to y then x2 is the best feature.\n",
    "* And we never drop such features\n",
    "\n",
    "> Co variance \n",
    "* Covariance measures how two variables move with respect to each other\n",
    "* And is an extension of the concept of variance (which tells about how a single variable varies)\n",
    "* It can take any value from -inf to +inf higher this value more dependent is the relationship\n",
    "* Covariance is only dependent on sign\n",
    "* A positive value shows both variables move in same direction\n",
    "*  A negative value shows both variables move in opposite direction\n",
    "\n",
    "> difference between coefficients of corelation and covariance \n",
    "* coefficient of corelation\n",
    "1. Indicates direction and strength of linear relationship \n",
    "2. Positive corelation coefficient close to 1 indicates a strong positive corelation and value close to -1 indicates a strong negative corelation.\n",
    "3. Corelation coefficient can be -1 to +1.\n",
    "\n",
    "* Covariance \n",
    "1. indicates direction of linear relationship\n",
    "2. Positive covaraiance indicates an increase in one variable indicates an increase in other \n",
    "3. covariance can be between -inf to +inf\n",
    "\n",
    "> evaluation metrics for linear regression\n",
    "* After we train our machine learning model,its immportant to understand how well our model has preformed \n",
    "* Evaluation metrics are used for this purpose\n",
    "\n",
    "> regression evaluation metrics \n",
    "* Unlike classification where we measure a models performance by checking how correct its predictions are , in regression we check it by measuring the difference in predicted and actual values \n",
    "* our objective is to minimize the metric score in order to improve our model\n",
    " \n",
    "1. sum of squared error(SSE) or sum of squared residual (SSRs)\n",
    "* It is used to determine the accuracy of our model ,the lower the SSE more will be the accuracy of our model \n",
    "* SSE = sumation(Yactual - Ypredicted)^2\n",
    "\n",
    "2. sum of squared regression(SSR) or regression error \n",
    "* The sum of squared diffrerences between predicted data points and the mean of the reponse variable\n",
    "* SSR = sumation(Ypredicted - Ymean)^2\n",
    "\n",
    "3. Sum of Squares Total (SST)\n",
    "* SST = SSE + SSR\n",
    "\n",
    "4. Mean squared error \n",
    "* It is also known as cost function or loss function\n",
    "* Loss is the error in our predicted value of m and c\n",
    "* We will use the mean squared error function to calculate the loss\n",
    "* In statistics, the mean squared error measures how close our predicted values are to the observed values.\n",
    "* Mean squared error >> It is the mean of square of all errors\n",
    "* Our main role is to redue the value of MSE i.e minimize this error to obtain the most accurate values of m and c\n",
    "* So if we get less error we will get less MSE\n",
    "* MSE = sumation(Yactual - Ypredicted)^2 / N\n",
    "\n",
    "5. Root Mean Squared Error\n",
    "* Error metric allows us to track the various matrices efficiency and accuracy\n",
    "* The Metric of the attribute changes when we cslculate the error using MSE\n",
    "* For example : If the unit of a distance based attribute is meters (m) the unit of mean squared error will be m2 \n",
    "which would make the calcualtions confusing. In order to avoid these we use the root mean squared error\n",
    "\n",
    "6. Mean absolute error(MAE)\n",
    "* As the name suggests the mean absolute error can be defined as the mean of the sum of the absolute errors differences between the predicted and actual values.\n",
    "* MAE are metrics used to evaluate the regression model.These metrics tells us how accurate the predictions are and what is the amount of deviation between the predicted and actual values.\n",
    "* When outliers are present always use MAE because it is robust to outliers\n",
    "* MAE = sumation |Yactual - Ypredicted| / N\n",
    "\n",
    "> Why MAE is more used often when outliers are present?\n",
    "* MAE measures the absolute difference between the predicted and actual values\n",
    "* This means that MAE is less sensitive to outliers have less impact on the evaluation of the model performance.\n",
    "* Oultiers have large impact on MSE,RMSE as they are squared in the calculation\n",
    "* As a result the presence of outliers can lead to overestimation of the error and a misleading evaluation of the model performance.\n",
    "\n",
    "7. Root Mean absolute error\n",
    "* RMAE = sqrt(MAE)\n",
    "\n",
    "> R2 score \n",
    "* It is also known as coefficient of determination >> we can also say that that it tells about the goodness of BFL\n",
    "* R^2 value, R2 score , coefficient of determination\n",
    "* It is used to find goodness of BFL\n",
    "* Goodness >> It tells us about closeness of the data point to the regression line means lessn error.\n",
    "* If data points are close towards the regression line it means that we have good R2 score\n",
    "* Less the error the better the R2 score\n",
    "* R2 score = 1 - (SSE - SST)\n",
    "* 1 - (unexplained variation / total variation)\n",
    "* Explained variation / total variation\n",
    "* In some cases R2 score can be negative \n",
    "* R2 represents the variance in the target variable that is explained by the model \n",
    "* It ranges from 0 to 1,where 1 indicates that the model fits the data perfectly and \n",
    "* 0 indicates that the model doesnot explain any of the variance in the data\n",
    "* If the model is worse than simply using the mean value of the target variable as a predictor ,then R2 value can become negative.\n",
    "* In simple words a negative R2 score tells us that our model doesnot fit in the data and it performs worse than the a simple model that just predicts the mean of the dependent variable.\n",
    "* this can happen when the model is overfitting the data or when the model is too simple ,when the model is poorly designed , when the model cannot capture the complexity of the data , when the model doesnot capture any of the underlying patterns in the data .\n",
    "\n",
    "> Adjusted R2 score\n",
    "* More R2 value >> Better the model coz all data points close to the BFL >> less error >> more accuracy \n",
    "* R2 score = 1 >> Good R2 score >> all data points on the BFL \n",
    "* R2 score = 0 >> Bad R2 score >> all data points are far away from the BFL\n",
    "* If SSE increases R2 score decreases >> Bad score of R2\n",
    "* If SSE decreases R2 score increases >> Good R2 score\n",
    "* When new fetaures are added to the data the r2 score either increase or remains the same .\n",
    "* However adding features doenot allow guarntee a better performance for the model and r2 score fails to adequately \n",
    "capture the negative impact of adding a feature to the model i.e whether the feature actually improves model predictions or not .\n",
    "* In order to address this problem the adjusted R2 score is used .\n",
    "* Adjusted R2 score = 1 - [(1 - R^2) * (N - 1)] / (N - p - 1)\n",
    "* R2 score is always greater than Adjusted R2 score \n",
    "\n",
    "> For understanding purpose \n",
    "* where metrics output  a value between 0 to 1 and the score can be used to objectively judge a models performance \n",
    "however in regression the target variable may not always be in the same range eg : the price of the house can be 6 digits no but a student marks will always be in the range of 0 to 100\n",
    "* This means that the metric scores for marks will be mostly a 2 digit number but that for housing prices can be anything between a 1 to 6 digit number.\n",
    "* In simpler words an accuracy of 0.90 or 90% is a good performance but does an RMSE of 90 indicate good performance.\n",
    "for house prices which is a 6 digit number its a good score but for students marks.\n",
    "* it is a terrible one! predicting a value of 10 when the actual value is 100 is much different than predicting a value of 200,000 when the actual value is 200,090\n",
    "\n",
    "> Why the adjusted R2 score metric used ?\n",
    "* R2 score acts as a benchmark metric for judging a regression models performance irrespective of the range of the va;ues the target variables present. The range of R2 score is between 0 to 1 (R2 can be negative also)\n",
    "* The greater the R2 score the more our model performs\n",
    "* when new features are added to the data R2 score either increases or remains the same.\n",
    "* However adding new features to the data doenot always mean that new features are contributing towards increaing R2 score of the model, it can also gave negative effects on R2 score or accuracy of the model which R2 score is not able to predict.\n",
    "* In order to address this problem we use adjusted R2 score\n",
    "\n",
    "> Summary \n",
    "* Coefficient of Determination (R2 score)\n",
    "* The Coefficient of Determination is the square of coefficient of corelation\n",
    "* Corelation coefficient represents the relationship between 2 variables ,while R2 score is the coefficient of determination and represents the percentage of variation of the independent variables contribute in the variation of the dependent variable.\n",
    "* Used to find goodness of BFL \n",
    "* MSE is scale varient\n",
    "* R2 score is scale invariant\n",
    "* R2 score >> 0 to 1\n",
    "* R2 can be negative\n",
    "* R2 = 1 >> Good score >> all data points are on BFL \n",
    "* R2 = 0 >> Bad score >> all data points are far away from the BFL\n",
    "* R2 score = Explained variation / Total variation\n",
    "* R2 score = 1 - SSE / SST\n",
    "* R2 score will never decrease \n",
    "\n",
    "> Adjusted R2 score\n",
    "* it will increase only for good predictors \n",
    "* Adjusted R2 score will always be less than or equal to R2 score\n",
    "* Adjusted R2 score = 1 - [(1 - R2 score) * (n -1)] / (n - P - 1)\n",
    "\n",
    "> What is relationship between MSE,RMSE and MAE and its speciality?\n",
    "* MSE,RMSE and MAE are measures of the performance of a regression model\n",
    "* While R2 score and adjusted R2 score are the measures of finding the godness of BFL \n",
    "* Mean Squared Error (MSE) >> it is the average of the sqaured error between the predicted and actual values\n",
    "* It is a measure of how close the predictions with respect to the actual values\n",
    "* Lower MSE indicates good Model performance\n",
    "* MSE is Useful when the dataset have a few oultiers \n",
    "* Root Mean Squared Error (RMSE) >> It is the square root of MSE and has the same uits as the dependent variable\n",
    "* It is a measure of the typical size of the errors in the prediction \n",
    "* RMSE is useful when you want to express the errors in the same units as the dependent variable\n",
    "* Mean absolute error (MAE) >> it is the average of the absolute error between the predicted and actual values \n",
    "* It is also a measure to find how close our predictions are to the actual values\n",
    "* it is less sensitive to outliers as compared to MSE,RMSE\n",
    "* R2 score >> It is a measure of how well the independent variables explained the variation in the dependent variable.\n",
    "* It ranges from 0 to 1 with higher values indicating a better fit \n",
    "* R2 is useful for comparing differences models or variations of same model\n",
    "* Adjusted R2 score is a modified version of R2 score that adjusts for the no of independenet variabkes in the model.\n",
    "* It penalizes model that includes unnecessary variables that donot improve the fit \n",
    "* In summary MSE,RMSE and MAE are measures of the performance of the model \n",
    "* while R2 score and adjusted R2 score are measures of the goodness of BFL \n",
    "* By usimg these matrix we can evaluate the performance of the model and goodness of BFL \n",
    "* And Informed decision about how to improve them \n",
    "\n",
    "> When to use MSE,RMSE,MAE,R2 score ,adjusted R2 score\n",
    "* MSE is used when there are less outliers .\n",
    "* RMSE is used when you want to express errors in the same unit as dependent variable.\n",
    "* MAE is used when there are outliers present in the dataset .\n",
    "It is useful when you want to penalize all errors equally\n",
    "* R2 score is useful when you want to understand how much variance in the target variable \n",
    "* Adjusted R2 score is useful when you have unnecessary varaiables in the dataset and to get correct R2 score \n",
    "\n",
    "> Why RMSE mostly used instead of MSE and MAE ?\n",
    "* MSE emphasizes large error \n",
    "* MAE doesnot emphasize large error \n",
    "* RMSE emphasizes large error\n",
    "* MSE is more sensitive to ouliers \n",
    "* MAE is less sensitive to ouliers\n",
    "* RMSE is more sensitive to outliers \n",
    "* Units of MSE are not as same as the original data \n",
    "* Units of RMSE are same as original data \n",
    "\n",
    "* MSE is simple the average of the squared difference between the predicted values and actual values\n",
    "Because of squaring of the error the units of MSE are not same as the original data. This makes it difficult to compare the magnitude of errors with the original data . MSE metric is useful because it emphaizes large errors but the squared term can make the metric more sensitive to outliers\n",
    "* MAE calculates the average of the absolute differences between the predicted values and actual values which is less sensitive to outliers but doesnot emphasize large errors such as MSE\n",
    "* RMSE takes the sqrt of MSE\n",
    "* RMSE can be interpreted in the same unit as the original data and makes it easy to understand the magnitude of errors.\n",
    "* RMSE is more sensitive to outliers than MAE \n",
    "* This means that if there are large errors in the predictions,RMSE will penalize the model more heavily than MAE\n",
    "* This makes a better choice when we want to avoid large errors in the predictions\n",
    "* RMSE is a compromise between two metrics MSE and MAE because it takes the sqrt of the MSE which makes it less sensitive to outliers tham MSE but still emphasizes large errors more tha MAE.\n",
    "* In summary RMSE is a good metric to use becuase it balances the benefits of MSE and MAE in a simple and interpretable way.\n",
    "\n",
    "> Why RMSE and R2 score are being the most commonly used?\n",
    "* RMSE and R2 score are the most commonly used metrics in regression models because they are both easy to understand and provide valuable information about the performance of the model.\n",
    "* RMSE or Root Mean Squared Error is a measure of the average error between the predicted and actual values in the same units as the dependent variable\n",
    "* It is easy to interpret because it represents the tyical amount of errors we can except in our predictions.RMSE is commonly used because it takes into account both large and small errors and is therfore more sensitive to oultietrs or extreme values that can have significant effects on the performance of the model.\n",
    "* R2 score is the measure of how well the independent variables contribute to the dependent variable.\n",
    "* It ranges from 0 to 1 with higher values indicating best fit \n",
    "* R2 is easy to interpret because it tells us how well the model is able to capture the underlying realtionships between the variables.\n",
    "* R2 is commonly used because it is widely recognized measure of the goodness of fit of the model and is therefore useful for comparing different models or variations of the same model.\n",
    "* Overall R2 score and RMSE are both commonly used because they are easy to understand and provide valuable information about the performance of the model.\n",
    "* By these metrics we can quickly evaluate the performance of our regression models and make informed decisions about how to improve them.\n",
    "\n",
    "> Physical interpreted definition of standard deviation and variance\n",
    "> Standard deviation\n",
    "* Standard deviation is a measure of the dispersion (how much our data spread)\n",
    "* Standard deviation is the measure of dispersion of a set of data from its mean \n",
    "* Data Spreaded more >> More standard deviation\n",
    "\n",
    "> variance \n",
    "* It is a measure of the variability of the data from mean , this particular estimates how far the set of numbers are spread out from the mean value \n",
    "* On average how far each value lies from the mean \n",
    "\n",
    "> Standard deviation\n",
    "* The spread of group of data from its mean value \n",
    "* Standard deviation = sqaure root of the variance \n",
    "* Unit >> same units as the mean of the data \n",
    "\n",
    "> Variance \n",
    "* Spread of each data point from each other and its mean .\n",
    "* The Variance measures thew average degree to which each point differs from the mean\n",
    "* it helps determine the data spread size when compared to the mean value\n",
    "* more variance means more variation in data values means larger gap between one data point and another data point\n",
    "* variance = square of standard deviation\n",
    "* unit >> squared units as mean of the data\n",
    "\n",
    "> Gradient Descent algorithm\n",
    "* Gradient descent is a method of updating m and c values to minimize the cost function(MSE) and to get the BFL.\n",
    "* BFL >> In linear regression we try to find the BFL which passes through maximum data points and we get minimum error between predicted and actual data points.\n",
    "* BFL will always have least mean square errors \n",
    "* A regression model uses gradient descent to find the best values of m and c .By finding Best m and c values we can get as minimum error between the predicted and actual data points.\n",
    "* To update m and c values we take gradient from the cost function . to find these gradients we take partial derivatives for m and c values\n",
    "* Calculate the partial derivative of the loss function with respect to m,and plug in the current values of x,y,m and c in it to obtain the derivatives value D\n",
    "* GLobal minima >> It is a point that obtains the absolute lowest value of our function \n",
    "* Learning rate >> It determines the size of the steps that are taken by the gradient descent algorithm\n",
    "* If learning rate is very small it would take a very long time to converge and become computationally expensive\n",
    "* If alpha is very large it may fail to converge and overshoot the minimum\n",
    "* The most commonly used learning rate are >> 0.001 ,0.003,0.01,0.03,0.1,0.3\n",
    "\n",
    "* Main aim >> \"This Method is used to minimize the cost function/loss function/MSE\" and by using MSE we will gind the BFL.\n",
    "* There are many algorithms to minimize the cost function but majorily Gradient Descent used.\n",
    "* One of the best algorithms to minimize the cost function\n",
    "* Until the function is close to or equal to zero , the model will continue to adjust its parameters to yield the smallest possible error.\n",
    "* Gradient Descent algorithm gives best values of m and c for the linear regression equation\n",
    "* with these values of m and c we will get the equation of the best fit line and ready to make predictions\n",
    "* Equation of line >> y = mx + c\n",
    "\n",
    "> Steps in gradient descent \n",
    "1. It will directly assume the value of m and c value for 1st iteration (m >> slope , c >> intercept)\n",
    "2. By using m and c values in the equation of line we get the value of y \n",
    "* y = mx + c >> where y=dependent variable , x = independent variable , m = Slope,C= intercept\n",
    "* we will get the x and y value by using that it will draw the regrression line.\n",
    "* Then we claculate the MSE value \n",
    "3. it will calculate the m and c values for 2nd iteration by using the formula \n",
    "* M(new) = M(old) - Learning rate * slope \n",
    "* M(new) = M(old) - alpha * slope \n",
    "* C(new) = C(old) - alpha * slope\n",
    "* By using that we get m and c value\n",
    "* By using that we draw the 2nd regression line \n",
    "* Then we calculate the MSE value \n",
    "4. By same method we draw multiple regression line and we find out the MSE values infinite time \n",
    "5. Among all these line select 1 regression line having lowest MSE value and this line will be consirded as BFL \n",
    "\n",
    "> Finding the best fit line or regression line\n",
    "* When working with linear regression our main goal is to find the best fit line which has less errors between the preicted values and actual values\n",
    "* Best fit line will have least error (Yactual - Ypredicted) \n",
    "* The best fit line will have have the lowest MSE\n",
    "* Best fit line will pass through maximum data points\n",
    "* Best fit line will have best m and c values \n",
    "* Gradient descent algorithm finds one BFL from infinite number of possiblities\n",
    "* In Gradient descent algorithm we use local minima and global minima in order to decrease the loss function \n",
    "* The point in a curve which is minimum as compared to its preceding and successive points is called local minima.\n",
    "* The point in the curve which is minimum compated to all the points in the curve is called as Global minima.\n",
    "* For a curve there can be more than one local minima but we will have only one global minima\n",
    "* GLobal minima >> Point of convergence  >> where the cost function is at its minimum\n",
    "\n",
    "> Types of Gradient Descent \n",
    "1. Batch Gradient Descent\n",
    "2. Stochastic Gradient Descent\n",
    "3. Minibatch Gradient Descent\n",
    "\n",
    "> Regularization in MAchine learning\n",
    "* While training a machine learning model the model can be easily overfitted or underfitted\n",
    "* To avoid these problems we use regularization\n",
    "* Regularization techniques help reduce the possibility of overfitting and help us obtain an optimal model.\n",
    "\n",
    "> Overfitting and Underfitting\n",
    "* In real world we will never get the dataset will never be clean and perfect\n",
    "* This means that each dataset contains impurities ,noisy data ,outliers, missing data or imbalanced data . due to these impurities different problems occur that affect the accuracy and the performance of the model.\n",
    "* On eof such problems is Overfitting.\n",
    "\n",
    "* Noisy data >> Noisy data is meaningless data or irrelevant data present in the dataset\n",
    "* It affects the performance of the model if not removed from the dataset.\n",
    "\n",
    "* Generallization >> It shows how well a model is trained to predict the unseen data.\n",
    "\n",
    "* Bias >> It is the difference of actual and predicted values of training data.\n",
    "* Error which occurs in training data accuracy \n",
    "* Training error >> Yactual - Ypredicted\n",
    "* Less error >> less bias >> high accuracy\n",
    "* More error >> more bias >> less accuracy\n",
    "\n",
    "* Variance >> Difference between training accuracy and testing accuracy\n",
    "* If the machine learning model performs better on training data and not on testing data then variance occurs.\n",
    "* Low variance >> Higher accuracy \n",
    "* Higher variance >> Lower accuracy\n",
    "\n",
    "\n",
    "> Overfitting \n",
    "* Model performing well on training data and not on testing data is called \"overfitting.\n",
    "* Variance >> Difference between training accuracy and testing accuracy\n",
    "* Low bias and high variance is called as Overfitting.\n",
    "\n",
    "> Underfitting\n",
    "* Model is neither performing well on training data and not on testing data.\n",
    "* High Bias and Low variance are called as Underfitting\n",
    "* To avoid Underfitting\n",
    "1. Proper feature selection \n",
    "2. Hypertuning \n",
    "3. Add more features\n",
    "\n",
    "> Best model \n",
    "* Model is performing well on training data as well as testing data.\n",
    "* Low bias and low variance \n",
    "\n",
    "> How to avoid Over fitting ?\n",
    "1. Hyperparameter tuning\n",
    "* Grid search cv\n",
    "* Randomized search cv\n",
    "2. Train with more data i.e more data more efficiency >> more accuracy \n",
    "3. Reduce the number of features\n",
    "* Feature selection techniques\n",
    "* Dimensionality reduction techniques\n",
    "4. Pruning (Cutting Branches of Decision Tree) >> Tree based Model \n",
    "5. Regularization (L1 and L2) >> Only used in Linear model (Linear regression and Logistic regression)\n",
    "* L1 Regularization >> Lasso regression\n",
    "* L2 Regularization >> Ridge regression\n",
    "6. Cross validation\n",
    "* K-fold cross validation\n",
    "7. Remove outliers \n",
    "8. Early stopping \n",
    "\n",
    "\n",
    "> How to avoid Underfitting ?\n",
    "1. Increase the number of features\n",
    "* Add new features \n",
    "* Add derived features (Creating Multiple features from single features)\n",
    "2. Use proper feature selection \n",
    "* Filter methods\n",
    "* Wrapper method \n",
    "* Embedded method \n",
    "3. Hyperparameter tuning \n",
    "\n",
    "> Bias variannce tradeoff\n",
    "* we need to find a good balance between the bias and variance of the model we have used \n",
    "* This tradeoff in complexity is what as referred as bias and variance trtadeoff\n",
    "* An optimal balance of bias and variance should never overfit or underfit the model.\n",
    "* This tradeoff applies to all forms of supervised learning : Classification ,regression\n",
    "* Low BIas > the average prediction is very close to the target value \n",
    "* High Bias >> the predictions differ too much from the actual value \n",
    "* Low variance >> The data points are compact and donot vary too much from the their mean value\n",
    "* High Variance >> Scattered data points with huge variations from the mean value and other data points\n",
    "* To make a good fit we need to have a coorect balance of bias and variance\n",
    "\n",
    "> How to fix bias and variance problems?\n",
    "* Fixing high bias \n",
    "1. Adding more input features will help improve the data to fit better.\n",
    "2. Add more ploynomial features to improve the complexity of the model\n",
    "3. Decrease the regularization term to have a balance between bias and variance\n",
    "\n",
    "* Fixing high variance \n",
    "1. Reduce the input features , use more features importance to reduce the overfitting the data \n",
    "2. getting more training data will help in this case \n",
    "\n",
    "> K fold Cross Validation\n",
    "* L fold cv is where out training data is split into k number of sections/folds\n",
    "* where each fold we are split into training and testing dataset \n",
    "* After that we find out each value of R2\n",
    "* K-fold cv is where a given data set is split into a k number of sections/folds where each fold is used as testing set at same point.Lets take the scenario of 5 fold cross validation(k=5)\n",
    "* Here the dataset is split in 5 folds . In the first iteration , the first fold is used to test the model and the rest are used to train the model .in the second iteration 2nd fold is used as the testing set while the rest are used to train the data . This process is repeated until each fold of the 5 folds have been used as testing set.\n",
    "\n",
    "> Outliers \n",
    "* Data points which are far away from the observations.\n",
    "\n",
    "1. How Ouliers are introduced in data ?\n",
    ">\n",
    "1. Data entry error >> human error \n",
    "2. Measurement error >> Machine error / Instrument error\n",
    "3. Intentional error >> Dummy dataset\n",
    "4. Sampling error >> Mixing of data from wrong resorces \n",
    "5. Natural error >> Most of the data from wromg resources \n",
    "6. Experiment error >> data extraction error\n",
    "7. Data processing error >> Data manipulation error \n",
    "\n",
    "2. Impact of outliers ?\n",
    "1. Reduce the power statistical analysis.(on mean value and std)\n",
    "2. High impact on mean value and std \n",
    "3. Agorithms donot perform well in presence of outliers. (Accuracy,precision,recall,mse)\n",
    "4. Impact on basic assumptions of regression (normality,homoscedasticity)\n",
    "5. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training time ,less accurate models and ultimately poor results.\n",
    "\n",
    "3. How to detect outliers ?\n",
    "1. Z-score \n",
    "2. IQR\n",
    "3. Boxplot\n",
    "4. Scatterplot\n",
    "\n",
    "4. How to handle oultiers ?\n",
    "1. delete observation \n",
    "2. Imputation >> Replace outliers with \n",
    "* Median\n",
    "* Mean \n",
    "* minimium value \n",
    "* maximium value \n",
    "* upper tail\n",
    "* lower tail\n",
    "* any static value \n",
    "* Zeros \n",
    "3. Transformation >> used to reduce the impact of outliers\n",
    "* Log transformation\n",
    "* Normalization (0 to 1 range)\n",
    "* Standardization (-3 to +3 ideal range)\n",
    "* Cuberoot transformation\n",
    "* reciprocal transformation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
