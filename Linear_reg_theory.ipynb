{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear regression \n",
    "* It is a predictive model used to find linear relationships between dependent and one or more independent variables.\n",
    "* Which says that it finds outhow the value of dependent variable changes according to the value of independent variable.\n",
    "* It is a parameteric algorithm >> assumptions on distribution of data.\n",
    "* \"Our aim in linear regression is to find the linear relationship between dependent and one or more independent variables.\"\n",
    "* In linear regression we find the best fit line that can accurately predict the value of dependent variable.\n",
    "\n",
    "> Types of linear regression\n",
    "1. Simple linear regression \n",
    "* If a single independent variable is used to predict the value of dependent variable then such type of linear regression is called Simple linear regression.\n",
    "* i.e. we have only one independent variable\n",
    "* y = mx + c\n",
    "m >> slope - linear regression coefficients (scale factor to each input value)\n",
    "c >> intercept of line (gives an additional degree of freedom)\n",
    "x >> Independent variable (Predictor Variable)\n",
    "y >> dependent variable\n",
    "\n",
    "2. Multiple linear regression\n",
    "* If we use more than one independent variable to predict the value of dependent variable then such type of linear regression is called Multiple linear regression.\n",
    "* we have one or more independent variable\n",
    "* Y = m1x1 + m2x2 + c\n",
    "* Y = m1x1 +m2x2 + ....+mnxn + c\n",
    "m1,m2,m3 >> slope \n",
    "x1,x2,x3 >> Independent variables\n",
    "Y >> dependent variables\n",
    "slope(m) >> (Y2 - Y1) / (X2 - X1)\n",
    "\n",
    "> Residuals or error or coefficient of regression(e):\n",
    "* Distance between actual value and predicted value is called as residual.\n",
    "* If the observed points are far away from the regression line ,then the residual will be high and so cost function will be also high.\n",
    "* If the observed points are close to the regression line ,then the residual will be low and so cost function will be also low.\n",
    "* e = Yactual - Ypredicted\n",
    "* Yactual - Actual value of Y >> Original value .. data point\n",
    "* Ypredicted - Predicted value of Y >> value according to the regression line\n",
    "\n",
    "> Linear regression line \n",
    "* A linear line showing linear relationship between dependent variable and independent variables is called as linear regression line.\n",
    "\n",
    "* Positive linear relationship : (R=1)\n",
    "* If we increase our value in independent variable and accordingly the value of dependent variable increases,then such relationship will be called as positive linear relationship.\n",
    "* Negative linear relationship : (R=-1)\n",
    "* If we decrease our value in independent variable and accordingly the value of dependent variable decreases,then such relationship will be called as negative linear relationship.\n",
    "\n",
    "> Advantages of linear regression \n",
    "1. Simple to implement and easier to interpret the output coefficients\n",
    "2. when you know the there is linear relationship between dependent variable and independent variables this algorithm is best to use and it is less complex as compared to other algorithms.\n",
    "3. Linear regression is prone to overfitting but it can be avoided using dimensionality reduction techniques,regularization (L1 and L2) techniques and cross validation.\n",
    "\n",
    "> Disadvantages of linear regression\n",
    "1. If the independent variables are corelated it may affect the performance \n",
    "2. It is only efficient for linear data (High Corr between x and y) \n",
    "3. Sometimes a lot of feature enginnering is required.\n",
    "4. Scaling is required >> predictors have a mean of 0 \n",
    "5. It is often quite prone to noise and overfitting.\n",
    "6. It is sensitive to missing values\n",
    "7. It is sensitive to outliers \n",
    "\n",
    "> Applications of Linear regression \n",
    "1. Forecasting the data \n",
    "2. Analyzing the time series \n",
    "3. Prize prediction \n",
    "4. Salary prediction \n",
    "\n",
    "> Assumptions of Linear regression \n",
    "1. Linearity >> linear relationship between the independent variables and dependent variables\n",
    "2. NoMultiCoLinearity >> All the independent variables should be independent to each other.\n",
    "3. Normality of residual >>\n",
    "4. Homoscedasticity >> variance of residual is same or constant at every level of x or for any value of x\n",
    "\n",
    "1. Linearity >> There should be a linear relationship between the independent variables and dependent variables.\n",
    "* There should be a linear relationship between the independent variables and dependent variables \n",
    "* A linear relationship tells us how 1 unit difference in independent variable affects the dependent variable.\n",
    "* An additive relationship suggests that effect of independent variable on dependent variable is independent of other independent variable.\n",
    "* In linear regression a straight line will be passed through maximum data points as good as possible.\n",
    "* As the line will be passed through maximum data points our coefficient of regression will be minimum.\n",
    "* The line shows linear relationship between independent variable and dependent variable.\n",
    "\n",
    "> How to check linearity?\n",
    "* Coefficient of corelation \n",
    "* scatterplot\n",
    "* Corelation matrix\n",
    "\n",
    "> How to handle linearity if violated?\n",
    "* Apply nonlinear transformation to the independent variable and/or dependent variable\n",
    "* Log transformation\n",
    "* sqrt transformation\n",
    "* reciprocal transformation\n",
    "\n",
    "> What if these linearity get violated?\n",
    "* if linearity gets violated then we will see that the regression line is not passing through maximum data points whcih says that doenot fulfill the requirements of linearity.\n",
    "* Due to which our coefficient of regression (error) will be maximum\n",
    "* It shows non linear relationship between the independent variable and the dependent variable.\n",
    "* Which will result in bad prediction of result so we cannot use Linear regression algorithm\n",
    "\n",
    "> No multicolinearity\n",
    "* All the independent variables are independent to each other is called as Nomulticolinearity.\n",
    "* There should be no linear relationship between the independent variables .\n",
    "* Multicolinearity happens when the independent variables are linearly correlated with each other.\n",
    "* This means that one independent variable can be predicted from another independent variable.\n",
    "* Multicolinearity can be a problem in a regression model because we would not be able to distinguish the individual effects of independent variable on dependent variable.\n",
    "* Y = M1X1 + M2X2 + C\n",
    "* Coefficient M1 is the increase in Y for a unit increase in X1 while keeping X2 constant.\n",
    "* But Since X1 and X2 are corelated changes in X1 will be reflected in X2 .\n",
    "* Multicolinearity doesnot affect the accuracy of the model but we will not be able to distinguish the effects of independent variable on dependent variable.\n",
    "* Mulitcolinearity means high corelation between two or more independent variables\n",
    "* Due to multicolinearity it may be difficult to distinguish the effects of independent variable on dependent variable.\n",
    "* Or we can say it is difficult to determine which independent variable is affecting the dependent variable and which is not .\n",
    "* So the model assumes either little or no multicolinearity between the features or the independent variables.\n",
    "* y = (m1 * x1) + (m2 * x2) + (m3 * x3) + ....... +(mn * xn) + C\n",
    "* If 2 independent variables are strongly correlated with each other then the X1 and X2 values become same or almost same ,regression model will not be able to determine M1 and M2 that means our regression model is unstable \n",
    "* Multcolinaerity is measured by VIF(Variance inflation factor)\n",
    "* VIF starts at 1 and has no upper limit \n",
    "* if VIF =1 no corelation between the independent variables\n",
    "* VIF exceeding 5 or 10 indicates high multcolinaerity between the independent variables\n",
    "\n",
    "> What if these nomulticolinearity gets violated?\n",
    "* If multicolinaerity available it may be difficult to find the true relationship between the independent variables and dependent variable.or we can say that it is difficult to determine which independent variables is affecting dependent variable and which are not.\n",
    "\n",
    "> How to detect multicolinaerity?\n",
    "* VIF(Variance inflation factors)\n",
    "* corelation matrix / corelation plot\n",
    "* scatter plots \n",
    "\n",
    "* The VIF score of independent variable represents how well the variable is explained by other independent variables.\n",
    "* VIF = 1 / (1 - R^2)\n",
    "* VIF = 1 >> No corealtion \n",
    "* VIF = 1 to 5 >> Moderate corelation\n",
    "* VIF > 10  >> High corelation\n",
    "\n",
    "> If any Multicolinaerity present in your model then how it can affect your model?\n",
    "* It will reduce the statistical major or power of your model .Efficiency of prediction will reduce .\n",
    "\n",
    "> How to handle this ?\n",
    "* dropping variables \n",
    "* combining variables\n",
    "* If there is high corelation in independent variables then \n",
    "1. Remove or drop the highly correlated variables(99% of the time we donot remove data as data is important)\n",
    "2. Linearly combine these 2 highly correlated variables\n",
    "3. Principle component analysis(PCA) technique .It is also know as dimensionality reduction technique\n",
    "4. Lasso Regression\n",
    "\n",
    "> Normal Distribution of the error (normality of residual)\n",
    "* if the data points are near the regression line the better the normal distribution of data .\n",
    "* It shows Linear strong relationship between dependent and independent variables.\n",
    "* Normal distribution >> How much our data points are closer or near to the mean of most frequent occurrance data points.\n",
    "* Residuals >> the diffrence between actual and predicted values \n",
    "* residuals >> (Yactual - Ypred)\n",
    "* A normal distribution has some important properties\n",
    "1. The mean,median,mode all represent the centre of the distribution\n",
    "2. The distribution is a bell shaped curve \n",
    "3. 68% of the data falls within 1 standard deviation of the mean ,=95% of the data falls within 2std  of the mean \n",
    "and =99.7% of the data falls within 3 std of the mean.\n",
    "\n",
    "> How to check normality ?\n",
    "* graphs for normality test :\n",
    " 1. Distribution curve,histogram (sns.kdeplot,sns.histplot)\n",
    " 2. QQ or quantile - quantile plot\n",
    "\n",
    "* statiostical tests for normality (Hypothesis testing)\n",
    "1. Shapiro test\n",
    "2. kstest\n",
    "3. normal test\n",
    "\n",
    "> how to handle normality?\n",
    "1. Check and remove outliers\n",
    "2. Apply a nonlinear transformation to the independent and or dependent variables\n",
    "a. log transformation\n",
    "b. Square root transformation\n",
    "c. reciprocal transformation\n",
    "\n",
    "> What if these normality gets violated?\n",
    "* if the data points is far away from the regression line then it is Bad normal distribution.\n",
    "* It shows Non linear realtionship between dependent and independent variables\n",
    "* Residual error should be normally distributed\n",
    "1. KDE plot\n",
    "2. QQ plot\n",
    "3. Skewness residual (skew = 0 >> data is norally distributed)\n",
    "4. Hypothesis testing\n",
    "  * shapiro\n",
    "  * kstest\n",
    "  * normal test\n",
    "\n",
    "> skewness \n",
    "* skewness can be used to test for normality.\n",
    "* if skewness is not close to zero then your data set is not normally distributed\n",
    "* Skewness is a numerical indicator of how far a data sample data deviates from the normal distribution\n",
    "* The data is visually represented as a bell shaped curve with equal mean(average) and mode(highest value in the data set)\n",
    "* It is a normal distribution\n",
    "* Positive skewness >> tail of the distribution is longer towards the right hand side \n",
    "                    >> if the data distribution mean is greater than mode\n",
    "* Negative skewness >> tail of the distribution is longer towards the left hand side\n",
    "                    >> if the data distribution mean is smaller than the mode \n",
    "* Symmetrical data  >> -0.5 to +0.5 symmetrical distribution\n",
    "                    >> -1 to -0.5 >> negatively skewed distribution\n",
    "                    skew < -1  >> highly negatively skewed distribution\n",
    "                    >> 0.5 to 1 >> positively skewed data \n",
    "                    >> skew > +1 highly positive skewed data\n",
    "\n",
    "> Hypothesis testing \n",
    "* Any data science project starts with exploring the data \n",
    "* when we perform an analysis on a sample through exploratory data analysis and inferential statistics we get information about the sample.\n",
    "* \"Hypothesis testing\" is done to confirm our observations about the population using sample data,within desired error level.\n",
    "* Through hypothesis testing we can determine whether we have enough statistical evidence to conclude if the hypothesis testing about the population is true or not \n",
    "* The Hypothesis is defined as the proposed explaination based on sufficient evidence or assumptions\n",
    "* it is just a guess based on some known facts but has not yet been proven \n",
    "* A good hypothesis is testable which results in either true or false\n",
    "* In this example a scietist just claims that UV rays are harmful to the eyes but we assume that may cause blindness\n",
    "* however it may or not be possible . hence these types of assumptions are called as hypothesis .\n",
    "* The null hypothesis represented as Ho is the initial claim that is based on the prevailing belief about that population.\n",
    "* The alternate hypothesis represented as H1 is the challenge to the null hypothesis.\n",
    "* It is the claim which we would like to prove as True\n",
    "\n",
    "\n",
    "* Null hypothesis >> data is normally distributed\n",
    "* Alternate hypothesis >> data is not normally distributed \n",
    "* p_val range >> 0 to 1\n",
    "* 0.05 is significant value in hypothesis testing \n",
    "* p_val >= 0.05   >> we are accepting null hypothesis\n",
    "* p_val < 0.05    >> we are accepting alternate hypothesis\n",
    "* Shapiro >> Shapiro test is a statistical test used to check whether the consirded data is normally distributed or not.\n",
    "* In Shapiro test ,the null hypothesis is states that the population is normally distributed\n",
    "* If the P_val is greater than 0.05 then the null hypothesis is accepted\n",
    "\n",
    "> homoscedasticity\n",
    "* Residuals have constant variance at every level of x .This is known as homoscedasticity.\n",
    "* when this is not the case, the residuals are said to suffer from heteroscedasticity.\n",
    "* In the Homoscedasticity all the data points disperse within the range from starting of the regression line at the end.\n",
    "* So due to that error value in between that.\n",
    "* It shows linear relationship between independent and dependent variable .\n",
    "* the error terms must have constant variance for all the values of independent variables is known as homoscedasticity.\n",
    "* The presence of non consistent variance in the error results in heteroscedasticity\n",
    "* In the heterscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "* So due to that error value from at one point to another point gradually increase\n",
    "* It shows non linear relationship between independent and dependent variable\n",
    "* generally this heterscedasticity occurs bcoz of non constant variance aries in the presence of outliers or extreme leverage values .Look like these values get too much weight thereby disproportionately influences the models performance \n",
    "* When heteroscedasticity is present in the regression analysis the results of the analysis become hard to trust\n",
    "* In the Homoscedasticity all the data points disperse within the range from starting of the regression line to the end.\n",
    "* In the heteroscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "\n",
    "> How to check homoscedasticity ?\n",
    "* Scatter plot between fitted value and residual plot\n",
    "\n",
    "> How to Handle homoscedasticity ?\n",
    "* Transform the independent variable(Y)\n",
    "* Log transformation of the dependent variable\n",
    "* Redifine the dependent variable\n",
    "* Use weighted regression\n",
    "\n",
    "> what if these homoscedasticity get violated ?\n",
    "* The presence of non constant variance in the error terms results in heteroscedasticity\n",
    "* If we get violated homoscedasticity all the data points disperse unequal range from starting of the regression line to the end.\n",
    "* So due to that error value ,from a one point to another point gradually increases.\n",
    "* It shows non linear relationship between the independent variable and the dependent variable\n",
    "* Generally this heteroscedasticity occurs due to non constant variance aries in the presence outliers or extreme levereage values.Look like these values get too much weight therby affecting the models performance \n",
    "\n",
    "> No autocorelation \n",
    "* There should be no corealtion between the residual error terms. Absence of this phenomenenon ,absence of this phenomenon is known as autocorelation.\n",
    "\n",
    "> Coefficient of corelation (corr) or Pearson correlation coefficient\n",
    "* Corelation coefficients are used to measure how strong a relationship is between two variables .\n",
    "* A correlation coefficient tells us the strength and direction of the relationship between the two variables.\n",
    "* Corelation coefficients shows how two variables related \n",
    "* Linear relationship between independent variables and dependent variable.\n",
    "* corr = covariance / prod of std\n",
    "* Corelation coefficients = covariance(x,y) / std(x) * std(y)\n",
    "* Corelation coefficients always lies between -1 to +1\n",
    "* where -1 represents X and Y are negatively correlated and +1 represents X and Y are positive correlated.\n",
    "* Where r is corelation coefficient\n",
    "* Best values of R >> -1 to +1\n",
    "* Positive Corr >> R is +ve\n",
    "* Negative Corr >> R is -ve\n",
    "* Good predictors >> R >= 0.7 OR R <= -0.7\n",
    "1 > R  > 0.7 >> very strong +ve Corelation coefficient\n",
    "-0.7 > R > -1 >> very strong -ve Corelation coefficient\n",
    "0.3 > R > -0.3 weak corr\n",
    "R = 1 >> indicates a strong +ve Corelation coefficient\n",
    "R = -1 >> indicates a strong -ve corelation coefficient \n",
    "R = 0 >> indicates no linear relationship\n",
    "\n",
    "> advantages of coefficient of corelation \n",
    "1. Its easy to workout and its easy to interpret\n",
    "2. Coefficient of corelation is an index of the strength of linear association between 2 variables\n",
    "\n",
    "> disadvantages of coefficient of corelation \n",
    "* Disadvantages of the corelation coefficient are that it only measures linear relationship between X and Y and for any relationship to exist ,any change in X has to have a constant proportional change in Y .\n",
    "* If the relationship is not linear then the result is not accurate \n",
    "* In addition to this the corelation is meaningless if it is about categorical data ,such as hair color or gender\n",
    "\n",
    "> feature selection technique \n",
    "* One of the technique is coefficient of corelation (R)\n",
    "* This is to select best feature \n",
    "independent variable x1,x2,x3\n",
    "dependent variable y \n",
    "* If x2 is highly correlated to y then x2 is the best feature.\n",
    "* And we never drop such features\n",
    "\n",
    "> Co variance \n",
    "* Covariance measures how two variables move with respect to each other\n",
    "* And is an extension of the concept of variance (which tells about how a single variable varies)\n",
    "* It can take any value from -inf to +inf higher this value more dependent is the relationship\n",
    "* Covariance is only dependent on sign\n",
    "* A positive value shows both variables move in same direction\n",
    "*  A negative value shows both variables move in opposite direction\n",
    "\n",
    "> difference between coefficients of corelation and covariance \n",
    "* coefficient of corelation\n",
    "1. Indicates direction and strength of linear relationship \n",
    "2. Positive corelation coefficient close to 1 indicates a strong positive corelation and value close to -1 indicates a strong negative corelation.\n",
    "3. Corelation coefficient can be -1 to +1.\n",
    "\n",
    "* Covariance \n",
    "1. indicates direction of linear relationship\n",
    "2. Positive covaraiance indicates an increase in one variable indicates an increase in other \n",
    "3. covariance can be between -inf to +inf\n",
    "\n",
    "> evaluation metrics for linear regression\n",
    "* After we train our machine learning model,its immportant to understand how well our model has preformed \n",
    "* Evaluation metrics are used for this purpose\n",
    "\n",
    "> regression evaluation metrics \n",
    "* Unlike classification where we measure a models performance by checking how correct its predictions are , in regression we check it by measuring the difference in predicted and actual values \n",
    "* our objective is to minimize the metric score in order to improve our model\n",
    " \n",
    "1. sum of squared error(SSE) or sum of squared residual (SSRs)\n",
    "* It is used to determine the accuracy of our model ,the lower the SSE more will be the accuracy of our model \n",
    "* SSE = sumation(Yactual - Ypredicted)^2\n",
    "\n",
    "2. sum of squared regression(SSR) or regression error \n",
    "* The sum of squared diffrerences between predicted data points and the mean of the reponse variable\n",
    "* SSR = sumation(Ypredicted - Ymean)^2\n",
    "\n",
    "3. Sum of Squares Total (SST)\n",
    "* SST = SSE + SSR\n",
    "\n",
    "4. Mean squared error \n",
    "* It is also known as loss function or cost function\n",
    "* The loss is the error in our predicted value of m and c\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
