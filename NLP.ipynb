{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Natural Language Processing\n",
    "* It is a branch of Artificial Intelligence.\n",
    "* Whenever text data will come NLP will come into picture.\n",
    "* The human capability of understanding a language will be given to our model by NLP.\n",
    "# Subsections \n",
    "1. NLU (Natural language understanding)\n",
    "2. NLG  (Natural language generation) >> We generate new text\n",
    "* Example > If we have watched a movie and if somebody ask us about the movie .If we tell about the script of the movie then this is called as NLU and if we are telling about the movie in our own language then this is called as NLG.\n",
    "* If we are taking as it from the original data then that is called as NLU and if we are generating new text from the original data then this is called as NLG.\n",
    "# Basic terms of any language\n",
    "1. Phonemes >> Smallest unit of any language ,characters , speech , sound \n",
    "2. Morphemes(words) & lexemes(run-running , swim-swimming)\n",
    "3. Syntax >> Phrases , sentences \n",
    "4. Context \n",
    "* meaning\n",
    "* Combinations of Syntaxes \n",
    "> NLP applications \n",
    "1. Sentiment analysis >> It is a classification problem (text classification).\n",
    "* Tweets >> Positive tweets , negative tweets , Neutral tweets \n",
    "* Movie reviews >> Positive reviews , negative reviews , Neutral reviews\n",
    "* In Sentiment analysis we analyze the sentiment of the text data whether it is positive or negative or neutral.\n",
    "* Sentiment analysis is quite important as we are dealing with product based and service based industry interactions between customers is crucial and we need to understand the review of the product or service which we are providing.We should be able to understand the sentiment of the text data which will be helpful for the industry to make necessary changes to satisfy the customer.\n",
    "* Surveys , google forms , audio files \n",
    "2. Document classification  \n",
    "* In this type we will get documents to classify.\n",
    "* In case of 2 documents it will be Binary classification and in case of multiple documents it will be Multiclass classification.\n",
    "* Adhar card , PAN card , Driving license , Voter ID \n",
    "3. Text Summarization \n",
    "* If we get 50 page data and we want to summarize it into 1 page.\n",
    "> Extractive Text Summarization \n",
    "* In Extractive Text Summarization we extract important lines/sentences and will return the summary of the original data.\n",
    "> Abstractive Text Summarization\n",
    "* Creating new summary (NLG)\n",
    "4. Topic Modelling / Topic Identification\n",
    "* Suppose we have 100 documents of data and we have 20 documents related to sports , 30 documents related to geography , and 50 documents related to ethics and we dont know that.If we build a model on Topic modeling and this will find out the hidden patternsbetween text from the 100 documents.\n",
    "* This model will return output as 20 documents related to 0 class , 30 documents related to 1 class , and 50 documents related to 2 class.\n",
    "* And then we will need to manually classify the headings of the documents.\n",
    "* Model will classify the documents based on similar words or sentences in different clusters.\n",
    "5. Chatbot \n",
    "* Automatic response generation \n",
    "* \n",
    "# NLP Pipeline \n",
    "> Data Extraction > EDA > Preprocessing > feature engineering > modelling > Evaluation > Deployment > again from data extraction cycle will continue \n",
    "1. Data extraction\n",
    "* We can get data in different data formats >> JSON , Text , CSV , Images(OCR , Pytesseract , Amazon textract , Google vision)\n",
    "> Data types \n",
    "* Public data >> easily available\n",
    "* Private data >> belongs to some organization\n",
    "> If data is not available \n",
    "* We take data from Public options >> Webscrapping \n",
    "> If data is available\n",
    "* Then we will download data.\n",
    "* In this way we get data.\n",
    "> What is noise in the text data?\n",
    "* \"We >>> loved $@ /\\ # the product will ___ defientlri recommend\"\n",
    "* Noise is available in above example.\n",
    "> Quality of data \n",
    "> If there is noise in the text data then what we would do?\n",
    "* We will process on extreme data preprocessing.\n",
    "> If there is minimal noise then what we would do?\n",
    "* Minimal data preprocessing\n",
    "* \n",
    "* After this we will get clean data.\n",
    "> Quantity of data?\n",
    "* In case of POC suppose we have 1GB of data that it is a good thing.\n",
    "* \n",
    "> Problems regarding data?\n",
    "* Quantity of data \n",
    "* Quality of data\n",
    "* Exact data / Specific data is not available for our use case.\n",
    "* Donot have continuous flow of data.\n",
    "* In Industry we donot get data in single time we will get data in cycles.\n",
    "* Monthly cycle , quarterly cycle(3 months) , yearly cycle\n",
    "* \n",
    "2. EDA \n",
    "> Ngram \n",
    "> Word cloud\n",
    "> Keyphrase extraction \n",
    "* \n",
    "> Ngram \n",
    "* Unigram \n",
    "* Bigram \n",
    "* Trigram \n",
    "* Quadragram \n",
    "> Suppose we have a example \"Rajesh is a hardworking guy.\"\n",
    "* Unigram = [Rajesh , is , a , hardworking , guy]\n",
    "* Bigram = [Rajesh is , is hardworking ,hardworking guy]\n",
    "* Trigram = [Rajesh is hardworking , is hardworking guy]\n",
    "> Why Ngram?\n",
    "* It is a part of EDA.\n",
    "* To get insights from the data(understanding words).\n",
    "* Suppose we are considering positive reviews  we will get >> Positive words by Ngram.\n",
    "* Also when we are considering negative reviews we will get >> Negative words by Ngram.\n",
    "* \n",
    "* To get domain specific stopwords.\n",
    "* \n",
    "> Word cloud\n",
    "* If a frequency of any word is higher than the word font will be higher in that word cloud.\n",
    "* If a frequency of any word is lower than the word font will be lower in that word cloud.\n",
    "* \n",
    "> Key Phrase extraction \n",
    "* To extract important keyphrase or keywords\n",
    "* RAKE ,YAKE Algorithm used for key phrase extraction.\n",
    "* \"We are learning NLP\" and we apply key phrase extraction\n",
    "* \"Learning NLP\" will be important key word.\n",
    "* \n",
    "4. Preprocessing\n",
    "> Tokenization : \n",
    "* Sentence tokenization\n",
    "* Word tokenization\n",
    "* Suppose we get text \"We are learning NLP.NLP is a huge domain.\"\n",
    "* Sentence tokenization : [We are learning NLP. , NLP is a huge domain.]\n",
    "* Word tokenization : [We, are , learning , NLP , . , NLP , is , a , huge , domain , .]\n",
    "> How does Sentence tokenization work or how does it knows it is a sentence?\n",
    "* It will check syntax.\n",
    "* it will check for Punctuation marks >> ! , . , ? \n",
    "* It will check for Consumptions >> and , but \n",
    "* \n",
    "> Normalization : \n",
    "* Suppose we have words \"G R E A T\" and another word \"great\" these 2 words have the same meaning but our model does not understand them.Model will allocate number for \"G R E A T\" and another number will be allocated for \"great\".Which is not Good.\n",
    "* So in NLP we will convert this in single case depends upon us we keep it in lowercase or Uppercase.In Industry standard practice is in Lowercase.\n",
    "* \n",
    "> Remove Punctuation/Symbols \n",
    "* We have string library we import punctuation\n",
    "* In this punctuation string we have multiple symbols.\n",
    "* And we can use this to remove punctuation.\n",
    "* \n",
    "> Remove Stopwords \n",
    "* Language specific stopwords : words in text which contributes grammatically but doenot have impact on sentence.   is , has , we , him\n",
    "* Domain specific stopwords : words in domain text which contributes grammatically but doenot have impact on sentence. doctor ,tablet , capsule , treatment\n",
    "* \"Rajesh is suffering from cancer.Right now Doctor Pravin is treating him.We have given him XYZ tablet.\"\n",
    "* \n",
    "> Lemmatization(lemma) & stemming(stem) : Words pruining \n",
    "* Lemma and stem are greak words \n",
    "* lemma >> meaningful root word >> has dictionary in backend(word net dictionary) : \n",
    "* example >> Running >> It will check the word in word net dictionary whether it is meaningful or not >> Is there any other root word available in word net dictionary >> Output = run.\n",
    "* \n",
    "* Stem >> Root word >> aggressive prunner : example >> running = run , swimming = swim. \n",
    "* Believe = Beli which is wrong as it is a aggressive prunner.\n",
    "* \n",
    "* We should use both lemma and stem and compare the results.\n",
    "* \n",
    "> Contraction mapping : text expanding \n",
    "* didn't >> did not , doesn't >> does not , haven't >> have not \n",
    "* Supose we have review : \"I didn't like the movie\" , \"I liked the movie\" and we remove the stopwords then we will get as \"like movie\" and \"liked movie\".\n",
    "* In first case we are getting the wrong interpretation.which is not good so we are contraction mapping.\n",
    "* We can remove stopwords from stopwords list.\n",
    "* \n",
    "> Handling accented characters :\n",
    "* We have unidecode library to handle accented characters.\n",
    "* Accented characters >> s : $ , a : @ ,\n",
    "* We will convert $ to s and @ to a.\n",
    "* \n",
    "> Autocorrection : \n",
    "* Correct spelling of words \n",
    "* We have autocorrect library and text blob library \n",
    "* \n",
    "5. Feature Engineering\n",
    "* We convert text to numeric format/Vectors in feature engineering of NLP.\n",
    "* Converting text to numeric format/Vectors is called as word embedding.\n",
    "> Word embedding \n",
    "* Frequency based word embedding : It focuses on word frequency and returns numeric format\n",
    "* Count Vectorizer \n",
    "* TFIDF\n",
    "* \n",
    "* Prediction based word embedding : It uses algorithm in backend and convert text to numeric format.\n",
    "* Word to weight \n",
    "* Fast text \n",
    "* Doc2Ve \n",
    "* \n",
    "6. Modelling \n",
    "* We have data in Numeric format.\n",
    "* We can use all classification algorithms\n",
    "* Logistic regression , SVM , Random Forest , AdaBoost , Naive Bayes , Decision Tree classifier, RNN , LSTM.\n",
    "* \n",
    "7. Evaluation\n",
    "* Confusion matrix, accuracy score , precision ,recall , F1-score \n",
    "* If we have low accuarcy we will jump on preprocessing or go for feature enginnering.\n",
    "* Suppose we have used frequency based first and now we will use prediction based for better result.\n",
    "* \n",
    "8. Deployment \n",
    "* If we get good accuracy then we will go for Deployment.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
