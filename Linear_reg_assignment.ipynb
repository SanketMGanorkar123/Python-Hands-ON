{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "* It is a predictive model used to find linear relationships between independent variables and dependent variable.\n",
    "* Which means that linear regression is used to find how dependent variable changes with a change in independent variables.\n",
    "* It is a parametric algorithm >> assumptions on distribition of data\n",
    "* our main aim of linear regression is to find out the linear relationship between independent variables and dependent variable.\n",
    "* The Goal of the linear regression is to find the best fit line that can accurately predict the output fot the continous dependent variable.\n",
    "\n",
    "> Types of Linear Regression\n",
    "1. Simple Linear Regression\n",
    "* If a single independent variable is used to predict the the value of numerical independent variable then such a linear regression algorithm is called Simple Linear Regression.\n",
    "* Only one independent variables.\n",
    "* Y = mX + C\n",
    "* m >> Slope >> Linear Regression coefficient \n",
    "* C >> Intercept of the line \n",
    "* X >> Independent variable\n",
    "* Y >> Dependent variable\n",
    "\n",
    "2. Multiple Linear Regression\n",
    "* If more than one independent variable is used to predict the value of numerical dependent variable then such a linear regression is called as Multiple linear regression.\n",
    "* Two or more independent variables\n",
    "> Y = m1X1 + m2X2 + m3X3 + C\n",
    "* m1,m2,m3 are the slopes of independent variable \n",
    "* X1,X2,X3 are the independent variable\n",
    "* Y >> Dependent varaiable \n",
    "* Slope >> m = (Y2 - Y1) / (X2 - X1)\n",
    "\n",
    "> Residuals or errors or coefficient of regression (e)\n",
    "* The distance between the actual and predicted values is called as residuals.\n",
    "* If the observed points are far away from the regression line then the residual will be high and so cost function will be also high.\n",
    "* If the observed points are near the regression line then the residual will be low and so cost function will be also low.\n",
    "> e = Yactual - Ypredicted\n",
    "* Yactual >> actual value of Y >> original value  >> data point\n",
    "* Ypredicted >> predicted value of Y >> value according to the regression line \n",
    "\n",
    "> Linear regression line \n",
    "* A linear line showing the relationship between the independent variables and dependent variable is called a linear regression line. A linear regression line can show 2 types of regression line.\n",
    "1. Positive linear relationship >> R = 1 \n",
    "* If the dependent variable increases by increasing the values independent variable then such a relationship is called as a positive linear relationship.\n",
    "2. Negative linear relationship >> R = -1\n",
    "* If the dependent variable decreases by decreasing the values independent variable then such a relationship is called as a negative linear relationship.\n",
    "\n",
    "> Advantages of Linear regression \n",
    "1. Simple to implement and easier to interpret the output coefficients\n",
    "2. When you know there is a linear relationship between independent variables and dependent variables then this is the best algorithm is best to use as compares to other complex algorithm\n",
    "3. Linear regression is prone to overfitting to avoid these problem we use dimensionality reduction technique , regularization(L1 and L2),cross validation \n",
    "\n",
    "> Disadvantages of linear regression \n",
    "1. If the independent features are corelated to each other then it may affect the performance of the model.\n",
    "2. It is only efficient if there is a linear relationsio between the independent variable and dependent variable .\n",
    "3. Sometimes a lot of feature engineering is required.\n",
    "4. Scaling is required \n",
    "5. It is often prone to noise and overfitting\n",
    "6. It is sensitive to missing values \n",
    "7. It is sensitive to outliers\n",
    "\n",
    "> Applications of linear regression\n",
    "1. Forecasting the data\n",
    "2. Analyzing the time series\n",
    "3. Price prediction\n",
    "4. Salary prediction\n",
    "\n",
    "> Assumptions of linear regression\n",
    "1. Linearity >> Linear relationship between the independent variable and dependent varaiable \n",
    "2. No multicolinearity >> All the independent variable should be independent to each other.\n",
    "3. Normality of residual \n",
    "4. Homoscedasticity >> Variance of residual is same or constant at every level of X\n",
    "\n",
    "\n",
    "> Linearity \n",
    "* There should be linear relationship between the independent variable and dependent variable.\n",
    "* A linear relationship between independent variable and dependent varaible says that a unit change in Independent variable there is a unit change in dependent varaiable \n",
    "* In linear regression a straight line will be passed through maximum data points.\n",
    "* So due to that Coefficient of reression is minimum.\n",
    "* It shows strong linear relationship between the independent varaiable and dependent variable\n",
    "\n",
    "> How to check Linearity ?\n",
    "1. Coefficient of corelation \n",
    "2. Scatterplot\n",
    "3. Corelation matrix\n",
    "\n",
    "> How to handle linearity if violated ?\n",
    "* Apply a non linear transformation to the independent and dependent variables.\n",
    "1. Log transformation\n",
    "2. Square root transformation\n",
    "3. Reciprocal transformation\n",
    "\n",
    "> What if these linearity get violated?\n",
    "* If our linearity gets violated then it says that our best fit line doenot pass through the maximum data points which says that there in no linear relationship between the independent and dependent variables and due to which our coeffiient of regression is maximum . It Shows non linear relationship between the independent and dependent variables resulting in inefficent model this will result in wrong predictions of output.\n",
    "\n",
    "> No Multcolinearity \n",
    "* All the independent variables should be independent to each other is called NoMultcolinearity \n",
    "* There should be no linear relationship between two independent variables.\n",
    "* Multicolinearity >> It occurs when independent variables are corelated to each other.\n",
    "* If there is Multicolinearity that means that an independent variable can be predicted from another independent variable.\n",
    "* Multicolinearity can be a problem in a regression model because we would not be able to distinguish the effects of the independent variable on dependent variables.\n",
    "* Y = m1X1 + m2X2 + m3X3 + C\n",
    "* Coefficient m1 is the increase in Y for a unit increase in X1 while keeping X2 constant \n",
    "* But since X1 and X2 are highly corelated to each other changes in X1 would also cause changes in X2 and we would not be able to see their individual effects on Y.\n",
    "* Multicolinearity may not affect the accuracy of the model as much .But we might lose reliability in detrmining the effects of individual features in your model and that can be a problem when it comes to interpretability .\n",
    "* Multicolinearity means high corelation between two or more independent variables.\n",
    "* Due to multcolinearity it may be difficult to find the true realtionship between the independent variables and the dependent variable.\n",
    "* So the model assumes either little or no multicolinearity between the independent variables.\n",
    "* Regression equation >> \n",
    "> Y = m1X1 + m2X2 + m3X3 + C\n",
    "* If 2 independent variables are independent on each other then changes in X1 will be affected on X2 and , as m1 is the coefficent where m1 is the increase in Y for a unit increase in X1 while keeping X2 constant ,above condition will be false as changes in X1 are affected on X2 so model will face problems for calculating m1 and m2 values .\n",
    "* It is difficult to determine which independent variable is affecting the dependent variable and which is not .\n",
    "* So it is important of no multicloinearity in our module .\n",
    "\n",
    "* Tolerance >> T = 1 - R^2   >> R^2 >> coefficient of determination\n",
    "* If T < 0.1  >> Multicolinearity is available \n",
    "* Variance inflation factor >> VIF = 1/(1 - R^2)\n",
    "* If VIF >= 10    >> Multicolinearity is available \n",
    "* If VIF <= 4     >> No Multicolinearity available\n",
    "* VIF starts with 1 and has no upper limit \n",
    "* If VIF = 1 >> there is no corelation between the independent variables\n",
    "* If VIF > 5 or 10 Indicates high multicolinearity between the independent variables and the others.\n",
    "\n",
    "> What if these No multicolinearity get violated?\n",
    "* If multicolinearity is available it may be difficult to determine the true relationship between the Independent and dependent variable. \n",
    "* Or we can say that we cannot determine the individual effects of the independent variables on dependent variable.\n",
    "* In this case our slope M will be less precise \n",
    "\n",
    "> How to detect Multicolinearity ?\n",
    "1. VIF (Variance Inflation factor)\n",
    "2. Corelation matrix\n",
    "3. Scatter plots\n",
    "\n",
    "> VIF (Variance Inflation factor)\n",
    "* The VIF of a independent varaiable represents how well the variable is explained by other independent variables \n",
    "* VIF = 1 / (1 - R^2)\n",
    "* VIF = 1 >> No corelation \n",
    "* VIF = 1 to 5 >> moderate corelation \n",
    "* VIF > 10  >> High Corealtion\n",
    "\n",
    "> If any Multicolinearity present in your model then how it can affect your model?\n",
    "* It will reduce the power of your model. \n",
    "* Power >> Efficiency of prediction is reduce \n",
    "\n",
    "> How to handle this?\n",
    "1. Dropping variables \n",
    "2. Combining variables \n",
    "\n",
    "* If there is high corelation between the idependent variables then at that time \n",
    "1. Remove / Drop the highly corelated variables >> Which doesnot happen often as data is important \n",
    "2. Lienarly combine these 2 highly corelated variables \n",
    "3. Principal Component analysis >> It is also known as Dimensionality reduction technique >> Unsupervised learning algorithm by reducing the columns \n",
    "4. Lasso regression\n",
    "\n",
    "> Normal distribution of the error (Normality of residual (Error))\n",
    "* If the data point is closer to the regression line then the better normal distribution of data.\n",
    "* It shows linear relationship between the independent variables and dependent variable.\n",
    "* Normal distribution >> How much our data points are closer or near to the mean of most frequent occurance data point. \n",
    "* Residuals >> Diffrence between the actual values and predicted values \n",
    "\n",
    "* A normal distribution has some important properties \n",
    "1. The Mean ,median and mode all represent the centre of the distribution \n",
    "2. The distribution is a bell shape.\n",
    "\n",
    "> How to check Normality ?\n",
    "1. Graphs for normality test    \n",
    "   1. Distribution curve ,histogram (sns.kdeplot,sns.distplot)\n",
    "   2. Q-Q or Quantile-Quantile plot \n",
    "2. Statistical tests for normality (Hypothesis testing)\n",
    "   1. Shapiro test\n",
    "   2. ks test\n",
    "   3. Normal test\n",
    "\n",
    "> How to handle normality ?\n",
    "1. Check and remove outliers \n",
    "2. Apply a nonlinear transformation to the independent variables and dependent variable\n",
    "    * Log transformation\n",
    "    * Square root transformation\n",
    "    * Reciprocal transformation\n",
    "\n",
    "> What if these normality gets violated?\n",
    "* If the data point are far away from the regression line then it is a bad normal distribution of data .\n",
    "* It shows nonlinear relationships between the independent variables and the dependent variable.\n",
    "* Residual (Error) should be normally distributed\n",
    "1. kdeplot\n",
    "2. QQ plot\n",
    "3. Skewness of residual (Skew = 0 >> Data is normally distributed)\n",
    "4. Hypothesis testing \n",
    "   1. Shapiro test\n",
    "   2. ks test\n",
    "   3. Normal test\n",
    "\n",
    "\n",
    "* QQ plot >>  If the data is normally distributed the points will fall on 45 degree reference line.\n",
    "          >>  If the data is not normally disrtibuted the points will deviate from the reference line.\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "> Skewness \n",
    "* Skewness can be used to test for norrmality.\n",
    "* If skewnes is not close to zero then your dataset is not normally distributed.\n",
    "* Skewness is a numerical indicator of how far data sample deviates from the normal distribution \n",
    "* The data is visually represented as a bell shaped curve with equal mean an dmode in a normal distribution \n",
    "* Positive skewness >> Tail of the distribution is longer towards right hand side of the distribution.\n",
    "* Negative skewness >> Tail of the distribution is longer towards left hand side of the distribution.\n",
    "* Symmterical data \n",
    "* -0.5 to +0.5   >> Symmetrical distribution \n",
    "* -1 to -0.5     >> Negatively skewed distribution \n",
    "* skew < -1      >> Highly negatively skewed data \n",
    "* +0.5 to 1      >> positively skewed data \n",
    "* skew > +1      >> Highly positively skewed data \n",
    "\n",
    "> Hypothesis testing \n",
    "* \"Hypothesis testing\" is done to confirm our observations from the population using sample data within the desired error level.\n",
    "* Through Hypothesis testing we ca determine whether we have enough statistical evidence to conclude if the hypothesis about the population is true or not.\n",
    "* Hypothesis is defined as proposed explaination based on insufficent evidence or assumptions \n",
    "* It is just a guess based on some known facts but has not yet been proven.\n",
    "* In this example a scientist just claims that UV rays are harmful to the eyes but we assume that it may cause blindness .However it may or may not be possible.hence these types of assumptions are called as hypotheses.\n",
    "* Null hypothesis testing >> Data is normally distributed \n",
    "* Alternate hypothesis testing >> Data is not normally distributed \n",
    "* p_val range >> 0 to 1\n",
    "* 0.05 is significant value in hypothesis testing \n",
    "* if p_val >= 0.05    >> null hypothesis is True \n",
    "* If p_val <  0.05    >> Alternate hypothesis is True \n",
    "\n",
    "> Homoscedasticity \n",
    "* Residuals have constant variance at every level of x.This is known as Homoscedasticity.\n",
    "* When this is not the case the residuals are said to suffer from heteroscedasticity.\n",
    "* In Homoscedasticity all the data points disperse within the range from starting point of the regression line to the end.\n",
    "* It shows Linear relationship between the Independent variables and the dependent variable.\n",
    "* The error terms should have constant variance at every level of x is known as Homoscedasticity\n",
    "* The presence of non constant variance in the error terms results in Heteroscedasticity.\n",
    "* In Heteroscedasticity all the data points disperse unequal range from starting point of the regression line to the end.\n",
    "* It shows non linear relationship between Independent Variables and Dependent Variables.\n",
    "* Generally this Heteroscedasticity occurs due to presence of non constant variance in the error terms which occurs due to the presence of Outliers or extreme leverage values.\n",
    "* These values sometimes get too much weight whivh result on decrease accuarcy of the model.\n",
    "* When Heteroscedasticity is present in your regression analysis the results of the analysis become hard to trust .\n",
    "\n",
    "> How to check Homoscedasticity?\n",
    "* Scatter plot between fitted value and residual plot.\n",
    "\n",
    "> How to handle Homoscedasticity ?\n",
    "1. Tranform the dependent variable >> log transformation of the dependent variable\n",
    "2. Redefine the dependent variable\n",
    "3. Use weighted regfression >> this type of regression assigns a weight to each data point based on the variance of its fitted value.\n",
    "\n",
    "> What if these Homoscedasticity get violated?\n",
    "* The presence of non constant variance in the error terms results in Heteroscedasticity.\n",
    "* If we get violated Homoscedasticity all the data points disperse unequal range from the starting of the regression line to the end.\n",
    "* So due to that error value from one point to another point gradually increases.\n",
    "* It shows non linear relationship between the independent variables and dependent varaiables \n",
    "* Generally Heteroscedasticity occurs because of the non constant varaiance which occurs due to outliers.\n",
    "* Look like these values get too much weight thereby influences the model performance.\n",
    "\n",
    "> No Autocorelation \n",
    "* There should be no corelation between the residual errors absence of this phenomenon is known as autocorelation.\n",
    "* Autocorelation usually occurs if there is dependency between the residual errors\n",
    "\n",
    "> Coefficient of Corelation (corr) or Pearson's Correlation?\n",
    "* Correlation coefficients are used to measure how strong a relationship between the variables.\n",
    "* A Correaltion Coefficients tells us the strength and direction of a relationship between variables.\n",
    "* Corelation of coefficients shows how 2 variables are related .\n",
    "* Linear relationship between Independent variables and dependent varaiables.\n",
    "* There are several types of corelation coefficients but the most popular is Pearson's coefficeient of correlation \n",
    ">> R-value \n",
    "> corr = Covariance / (Prod of std)\n",
    "\n",
    "> r = sumation[(Xi - Xmean) (Yi - Ymean)] / sqrt[sumation[(Xi - Xmean)^2] sqrt[sumation[(Xi - Xmean)^2 sumation[(Yi - Ymean)^2]]]]\n",
    "\n",
    "> Correlation coefficient(r) = Covariance(x,y) / std dev(x) * std dev(y)\n",
    "* Correlation coefficient always lies between -1 to +1 \n",
    "* Where -1 represents X and Y are negatively correlated and +1 represents X and Y are Positively correlated.\n",
    "* R = 0.7 to 1 >> very strong +ve relationship\n",
    "* R = -0.7 to -1 >> very strong -ve relationship\n",
    "* R = -0.3 to +0.3 >> weak correlation \n",
    "* R = 1 >> Indicates a strong +ve relationship\n",
    "* R = -1 >> Indicates a strong negative relationship.If one variable increases other variable decreases.\n",
    "* R = 0  >> It means there is no linear relationship between 2 variables.\n",
    "\n",
    "> Advantages of Coeffient of Correlation\n",
    "1. It is easy to work out and its easy to interpret \n",
    "2. Coeffient of Correlation is an index of the strength of linear relationship between 2 variables.\n",
    "\n",
    "> Disadvantages of Coefficient of Correlation \n",
    "* Diadvantages of Coeffiient of Correlation is that it only measures linear relationship between X and Y for any relationship to exist any change in X has to have constant proprtional change in Y \n",
    "* If the relationship is not linear then the result is not accuarate.\n",
    "\n",
    "> Feature Selection Technique \n",
    "* One of the best technique is coefficient of correlation (r)\n",
    "* This is used to select best feature \n",
    "\n",
    "> Covariance \n",
    "* Coavariance measures how two variables move with respect to each other and is an extension of the concept of variance (which tells us how a single variable varies)\n",
    "* It can take any value from \"-inf to + inf\" higher this value more dependent is the relationship\n",
    "* Covariance is only dependent upon sign \n",
    "* A positive values shows both variable moves in same direction\n",
    "* A Negative values shows both variable moves in opposite direction\n",
    "* Covariance is a measured used to determine how much variable change in randomly.\n",
    "* The Covariance is a product of the units of two variables. The value of covariance lies between -inf to +inf.\n",
    "* Cov (x,y) = sumation[(Xi - Xmean) (Yi - Ymean)] / N\n",
    "\n",
    "> Difference between Corelation of coefficient and variance \n",
    "* Covariance indicates direction of linear relationship while R indicates direction and strength of linear relationship.\n",
    "* Positive Covariance indicates dependent and independent variables move in same direction while positive R shows positive linear relationship between variables.\n",
    "* Covariance can be between -inf to +inf and R is between -1 to +1\n",
    "\n",
    "> Evalauation metrics for linear regression \n",
    "* After we train our machine learning model its important to understand how well our model has performed. Evaluation metrics are used for this purpose.\n",
    "\n",
    "> Regression evaluation metrics \n",
    "* Unlike classification where we measure a models performance by checking how correct its prediction are , in regression we check its performance by measuring the difference in predicted and actual values.\n",
    "* Our objective is to minimize the score in order to improve the model.\n",
    "\n",
    "> Sum of squared errors (SSE) or sum of squared residual (SSRs)\n",
    "* Sum of Squared errors is an accuracy measure where the errors are squared then added.\n",
    "* It is used to determine the accuracy of the forecasting model when the data points are similar in magnitude.\n",
    "* The Lower the SSE the more accuarcy of the forecast.\n",
    "* SSE = sumation[(Yactual - Ypredicted)^2]\n",
    "\n",
    "> Sum of Squared regression (SSR) or regression error \n",
    "* The sum of squared difference betwen the predicted data points and the mean of the reponse varaiable(Y).\n",
    "* SSR = sumation[(Ypredicted - Ymean)^2]\n",
    "\n",
    "> Sum of Squares total (SST)\n",
    "* The sum of squared difference between actual data points and mean of the response varaiable (Y).\n",
    "* SST = SSE + SSR\n",
    "\n",
    "> Mean squared error \n",
    "* It is also known as cost function or loss function \n",
    "* The loss is the error in our predicted value of m and c.\n",
    "* We will use the mean squared error function to calculate the loss.\n",
    "* In statistics Mean squared error measures how close predicted values are to the observed points.\n",
    "* It measures the mean of the sum of squares of error difference between the actual and predicted values . we often used residual to refer to these individual differences.\n",
    "* MSE is the mean of the squares of all errors.\n",
    "* Our main role is to the reduce the value of MSE by minimizing this error to obtain the most accurate of m and c .\\\n",
    "* If we get less error or residual then the less MSE value.\n",
    "* MSE = sumation[(Yactual - Ypred)^2] / N    >>> MSE = MEan of SSE\n",
    "* Root mean squared error \n",
    "\n",
    "> Root mean squared error (RMSE)\n",
    "* If the unit of distance based attribute is meters(m) the unit of mean squared error will be m^2 ,which would make calculations confusing.In order to avoid this we use the root of mean squared error.\n",
    "* RMSE = sqrt(MSE)\n",
    "\n",
    "> Mean absolute error \n",
    "* As the name suggests the mean absolute error can be defined as the mean of the sum of absolute error differences between the predicted and actual values.\n",
    "* MAE are metrics used to evaluate a regression model.These metrics tell us how accuarte our predictions are what is the amount of deviation from the actual values.\n",
    "* When outliers are present then always use MAE because it is robust to Outliers.\n",
    "* MAE = sumation [|Yactaul - Ypredicted|] / N\n",
    "\n",
    "> Why MAE is more used when in data outliers are avialable ?\n",
    "* MAE measures the absolute differnces between the predicted and actual values.\n",
    "* This means that MAE is less sensitive to outliers have less impact on the evaluation of the models performance.\n",
    "* Outliers have large impact on MSE and RMSE as they are squared in the calculation.\n",
    "* As a result the presence of outliers can lead to overestimation of the error and a misleading evaluation of the models performance.\n",
    "\n",
    "> R2 score \n",
    "* It is also known as Coefficient of determination. It can be determined to find the goodness of the BFL.\n",
    "* All are same >> R2 score \n",
    "               >> Coefficient of determination \n",
    "* It is used to find the goodness of BFL.\n",
    "* Goodness >> It tells us how much closeness is there between the data points and the regression line which means less error .\n",
    "* If data points are close to the regression line thata means we have good R2 score \n",
    "* The less we have error the better the value of R2 score.\n",
    "* R2 score = 1 - (SSE / SST)\n",
    "* R2 score = 1 - (Uexplained variation / Total variation)\n",
    "* R2 score = (Explained variation / Total variation)\n",
    "* R2score is between 0 to 1.\n",
    "* R2 score can be negative.\n",
    "* But in some cases it is possible for the R2 score to become negative.\n",
    "* R2 score represents the variance in the target variable that is explained by the model.\n",
    "* It ranges from  0 to 1 where 1 indicates that the model fits the data perfectly and 0 indicates that the model doesnot fit the model perfectly.\n",
    "* If the model is worse than simply using the mean value of the target varaible as predictor then R2 value can become negative.\n",
    "* In simple words a negative R2 score indicates that the model is not a good fit for the data and it performs worse than simple model that just predicts the mean of the deoendent variable.\n",
    "* This can happen when the model is overfitting the data or when the model is too simple when the model is poorly designed when the model cannot capture the complexity of the data and when the model doenot capture the underlying patterns in the data.\n",
    "\n",
    "> Adjusted R2 score \n",
    "* R2 = 1 >> Good score >> all the data points are on BFL\n",
    "* R2 = 0 >> Bad score >> data points are far away from BFL\n",
    "* When new features are added to the data the R2score either increases or remains same.\n",
    "* However adding new features doenot always mean that it will increase the accuarcy or performance of the model and R2 SCore fails to show changes to capture the negative effects of such features.\n",
    "* To avoid these we use adjusted R2 score.\n",
    "\n",
    "> Adjusted R2 score = 1 - [(1- R^2) (N - 1) / (N - p -1)] \n",
    "* R2 score is always greater than adjusted R2 score\n",
    "* R2 score acts a benchmark metric for judging the performance of the model irrespective of the range of values the target variable presents.The range of R2 score is between 0 and 1 (R2 score can be negative)\n",
    "* The Greater the R2 score the better our models performance \n",
    "* when new features are added to the data the R2 score either increases or remains same.\n",
    "* However adding new features to the data doenot always guarantee a better performance of the model and the R2 score fails to adequately capture the negative effects of adding features to our model.\n",
    "* In Order to address this problem we use adjusted R2 Score\n",
    "\n",
    "> Summary \n",
    "* Coefficient of determonation (R2 score) \n",
    "* Coefficient of determination is square of Coefficient of Correlation\n",
    "* Coefficient of corelation indicate the relatioship between the variables while the R2 score is the coefficient of  determination and represents the percentage of the variation of the independnt variables contribute in the variation of the depedent varaible .\n",
    "* Used to find the goodness of BFL.\n",
    "* MSE is scale varient \n",
    "* R2 score is scale invarient.\n",
    "* R2score >> range between 0 and 1 \n",
    "* r2 score can be negative \n",
    "* R2 score = 1   >> Good score >>> all data points are on BFL \n",
    "* R2 score = 0   >> Bad score  >>> data points are away from BFL\n",
    "* R2 = Expalined variation / Total variation\n",
    "\n",
    "> Adjusted R2 score \n",
    "* It will increase only for good predictors \n",
    "* Adjusted value will always be less than r2 score.\n",
    "* Adjusted R2 scotre = 1 - [(1 - R^2) * (N - 1) / (N - p -1)]\n",
    "\n",
    "> What is relationship between MSE,RMSE,MAE and its speciality?\n",
    "* MSE,RMSE,MAE are the measures of the performance of the model while R2 score and adjusted R2 score are measures to find the goodness of BFL.\n",
    "* MSE is the average of the squared errors between the predicted and actual values.\n",
    "* It is a measure of how close the predictions are to actual values.\n",
    "* Lower MSE indicating better performance.\n",
    "* MSE is useful when the dataset has a few outliers.\n",
    "* RMSE is the sqrt of MSE and has the same units as dependent varaiable .It is the measure of the typical size of the errors in the predictions.\n",
    "* RMSE is useful when we want to express errors in the same unit as the target varaiable.\n",
    "* MAE is the average of the absolute error betweent the predicted and actual values.It is also a measure of how gthe actual values are close to predicted values on the regression line.\n",
    "* R2 SCore is a measure of how well the independent varaiables explain the varaiation in the dependent varaiable.\n",
    "* It ranges from values between the values from 0 to 1 with higher values indicating better fit.\n",
    "* R2 score is useful for comparing different models or variations of the same model.\n",
    "* Adjusted R2 score is a modeified version of R2 score that adjusts for the number of independnt varaibles in the model.It penalizes models that include unnecessary varaiables that donot improve the fit and it useful for comparing different models with different numbers of independent varaiables.\n",
    "* In summary MSE,RMSE,MAE are the measures to check the performance of the model while R2score and adjusted R2score are the measures of the goodness of fit.\n",
    "* By using these metrics we can evaluate the performance of the model and goodness of fit of our regression model amd make informed decision about how to improve them.\n",
    "\n",
    "> When to use MSE,RMSE,MAE,R2 score,Adjusted R2score?\n",
    "* MSE is useful when the dataset has a less outliers.\n",
    "* RMSE is useful when you want to express units of the errors as the same unit as the target variable.\n",
    "* MAE is useful when dataset has more outliers.It is useful when you want to penalize all errors equally.\n",
    "* R2 score is useful when yo want to measure how well the indepdent varaiables expalin the variation in the dependent varaiable.\n",
    "* Adjusted R2 score is useful when you want to check how irrelevant independent variables are affecting the performance of the model.\n",
    "\n",
    "> Why RMSE mostly used instead of MSE and MAE ?\n",
    "* RMSE is a good metric because it balances the benefit of MSE and MAE.\n",
    "* MSE emphazies large error.\n",
    "* MAE doesnot emphazies large error.\n",
    "* RMSE emphazies large error \n",
    "\n",
    "* MSE is more sensitive to outliers than RMSE\n",
    "* MAE is less sensitive to outliers\n",
    "* RMSE is more sensitive to outliers than MAE\n",
    "* Units of MSE are not same as units of Target variable.\n",
    "* units of RMSE are same as the units of Target variable.\n",
    "\n",
    "* MSE is simply the average of the squared differences between the actual and predicted values.Because of squaring of errors the units of MSE are not as same as the original data.\n",
    "* This makes it difficult to compare the magnitude of the errors with the original data.\n",
    "* MSE metric is useful because it emphazises larger errors but the squared terms can make it sensitive to outliers.\n",
    "* MAE is the average of the absolute difference between the actual and predicted values .Which is less sensitive to oultiers but doesnot emphazies large error as such as MSE\n",
    "* RMSE is the square root of MSE \n",
    "* RMSE can be interpreted in the same unit as the original data amd makes it easier to understand the magnitude of errors.\n",
    "* RMSE is more sensitive to Outliers than MAE\n",
    "* This means that if there are large errors in the predictions RMSE will penalize the model more heavily than MAE\n",
    "* This makes RMSE a better choice when we want to avoid large errors in our predictions.\n",
    "* RMSE is a comparison between two metrics MSE and MAE because it takes the square root of the MSE which makes it less sensitive to outliers than MSE but still emphazies large errors more than MSE.\n",
    "* In summary RMSE is a good metric to use it balances the benefits of MSE and MAE.\n",
    "\n",
    "> Why RMSE and R2 being the most commonly used ?\n",
    "* RMSE and R2score are two of the most commonly used metrics in regression analysis becauase they are both easy to understand and provide valuable information about the performance of the model.\n",
    "* RMSE is the root mean squared error is a measure of the average error between actual and predicted values in the same units as the dependent variable.\n",
    "* It is easy to interpret because it represents the typical amount of error we can expect in our predictions.\n",
    "* RMSE is commonly used as it takes into account both small error and large error and is therefore more sensistive to outliers or extreme values that can have significant impact on performance of the model.\n",
    "* R2 score is a measure of how well the independent varaibles contribute in the dependent variable.\n",
    "* It ranges from values between 0 to 1 with higher values indicating better fit.\n",
    "* R2 score is easy to interpret because it tells us how well the model is able to capture the underlying relationships between the variables.\n",
    "* R2 score is commonly used technique to find the Goodness of BFL and is therefore useful for comparing different models or variations of the same model.\n",
    "* Overall RMSE and R2 score are the most commonly used because they are easy to understand and provide valuable information about the performance of the model.\n",
    "* By using these metrics we can quickly evaluate the performance of our regression models and make informed decisions about how to improve them.\n",
    "\n",
    "> Physical interpreted definition of Standard deviation and variance \n",
    "> Standard deviation\n",
    "* Standard deviation is the measure of the dispersion (How much our data spread) >> extend to which distribution is stretched relative to the mean of the data point.\n",
    "* Standard deviation is a measure of dispersion of a set of data from its mean.\n",
    "* Data spreaded more  >>>>   More standard deviation\n",
    "\n",
    "> Variance \n",
    "* It is a measure of variability of data from its mean.It tells how data is varying from each other from the mean of the data point.\n",
    "* On average how far each value lies from the mean.\n",
    "\n",
    "> Standard deviation \n",
    "* The spread of froup of data from its mean.\n",
    "* Standard deviation = square root of varaiance \n",
    "* Unit >> same unit as the mean of the data \n",
    "\n",
    "> Variance \n",
    "* Spread of each data point from each other and its mean.\n",
    "* It helps detrmine the data spread size when compared to mean value.\n",
    "* More variance means more variation in data values means larger gap between one data point and another data point.\n",
    "* Variance = Square of standard deviation \n",
    "* Unit >> Squared units as mean of the data \n",
    "\n",
    "\n",
    "> Gradient Descent algorithm \n",
    "* Gradient descent is a method of updating m and c values to minimze the cost function and to get the BFL.\n",
    "* BFL >> When working with linear regression our main goal is to find the the best fit line where the error between the actual and predicted values and it should be minimum.\n",
    "* BFL will always have least  mean sqoare error \n",
    "* A regression model uses gradient descent to update the coeffiecents of the line (m and c) by reducing the cost function by a random selection of coefficient values and then iteratively updating the values to reach the minimum cost function.\n",
    "* To update m and c we take gradients from the cost fucntion. To find these gradients we take partial derivates of m and c.\n",
    "* Calculate the partial derivative of the loss function with respect to m and plug in the current values of x,y,m and c in to obtain the deriavtives value D\n",
    "* Global minima >> It is a point that obtains the absloute lowest value of our functionn\n",
    "* Learning rate >> It detrrmines the size of the steps that are taken by the gradient descent algorithm.\n",
    "* If learning rate is very small it woulds take an long time to converge and become computaionally expensive.\n",
    "* If Alpha is very large it may fail to converge and overshooy the minimum.\n",
    "* gradient descent algorithm is used to minimize the cost function loss function an by using the MSE value we find out the best fir line.\n",
    "* one of the best algorithm to minimixe the error.\n",
    "* Until the function is close to or equal to zero the model will continue to adjust its parameters to yield the smallest possible error.\n",
    "* Gradient descent algorithm gives optimum value of m and c of the linear regression equation \n",
    "* With these values of M and C  we will get the equation of the best fit line and ready to make predictions \n",
    "\n",
    "> Steps in Gradient Descent?\n",
    "1. It will directly assume the value of m and c value for 1st iteration.\n",
    "2. By using the m and c values and equation of the line we get the value of Y\n",
    "* we get the x and y value by using that it will draw the regression line \\\n",
    "* Then we calculate the MSE value.\n",
    "3. It will calculate the m and c values FOR 2nd iteration by using the formula \n",
    "* Mnew = Mold - Learning Rate(alpha) * SLope \n",
    "* Cnew = Cold - learning rate (alpha) * slope \n",
    "* By using that we get m and c values \n",
    "* By using that we draw the 2nd regression line.\n",
    "* then we calculate MSE value .\n",
    "4. By same method we find multiple regression line and we find out the MSE value infinite time.\n",
    "5. Among all these lines select 1 line having minimum MSE and consider this line as Best fit line.\n",
    "\n",
    "> Finding the Best fit line or regression line ?\n",
    "* While working with linear regression our main goal is to find out the BFL that has minimum error between the predicted values and actual values.\n",
    "* BFL will have least error \n",
    "* BFL will have the lowest MSE \n",
    "* BFL will pass through maximum data points \n",
    "* BFL will have best m and c values \n",
    "* Gradient descent algortihm finds one BFL among infinite number of Possiblities \n",
    "\n",
    "\n",
    "* In gradient descent algorithm we use local and global minima in order to decraese the loss function \n",
    "* The point in a curve which is minimum when compared to its precedding and successive points is called as Local minima.\n",
    "* The points in a curve which is minimum compared to all points in the curve is called as Global minima.\n",
    "* For a curve there can be more than one local minima but there will be one global minima.\n",
    "* Global minima >> point of convergence where the cost function is at its minimum.\n",
    "\n",
    "> Regularization in machine learning \n",
    "* When training a machine learning model the model can be easily overfitted or underfitted \n",
    "* To avoid these we use regularization in machine learning \n",
    "* Regualrization techniques help reduce the possiblity of overfitting and help us obtain an optimal model.\n",
    "\n",
    "> Overfitting and underfitting \n",
    "* In real world the dataset present will never be clean and perfect.\n",
    "* It mean each data contains impurity ,noise data ,oultiers,missing data or imbalanced data.Due to these impurities different problems occur that affect the accuracy and the performance of the model.\n",
    "* One of such problems is overfitting in machine learning.\n",
    "* Overfitting is a problem thata model can exhibit.\n",
    "\n",
    "* Noise >> Noise is a meaningless data or irerelevant data present in your dataset.It affects the performance of your model if it is not removed.\n",
    "* Generalization >> It shows how well your model is trained to predict unseen data.\n",
    "* Bias >> Bias is the differnce betwen the actual values and trained values of the training data.\n",
    "* Training error >> Yactual - Ypredicted\n",
    "* Less error >> less bias   >> high accuracy \n",
    "* More erro >> More bias   >> less accuracy \n",
    "\n",
    "> Variance \n",
    "* Variance tells us about the spread of the data points .It calculates how much a data point differs from its mean value and how far it is from the other data points in the dataset.\n",
    "* Difference between the taining and testing accuracy error \n",
    "* If the machine learning model performs well only on training data and not well on testing data set then variance occurs.\n",
    "* Low variance >> high accuracy \n",
    "* High varaince >> low accuarcy \n",
    "\n",
    "> Overfitting \n",
    "* Model is performing well on training dataset and not on testing dataset is called as overfitting.\n",
    "* Training accuracy is much higher than testing accuarcy.\n",
    "* Low bias and high variance \n",
    "\n",
    "> Underfitting \n",
    "* Model is neither performing well on training data not on testing data.\n",
    "* High Bias and low variance \n",
    "* To avoid Underfitting \n",
    "1. Proper feature selection \n",
    "2. Hypertuning \n",
    "3. Add more features \n",
    "\n",
    "> Best model \n",
    "* Model is performing well on training as well as testing dataset \n",
    "* Low Bias and Low variance\n",
    "\n",
    "> How to avoid Overfitting ?\n",
    "1. Hyperparameter tuning \n",
    "* Grid search CV \n",
    "* Randomized search CV \n",
    "2. TRain with more data \n",
    "3. Reduce the number of features \n",
    "* Feature selection technique \n",
    "* Dimensionality reduction technique (PCA)\n",
    "4. Pruning (Cutting Branches of Decision Tree)\n",
    "5. Regularization (L1 and L2) >> only used in linear model (Linear regressionk and Logistic regression)\n",
    "* L1 Regularization >> Lasso regression \n",
    "* L2 Regularizatrion >> Ridge regression \n",
    "6. Cross validation \n",
    "* K-fold cross validation \n",
    "7. Remove oultiers \n",
    "\n",
    "> How to avoid Underfitting ?\n",
    "1. Increase the number of features \n",
    "* add new features \n",
    "* Add derived features(Creating multiple featutes from single feature)\n",
    "2. Use proper selection technique\n",
    "* Filter methods \n",
    "* Wrapper methods\n",
    "* Embedded methods\n",
    "3. Hyperparameter tuning \n",
    "\n",
    "> Bias and Variance tradeoff \n",
    "* You need to find a good balance between the bias and variance of the model we have used \n",
    "* This tradeoff in complexity is what is referred as bias and variance tradeoffs\n",
    "* An optimal balance of bias and variance should never overfit or undefit the model.\n",
    "* This tradeoff between the bias and varaiance applies toa all supervised learning algorithm \n",
    "\n",
    "* low bias >> the difference between the actual and predicted values of training data is low.\n",
    "* High bias >> the difference between the actual and predicted values of training data is high.\n",
    "* Low variance >> diffrence between the accuaracy of training data and testing data is low.\n",
    "* High variance >> difference between the accuracy of trainig data and testing data is high \n",
    "* To make a good fit we need to have a good balance between bias and variance \n",
    "\n",
    "> How to fix Bias and variance problems?\n",
    "> Fixing high bias \n",
    "* Adding more input features will help improve the data to fit better.\n",
    "* Add more ploynomial features to improve the complexity of the model.\n",
    "* Decrease the regularization terms to have a balance between bias and variance \n",
    "\n",
    "> Fixing high variance \n",
    "* Reduve the input features use only relevant features to reduce the overfitting.\n",
    "* Getting more training data will help in this case .\n",
    "\n",
    "> Outliers \n",
    "* How outliers are introduced in data ?\n",
    "1. Data entry error >> human error \n",
    "2. Measurement error >> Machine error \n",
    "3. Intentional error >> DUmmy dataset\n",
    "4. Sampling error >> Mixing of data from resources \n",
    "5. Natural error >> Most of the data from wrong resources\n",
    "6. Experimental errors >> data extraction or experiment planning \n",
    "7. data processing errors >> data maniupulatioj error \n",
    "\n",
    "> impact of outliers ?\n",
    "1. Reduce the power statistical analysis \n",
    "2. High impact on mean value and std \n",
    "3. Algorithms donot perform well in presence of outliers \n",
    "4. Impact on \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
