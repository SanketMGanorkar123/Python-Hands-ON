{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction techniques \n",
    "* In Dimensionality reduction techniques we reduce the number of features.\n",
    "1. Feature selection\n",
    "   1. Filter Method \n",
    "   2. Wrapper Method \n",
    "   3. Embedded method\n",
    "2. Feature extraction(PCA)\n",
    "   1. Linear discrimnant method\n",
    "   2. Principal component analysis(PCA)\n",
    "* 90% we use PCA \n",
    "> Feature selection\n",
    "* In Feature selection we select the subset of original dataset i.e we are selecting the best features and then we will train our model on the selected features.\n",
    "* In Feature selection we select the best features by their variance the more the variance the more it is contributing in predicting Y.\n",
    "* But when we are selecting the best feature between two features if the variance of both features are same then we may face problem in feature selection so in such cases we use PCA.\n",
    "> PCA (Principle component analysis)\n",
    "* It is one of the most popular feature extraction techniques.\n",
    "* It is a supervised machine learning algorithm which is used for dimensionality reduction techniques.\n",
    "* In PCA we convert high dimensional data into low dimensional data.\n",
    "* When we had 2 features in the data and we had to select the best feature among them but if they had same variance then we might face difficulties in selecting the best feature.\n",
    "* And by selecting the best feature we may also avoid the another feature which was contributing in predicting Y.\n",
    "* In this form we are loosing our data which is not a ideal way while building the model.\n",
    "* In PCA we combine both features and create a single feature which is combination of the two features.\n",
    "* So in this way we are performing dimensionality reduction techniques and also increasing the accuracy of the model.\n",
    "* Suppose we have features X1, X2, X3 and X4 as independent variables and Y as Target variable.\n",
    "* In PCA we will get number of columns(PC1, PC2, PC3, PC4)(PC = Principal component) equal to number of features(X1, X2, X3, X4).\n",
    "* PC1 will be the extraction of X1, X2, X3, X4 features.\n",
    "* PC2 will be the extraction of X1, X2, X3, X4 features.\n",
    "* Same for PC3 and PC4.\n",
    "* That is we are having essence of X1,X2,X3,X4 in each principal component.\n",
    "* As PC1 , PC2 , PC3 , PC4 are the extraction of X1, X2, X3, X4 and suppose by using PC1 , PC2 , PC3 , PC4 we are getting accuracy as 96% , by using only PC1 , PC2 we may get accuracy as 95%.\n",
    "* In this way by selecting PC1 and PC2 we are getting accuracy equivalent to Combination of PC1 , PC2 , PC3 , PC4 and also we are reducing the number of features and so we are converting high dimensional data to low dimensional data.\n",
    "> Difference between Covariance and Coefficient of Corelation(R)\n",
    "* R has a range between -1 to +1 ,Covariance range is -infinity to +infinity.\n",
    "* Covariance shows directional relationship between 2 variables , and R shows strength of relationship between 2 variables.\n",
    "* Formula for covariance is = summation((Xi - Xmean) (Yi - Ymean)) / N\n",
    "* Formula for R = summation((Xi - Xmean)^2 (Yi - Ymean)^2) / Sqrt((Xi - Xmean)^2 (Yi - Ymean)^2)\n",
    "* Formula for R = Covariance / Product of Standard Deviation\n",
    "> Mean \n",
    "* Mean tells us central tendency of any kind of data.\n",
    "> Variance \n",
    "* Formula for variance is = summation((Xi - Xmean)^2) / N\n",
    "* Variance tells us the spread of the data.\n",
    "* \n",
    "> Working of PCA \n",
    "* Previously when we had 2 features in the dataset with same variance we were not able to select the best feature among the two.\n",
    "* In PCA we are shifting the new X axis towards the data points and creating the difference between the variance.\n",
    "* While sifting the new X axis we will see for where we get maximum variance after that the perpendicular line will be the Y axis.\n",
    "* The new X axis will be the PC1 and the Y axis will be the PC2.\n",
    "> Difference between variance and Covariance \n",
    "* Variance will never give the direction , it will only show the spread of the data.\n",
    "* Covariance gives the directional relationship between 2 data points.\n",
    "* From further on we will not use variance and the reason is that Covariance gives direction and spread of the data points and variance doesnot.\n",
    "> Revise on Eigen values and Eigen vectors these both come under linear transformation.\n",
    "* In PCA Eigen values represent the amount of variance captured by each principal component.\n",
    "* Larger Eigen values indicate more significant components.\n",
    "* Eigen vectors represent the direction of maximum variance for each principal component.\n",
    "* In short Eigen values guide the importance of components and Eigen vectors define their directions for dimensionality reduction in PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
